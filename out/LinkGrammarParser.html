<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Link Grammar Parser
      </h1>
      <p>
        From <a href="http://www.link.cs.cmu.edu/link/">http://www.link.cs.cmu.edu/link/</a> :
      </p>
      <dl>
        <dt> </dt>
        <dd>The Link Grammar Parser is a syntactic parser of English, based on link grammar, an original theory of English syntax. Given a sentence, the system assigns to it a syntactic structure, which consists of a set of labeled links connecting pairs of words.</dd>
      </dl>
      <p>
        I've used this and it's stunning at finding grammatical errors in English text. It produces correct parses for grammatically correct sentences, including multiple versions for ambiguous sentences, and is reasonably robust in the face of unrecognised words.
      </p>
      <p>
        You give it a grammatically correct or near correct sentence and it will tell you what part of speech each word is. - <a href="http://www.link.cs.cmu.edu/link/submit-sentence-4.html">http://www.link.cs.cmu.edu/link/submit-sentence-4.html</a> lets you use it to analyse single sentences, <a href="http://www.link.cs.cmu.edu/link/">http://www.link.cs.cmu.edu/link/</a> gives a more complete outline. The output is a little cryptic, but a full explanation of all the terms is given on <a href="http://www.link.cs.cmu.edu/link/dict/index.html">http://www.link.cs.cmu.edu/link/dict/index.html</a>
      </p>
      <ul>
        <li>
           Worthwhile for people to know about, but just for the record, it is beyond the state of the art to <strong>accurately</strong> automatically analyze the parts of speech of words in unconstrained text. E.g. 95% accurate would be very nice, but can't currently be done. On the other hand, some applications are worthwhile even with e.g. 60% accuracy.
        </li>
      </ul>
      <ul>
        <li>
           <em>See further down the page for this author's examples of "unconstrained text".</em>
        </li>
      </ul>
      <p>
        I believe this simply to be false, unless by "unconstrained" you mean "context-free and in ungrammatical streams." The <a href="LinkGrammarParser.html">LinkGrammarParser</a> achieves far better than 95% with grammatical text, and does very well with ungrammatical but reasonable text.
      </p>
      <p>
        Hmmm:  
      </p>
      <code>
        (S (VP (S Worthwhile<br/>
        (PP (PP for<br/>
        (NP (NP people)<br/>
        (SBAR (WHNP to)<br/>
        (VP know<br/>
        (PP about)))))<br/>
        , but just<br/>
        (PP for<br/>
        (NP the record))))<br/>
        ,)<br/>
        (S (NP it)<br/>
        (VP is<br/>
        (PP beyond<br/>
        (NP (NP the state)<br/>
        (PP of<br/>
        (NP the art))))<br/>
        (S (VP to<br/>
        (VP (ADVP accurately)<br/>
        (ADVP automatically)<br/>
        analyze<br/>
        (NP (NP the parts)<br/>
        (PP of<br/>
        (NP (NP speech)<br/>
        (PP of<br/>
        (NP words)))))<br/>
        (PP in<br/>
        (NP unconstrained text)))))))<br/>
        .)<br/>
      </code>
      <p>
        ...looks pretty damn accurate to me!
      </p>
      <ul>
        <li>
           Citation/URL? (I see that they added a "morpho-guessing" feature some years back that they said improved things a lot, but I don't see accuracy figures.)
        </li>
      </ul>
      <p>
        <em>Based purely on having used it on over 1000 sentences and agreeing with its analysis on the 300 I checked by hand.</em>
      </p>
      <p>
        It's nice that it worked well for you, but that's a different question, really.
      </p>
      <p>
        No matter which AI problem we talk about, it is well known that they are all quite difficult, and it is quite important to know how accurate one can expect an algorithm to be. In testing OCR systems, the algorithms are often 100% accurate on samples "in the lab", but when tested on samples from the wild (e.g. seventh-generation photocopies with never-before-seen fonts), accuracy can drop to 60%. That's not hypothetical, I'm talking about tests I've run myself.
      </p>
      <p>
        Similar things happen in every AI domain from machine vision to linguistics. Accurate tagging of parts of speech is well known to be one of those problems. Yes, various algorithms can approach 100% accuracy on some kinds of sentences, but no, they don't on a large corpus e.g. drawn from random magazines and newspapers.
      </p>
      <p>
        Just as a by the way, 95% sounds good, and is good enough for some purposes, but for many other applications something closer to 99.9999% would be needed (e.g. when a technology is competing with minimum wage human operators).
      </p>
      <p>
        -- <a href="DougMerritt.html">DougMerritt</a>
      </p>
      <p>
        What are you saying?  That if an AI problem is ever essentially cracked, it ceases to be an AI problem?  The links provided open the algorithm to a theoretically infinite corpus...  so let the failures be noted and the reasons discussed (that might fail, let's take a look...)
      </p>
      <code>
        (S so<br/>
        (S (VP let<br/>
        (NP (NP (NP the failures)<br/>
        be<br/>
        (VP noted))<br/>
        and<br/>
        (NP (NP the reasons)<br/>
        (VP discussed))))))<br/>
      </code>
      <p>
        ...wrong, I think.  Compare the expanded sentence "let the failures be noted and let the reasons be discussed"
      </p>
      <code>
        (S (S (VP let<br/>
        (NP the failures)<br/>
        (VP be<br/>
        (VP noted))))<br/>
        and<br/>
        (S (VP let<br/>
        (NP the reasons)<br/>
        (VP be<br/>
        (VP discussed)))))<br/>
      </code>
      <p>
        ...failure under ellipsis with conjunction
      </p>
      <p>
        <em>No, no, I'm saying that your results are not following rigorous standards for statistical analysis, so they are statistically meaningless, whereas for most purposes, when evaluating AI algorithms, people need rigorous statistically meaningful measurements. This is an area I've done a lot of professional work in, and I've seen enormous differences between anecdotal evidence from casual testing versus careful methodology using large test sets painstakingly gathered from a large number of real world sources.</em>
      </p>
      <p>
        Well they're not my results, but I agree that they are anecdotal rather than statistical evidence.  When testing such things, it is human nature to try to break them, so I would expect the anecdotal evidence to overstate rather than understate the error rate, but who knows?
      </p>
      <p>
        <em>Only if you're evil minded like I am. Try it on Shakespeare, poetry, street jargon, surrealist essays, and real horror cases like literary/theatre/art criticism. :-)</em>
      </p>
      <p>
        Since I am predisposed to it and hardly likely to be fair, and since you are, by your own admission, evil-minded, I would suggest that <strong>you</strong> try it out and tell us your results. Further, I did say "grammatically correct", into which category Shakespeare, poetry, street jargon, surrealist essays, and real horror cases like literary/theatre/art criticism frequently fail to fall.
      </p>
      <p>
        It does appear that we are talking at cross-purposes. You seem to be using the term "unconstrained" to mean "random mish-mash of words that nonetheless manages to communicate something to someone."
      </p>
      <hr/>
      <p>
        <a href="CategoryNaturalLanguage.html">CategoryNaturalLanguage</a>
      </p>
    </div>
  </body>
</html>