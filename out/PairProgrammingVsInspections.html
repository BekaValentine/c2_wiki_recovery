<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Pair Programming Vs Inspections
      </h1>
      <p>
        It has been widely reported that Pair Programming is a suitable substitute for formal inspections.  For example, in XP, we do not typically use formal inspections in addition to the standard XP techniques.
      </p>
      <p>
        What decision should an organization make if it is not practicing all (or any for that matter) of the XP techniques, but does conduct formal inspections.  Should <a href="PairProgramming.html">PairProgramming</a> replace inspections as a defect detection mechanism?  What is the experiment that one could run to determine this?
      </p>
      <p>
        My goal for this page is to have a set of experiments that an organization could apply while rolling out Pair Programming such that it could quantitatively determine if <a href="PairProgramming.html">PairProgramming</a> is suitably substituting for Inspections.
      </p>
      <p>
        Approach
      </p>
      <p>
        The easiest way to do this would be to conduct an experiment similar to what <a href="LaurieWilliams.html">LaurieWilliams</a> did with her class at the University of Utah (<a href="http://collaboration.csc.ncsu.edu/laurie/Papers/CSED.pdf).">http://collaboration.csc.ncsu.edu/laurie/Papers/CSED.pdf).</a>  Unfortunately, I don't have the luxury of giving the same problem to 2 randomized trial sets.  Whatever experiment comes out of this it will need to evaluate data from different individuals (those pair programming, and those not), and different sets of problems.  The results will skew if the group pair programming is better (or worse) than the group solo programming.
      </p>
      <p>
        Questions:
      </p>
      <p>
        Does the cost of defect removal from an inspection outweigh the benefits of preventing the defect from reaching test?
      </p>
      <p>
        Does pair programming remove the same type of defects removed by inspections?  Does the pair have the same ?Blind Eye? to certain types of defects that a solo programmer has?
      </p>
      <p>
        Metrics:
      </p>
      <p>
        Effort spent pairing (effort hours)
        Defects found inspecting paired code (major/minor)
        Effort spent in inspections (effort hours)
      </p>
      <p>
        These are the easy numbers to gather.  More expensive are such things as defects released to test (or beyond).  We don?t commonly keep track of the specific code that originally caused individual defects.
      </p>
      <p>
        Other Considerations
      </p>
      <p>
        Should we try to normalize the pairs based on some kind of expert/average/novice scale? 
      </p>
      <p>
        <a href="MichaelKirby.html">MichaelKirby</a>
      </p>
      <hr/>
      <p>
        <a href="CategoryPairProgramming.html">CategoryPairProgramming</a>
      </p>
    </div>
  </body>
</html>