<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Double After Full
      </h1>
      <p>
        An algorithm for space allocation:
      </p>
      <dl>
        <dt> </dt>
        <dd>When a container gets full, reallocate with twice as much space as last time.</dd>
      </dl>
      <p>
        It sounds like it will be too greedy to work out.  In practice, it's usually a good space/time tradeoff.
      </p>
      <p>
        <strong>Mathematical Basis.</strong>
      </p>
      <p>
        This scheme of doubling memory is denoted as x2
      </p>
      <ul>
        <li>
           Q/. Does this copy memory a lot of times?
        </li>
        <li>
           A/. For <a href="SufficientlyLarge.html">SufficientlyLarge</a> values of N, the algorithm allocates and copies 2N copies of the data. Thus in big O notation the algorithm is O(N).
        </li>
        <li>
           Q/. What about other values other than x2 allocation?
        </li>
        <li>
           A/. All multiplicative schemes x1.5 x1e99 x1.001 are all in big O notation, k O(N). 
        </li>
      </ul>
      <p>
        See basic Mathematics text for 'sum of geometric series'. Note how this demonstrates the lie of big O analysis. Big O notation ignores all 'k' values, but for the x1.001 algorithm the 'k' is likely to actually be the only important part for many practical values of N.
      </p>
      <ul>
        <li>
           Q/. So why x2 and not x3 or x1.5?
        </li>
        <li>
           A/. x2 has been found to work for many people and many problems :) (It's fashionable, and easy to compute, and computer folk have a fetish for powers of two)
          <ul>
            <li>
               A2. For <em>any</em> memory reuse, the <a href="GoldenRatio.html">GoldenRatio</a> is the best growth factor if there is no memory allocated for bookeeping purposes - if there is then a smaller growth factor (i.e. x1.5) is better. There are some long discussions on what is the best growth factor here: <a href="http://tinyurl.com/6awz7">http://tinyurl.com/6awz7</a> -- <a href="JamesKeogh.html">JamesKeogh</a>
            </li>
          </ul>
        </li>
      </ul>
      <p>
        A more controversial but accurate statement and analysis.
      </p>
      <ul>
        <li>
           Q/. are there other enhancements ?
        </li>
        <li>
           A/. For vector <T *> A; Don't start at N=1 the allocator will probably frequently give you 12-16 bytes of RAM any away. The time taken to double memory when N is small is represented by k + O(N) where O(N) << k and thus the O(N) may be ignored!!!! Ignoring that term results in O(ln(N)) for the whole doubling sequence of the algorithm and more generally O(ln(N/S)) where S is the value of the initial allocation size.
        </li>
      </ul>
      <p>
        <strong>Conclusion:</strong>
      </p>
      <ul>
        <li>
           The doubling is mathematically sound.
        </li>
        <li>
           Other versions using Multipliers sufficiently near 2 are also mathematically and practically sound.
        </li>
        <li>
           2 is fashionable. Make measurements to prove a different value is 'better' in your case.
        </li>
        <li>
           All the following really nifty people use two too.
        </li>
      </ul>
      <p>
        <strong>Known uses/references:</strong>
      </p>
      <ul>
        <li>
           The GNU C++ <a href="StandardTemplateLibrary.html">StandardTemplateLibrary</a> container classes
        </li>
        <li>
           Smalltalk-80
        </li>
        <li>
           <a href="DonaldKnuth.html">DonaldKnuth</a> proves that doubling when full is the optimal allocation strategy in <a href="TheArtOfComputerProgramming.html">TheArtOfComputerProgramming</a>, Chapter 2 <em>(I couldn't find this. What page was it on?)</em>
        </li>
        <li>
           <a href="IntroductionToAlgorithms.html">IntroductionToAlgorithms</a> (Cormen et. al.) Chapter 18 Amortized Analysis: Dynamic tables (p. 367)
        </li>
      </ul>
      <p>
        <strong>Lack of uses:</strong>
      </p>
      <ul>
        <li>
           Java's <a href="ArrayList.html">ArrayList</a> also seems to use the <a href="GoldenRatio.html">GoldenRatio</a> algorithm. It multiplies by 3/2 (or half-again after full).
        </li>
      </ul>
      <p>
        I am not Knuth, but I don't think anyone else authoring this page is either:
      </p>
      <ul>
        <li>
           Doubling (x2) for <a href="SufficientlyLarge.html">SufficientlyLarge</a> N results in, on average, 2N allocations and copies. 1 + 1/2 + 1/4 + 1/8 + ...  On average, it uses 50% more memory than is needed and can never reuse its own deallocated memory. This is ONE cost benefit trade off. 
        </li>
        <li>
           Java's alg (x1.5) for <a href="SufficientlyLarge.html">SufficientlyLarge</a> N results in, on average, 3N allocations and copies. 1 + 2/3 + 4/9 + ... On average, it uses <25% more memory than is needed, but can reuse deallocated memory after growing a few times. This is another ONE cost benefit trade off. 
        </li>
      </ul>
      <p>
        In general, if when you run out of space you allocate <em>k</em> times
        as much again, then (in the limit of large <em>N</em>)
      </p>
      <ul>
        <li>
           each item of data gets allocated/copied about <em>k</em>/(<em>k</em>-1) times;
        </li>
        <li>
           you use, on average, (<em>k</em>-1)/log(<em>k</em>) times as much memory as the theoretical minimum. (That's the natural logarithm, not log to base 10 or base 2.)
        </li>
      </ul>
      <p>
        We can redline the bogosity meter and claim that the best overall efficiency comes when the product of these two inefficiency factors is as small as possible; that means you're minimizing <em>k</em>/log(<em>k</em>), which happens when <em>k</em>=<em>e</em>. Ha.
      </p>
      <p>
        (In reality, you should take little notice of this sort of
        analysis. Try some different schemes and measure.)  <em>False.  This type of analysis is as (not more) important as experimenting.  Anyone who does (or is only capable of) only one of the two is doomed to mediocrity.</em>
      </p>
      <hr/>
      <p>
        Suppose your data structures can shrink as well as grow.
        You might think that you should double your allocation when
        you fill up, and halve your allocation when you can. That's
        a bad idea, because if the size happens to wobble around near
        the boundary you'll spend all your time growing and shrinking.
        So you should build in some hysteresis: grow and shrink by a
        factor of <em>k</em>, but don't shrink until you're too large by
        a factor of <em>k1</em>, where <em>k1</em>><em>k</em>.
      </p>
      <p>
        Example: Double when full, halve when one quarter full.
      </p>
    </div>
  </body>
</html>