<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Spiritual Machines
      </h1>
      <p>
        <em>The Age of Spiritual Machines: When Computers Exceed Human Intelligence</em> by Ray Kurzweil (Penguin, 2000) ISBN: 0140282025.
      </p>
      <p>
        <em>from amazon ...</em>
      </p>
      <p>
        How much do we humans enjoy our current status as the most intelligent beings on earth? Enough to try to stop our own inventions from surpassing us in smarts? If so, we'd better pull the plug right now, because if RayKurzweil is right, we've only got until about 2020 before computers outpace the human brain in computational power.
      </p>
      <hr/>
      <ul>
        <li>
           <a href="http://www.wired.com/wired/archive/8.04/joy.html">http://www.wired.com/wired/archive/8.04/joy.html</a> -- <a href="BillJoy.html">BillJoy</a>'s front page article in Wired: (Why the future doesn't need us: Our most powerful 21st-century technologies - robotics, genetic engineering, and nanotech - are threatening to make humans an endangered species.)
        </li>
        <li>
           <a href="http://www.sciencefriday.com/pages/2000/Mar/hour2_031700.html">http://www.sciencefriday.com/pages/2000/Mar/hour2_031700.html</a> -- Kurzweil, Joy and Turkle.
        </li>
        <li>
           <a href="http://www.stanford.edu/dept/symbol/Hofstadter-event.html">http://www.stanford.edu/dept/symbol/Hofstadter-event.html</a> -- Kurzweil, Joy and others debate at Stanford.
        </li>
      </ul>
      <hr/>
      <p>
        I was present at the Saturday (April 1!) "debate" with Joy and Kurzweil at Stanford. The auditorium(s) were packed, standing-room-only out into the halls. Many young people and lots of techno types from Silicon Valley. Evidence I believe of much interest and concern about these issues.
      </p>
      <p>
        I only stuck around for the first segment with Kurzweil, Joy, and Morovec speaking (in that order). Perhaps I'm biased, but I have to say that Joy had way more passion and convincing arguments than Kurzweil/Morovec. Read the Wired article above for a summary. Joy is not so concerned about computers outpacing human intelligence, that seems pretty distant still in the future. The concern is more with what will happen in the next 20 years with the explosive growth of computer power (estimated 10**6 increase) and the public knowledge and technology to manipulate genes and the human genome. The potential for weapons of mass destruction is very real. Already, artificial genes inserted into the environment are caused unintended and destructive effects. What will happen if access to this type of technology is widespread? And disgruntled or alienated people such as a unibomber with access to the human genome project (public domain on the internet) and a genetics lab (just ask your biology major friend for help) begin to design REAL biological, (not computer) viruses.
      </p>
      <p>
        Joy's argument: our species is not yet mature enough to handle these technologies wisely. The only recourse is to "relinquish" some of these capabilities before it is too late. Or institute some kind of control or regulatory system that oversees the technology. After the Wired article, Joy has been invited to speak in front of national organizations such as the NSF, American Association for the Advancement of Science, etc. The concern is very real.
      </p>
      <p>
        The nanotech and robotics potential for abuse is maybe 20 or 50 years beyond that of genetics engineering, but still cause for concern.
      </p>
      <p>
        -- Bill Croft (<a href="mailto:croft@lightfield.com)">mailto:croft@lightfield.com)</a>
      </p>
      <p>
        The problem with Joy's solution is that it is unenforceable. The whole of [Pick your own good guys] might actually manage it (unlikely as that is) and would then have no understanding of the weapons coming out of [Pick your own bogeyman]. So, just in case, they'll probably keep someone looking at it, in which case everyone else will too and... oh well. Relinquishment is not an option in a divided world, so it is probably best just to go for it. At least that way we'll have some idea of what you are dealing with when bad things happen.
      </p>
      <hr/>
      <p>
        <a href="CategoryBook.html">CategoryBook</a>
      </p>
    </div>
  </body>
</html>