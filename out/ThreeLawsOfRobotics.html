<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Three Laws Of Robotics
      </h1>
      <p>
        <a href="IsaacAsimov.html">IsaacAsimov</a>'s Three Laws of Robotics:
      </p>
      <ol>
        <li>
           A robot may not injure a human being or, through inaction, allow a human being to come to harm.
        </li>
      </ol>
      <ol>
        <li>
           A robot must obey orders given it by human beings except where such orders would conflict with the First Law.
        </li>
      </ol>
      <ol>
        <li>
           A robot must protect its own existence as long as such protection does not conflict with the First or Second Law. 
        </li>
      </ol>
      <p>
        -- <a href="StevenNewton.html">StevenNewton</a>
      </p>
      <p>
        <a href="IsaacAsimov.html">IsaacAsimov</a> wrote wonderful stories and novels around these laws. Oh, and he invented the three laws. The <a href="ThreeLawsOfRobotics.html">ThreeLawsOfRobotics</a> are first spelled out in the story "Runaround."
      </p>
      <p>
        <em></em><a href="ArthurCeeClarke.html">ArthurCeeClarke</a> reports that Asimov said that he first heard the laws from John Campbell. This may be a joke by Asimov, or possibly Clarke.<em></em>
      </p>
      <hr/>
      <p>
        <strong>The 0th Law</strong>
      </p>
      <p>
        In the chapter "The Duel" in <em>Robots and Empire</em>, Asimov first presents another law, which he calls the Zeroth Law of Robotics, and adjusts the other ones accordingly:
      </p>
      <ol>
        <li>
           A robot may not harm <em>humanity,</em> or through inaction allow <em>humanity</em> to come to harm.
        </li>
      </ol>
      <ol>
        <li>
           A robot may not harm a human, or through inaction allow a human to come to harm, unless this interferes with the zeroth law.
        </li>
      </ol>
      <ol>
        <li>
           A robot must obey orders given to it by a human being unless such orders interfere with the zeroth or first laws.
        </li>
      </ol>
      <ol>
        <li>
           A robot must defend its own existence unless such defense interferes with the zeroth, first or second laws.
        </li>
      </ol>
      <p>
        <em>Does the zeroth law apply to programs in general? If so, there are so many programs whose heads must roll. The pile of spaghetti that I'm currently working is certainly damaging my humanity.</em>
      </p>
      <p>
        The problem I see with the Zeroth law is that it's based on groupthink - it's a way of saying, "I have a right to do to you anything I choose to do, <i>for the good of humanity.</i> It's the rationalization for all collectivist systems, which have proven to lead to tyranny and mass murder <i><b>EVERY SINGLE TIME!</b></i> - Rich Grise, Radical Libertarian Loon
      </p>
      <p>
        The problem with the Zeroth (if you're using it), First, and Second Laws is, of course, perception and definition. Define "human", and "harm", and figure out how to write a program to perceive both with reasonably high accuracy, and these laws become implementable.
      </p>
      <p>
        <em>It's not the only problem. Another is that often any course of action allows some people to come to harm who could have been helped. You can pretend to get around this by amending the Laws to talk about the "greatest good of the greatest number" or something of the kind, but that doesn't really help because of </em>another<em> problem: you often can't know enough about the consequences of your actions to tell whom they will hurt and harm.</em>
      </p>
      <p>
        <em>Difficult subject, ethics.</em>
      </p>
      <p>
        <em>It's supposed to be probabilistic. The robot takes the set of actions that should violate the laws the least. For instance (allow me to steal this instance from one of Asimov's novels) if one man is trying to kill ten, and the robot can only stop him by killing him, the robot will kill the man.</em>
      </p>
      <ul>
        <li>
           In the earlier novels/collections ("I, Robot", "Caves of Steel", etc) this is completely false, a robot faced with that choice would go nuts/fry its positronic circuitry/etc. Only in later novels where the Zeroth Law is introduced is it even remotely <strong>possible</strong> (but typically still extremely unlikely) that a robot would do a probabilistic trade-off of that sort. And in fact, Asimov underscored throughout even all of the later novels how difficult it was for a robot to make that probabilistic trade-off; it was <strong>never</strong> simply a matter of a robot doing a Bayesian analysis of "the greatest good for the greatest number". Never. For the later novels with highly advanced robots operating under the Zeroth Law, it was, in effect, extremely painful for them to make such trade-offs; they would recruit human agents to perform the necessary action, for instance, at great risk, rather than do so themselves.
        </li>
      </ul>
      <p>
        The Will Smith movie <em>I, Robot</em> (not the book) hints at the Zeroth Law.
      </p>
      <hr/>
      <p>
        <strong>Proposed 4th Laws</strong>
      </p>
      <p>
        What I wonder about is Asimov's Fourth Law imprinted in all humans which prevents them from erasing the first three laws from robots. That's the one I have trouble seeing implemented.
      </p>
      <p>
        Someone (not me) came up with a 4th law:
      </p>
      <ol>
        <li>
           A robot must practice proper d[/m]ental hygiene as long as this does not conflict with the First, Second, or Third Law. [ :-) can be subsumed under 3. -fp]
        </li>
      </ol>
      <ul>
        <li>
           Since "mental hygiene" is undefined, this comment is vague to meaningless (until a definition <strong>is</strong> made).
        </li>
      </ul>
      <p>
        <em>Pardon me? Assuming this robot has any contact with humanity, I think it's fair to say this can be subsumed under the first law! -- </em><a href="AdamBerger.html">AdamBerger</a><em></em>
      </p>
      <p>
        What... a robot should have good dental hygiene? Or mental hygiene should it be?
      </p>
      <p>
        <em>If the robot is Gladia's "husband" R. Jander, he'd better have good dental hygiene!</em>
      </p>
      <p>
        As mentioned below, the laws are supposed to be so hardwired into the robot's brain that it would be completely inoperable before one failed. Extensive testing of each one before release apparently guaranteed this.
      </p>
      <ul>
        <li>
           Not just testing, also mathematical analysis of the design prior to implementation.
        </li>
      </ul>
      <hr/>
      <p>
        <strong>Proposed Modification</strong>
      </p>
      <p>
        Asimov could have moved the inaction clause of the first law to a separate law something like:
      </p>
      <ol>
        <li>
           A robot may not harm a human
        </li>
      </ol>
      <ol>
        <li>
           A robot must obey humans
        </li>
      </ol>
      <ol>
        <li>
           A robot must not through inaction allow harm to a human
        </li>
      </ol>
      <p>
        To use the chauffeur example, you could explain to the robot that if he doesn't drive you to work, you will drive yourself. The robot, knowing that it is a better driver than you are, would thereby drive you where you wanted to go.
      </p>
      <p>
        But if you did this, you'd open situations where a robot is an accomplice to murder.
      </p>
      <p>
        <em>This is simply incorrect (it has been analyzed to death, decades ago). For instance, the above proposed rewrite does not cover "a robot must not, through inaction, allow an order to be disobeyed, so long as that inaction does not violate the first law". Yet that condition is in fact a consequence of the original laws. No shooting from the hip, please. :-)</em>
      </p>
      <hr/>
      <p>
        <strong>The Ten Ethical Laws of Robotics</strong>
      </p>
      <p>
        See <a href="http://www.ethicalvalues.com/">http://www.ethicalvalues.com/</a> for the "The Ten Ethical Laws of Robotics," an updated, expanded effort to codify virtue for the use of AI.
      </p>
      <hr/>
      <p>
        <strong>First Law of Computing</strong>
      </p>
      <p>
        User interface pioneer <a href="JefRaskin.html">JefRaskin</a> has coined the First Law of Computing: "No computer shall harm a user's data, or through inaction allow harm to come to a user's data." 
      </p>
      <hr/>
      <p>
        <strong>Asimov's Laws and </strong><a href="ArtificialIntelligence.html">ArtificialIntelligence</a><strong></strong>
      </p>
      <p>
        This is a good set of rules for programming any intelligent system. Actual implementation of the three laws is at least a few years away, however. -- <a href="MikeGodfrey.html">MikeGodfrey</a>
      </p>
      <p>
        It is a safe set of rules for programming an intelligent system, but it does have its problems. Asimov went into exquisite detail on these issues in <em>I, Robot</em> and a plethora of short stories. The chief problem is that the First Law forbids a robot from letting a human take normal human risks.
      </p>
      <p>
        Consider, for instance, an Asimovian chauffeur. Such a machine would not obey your order to drive you to work, because that would be putting you on the road, in harms' way. No matter how good the chauffeur is, it would put you at risk of all those incompetent drunk drivers out there. The only way to get it to drive you would be to threaten to drive yourself there (which would be much more dangerous!) Even then, it might consider physically restraining you to keep you from being on the dangerous street.
      </p>
      <ul>
        <li>
           Another story, "With Folded Hands" (by Jack Williamson), explores robots having these ethics. Removal of all risk effectively destroys the human experience.
        </li>
      </ul>
      <p>
        Part of the difficulty is that Asimovian robots are often "dumber" than humans, and thus not able to see the long-term safety of a choice (going out means making money, and poverty is a killer). However, one major lesson of the <a href="ThreeLawsOfRobotics.html">ThreeLawsOfRobotics</a> is that we humans often trade off safety for other gains.
      </p>
      <p>
        That being said, remember that Asimov wrote these stories when everyone else's ideas of robots were mechanical monsters sent to destroy humanity. These laws are great for hog-tying a potentially hostile AI.
      </p>
      <p>
        -- <a href="AnonymousDonor.html">AnonymousDonor</a>
      </p>
      <p>
        Obviously you could implement the <a href="ThreeLawsOfRobotics.html">ThreeLawsOfRobotics</a> on a sliding scale, much like any other fuzzy system. For example, telling a robot to drive you to work, and the robot knowing the risks involved in driving you to work are not outweighed by whether or not you lose your job wouldn't be too hard, with a sufficiently strong order. If these Laws were ever implemented as an <a href="ArtificialIntelligenceParadigm.html">ArtificialIntelligenceParadigm</a> they would probably be able to distinguish degrees of harm. -- <a href="MikeGodfrey.html">MikeGodfrey</a>
      </p>
      <p>
        <em>Having recently re-read </em>I, Robot<em>, I can say that Asimov did consider them a sliding scale. In "Little Lost Robot", a robot is told emphatically to "get lost", and that outweighs all but the most direct threats to human life, making it hard to find.</em>
      </p>
      <hr/>
      <p>
        <strong>Robots, Society, and Positronic Brains</strong>
      </p>
      <p>
        Asimov's laws are not courtroom-style laws but engineering or design laws for a fictional civilization with the technology to implement them.
      </p>
      <p>
        Note that the technology predicated is very advanced from today's, let alone when Asimov started writing about it. He doesn't give any clues to the processing power of the robot's brains, but says that they are not electronic but positronic; the fictional engineers have set aside the sluggish electron-based computing devices that we still use. There is a large risk that I don't think Asimov explores. A positronic brain must be interfaced at some point to machinery made of normal matter, but a single stray positron colliding with the normal matter outside of the robot's brain would liberate a large amount of energy in the form of gamma and other radiation. A robot's brain would have to incorporate considerable safeguards against accident if we wanted to avoid a nuclear-sized explosion the first time one was crushed in a collapsing building. So we can conclude that a positronic processor is considerably more powerful than a comparably-sized electronic one, or the risks would not be worthwhile. :-) Or maybe some simple way of handling anti-matter is just around the corner.
      </p>
      <p>
        A large company, US Robots and Mechanical Men, owns positronic brain technology. It appears to monopolize the design and building of robots, and has made the commendable decision to make the three laws a fundamental component of the architecture of positronic brains.
      </p>
      <p>
        It's important to realize that by the time Asimov sets the earliest stories, the three laws were essentially set in stone. Nobody could make a functioning robot without incorporating the three laws. Defective or badly designed robots were fail-safe in the sense that they would be the equivalent comatose or dead before they could act contrary to the laws.
      </p>
      <p>
        Without the three laws, robots would not be accepted by the general public, and US Robots still had to work very hard to transfer the technology from the industries such as space exploration to the domestic, such as house servants.
      </p>
      <p>
        By the way, this state of affairs is reportedly something Asimov aimed for in writing the stories. At the time he started writing about robots, science fiction was full of robots going mad, taking over the world, and so forth. Asimov reportedly wanted to write stories that were completely different and a bit less silly, perhaps.
      </p>
      <p>
        We in present-day software engineering clearly have a way to go before we can approach anything like the world of the stories. Interestingly for Microsoft watchers, US Robots seems to be a pretty powerful monopoly - I don't think anyone else make positronic brains for some time, until the spread of humanity to the stars.
      </p>
      <hr/>
      <p>
        As interesting as the topic is, it is on the other hand well known that no rigid set of rules can cover all real world eventualities effectively. In real-world (not science-fictional) AI, this is in part called the "brittleness of expert system rules". In another interesting real world area, this is called the trade-off between justice (or mercy...the two sometimes being mutually exclusive) and the rule of law.
      </p>
      <p>
        The rule of law is obviously easier to analyze. That's not to say that it can be, even in theory, made to work as an absolute. And Asimov's Robot stories explore precisely those difficulties. So Asimov's Law's are very interesting food for thought, but are not, and can not, be made into perfectly effective absolutes. -- <a href="DougMerritt.html">DougMerritt</a>
      </p>
    </div>
  </body>
</html>