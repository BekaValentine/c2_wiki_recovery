<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Wiki Slurp
      </h1>
      <p>
        <em>wiki-slurp</em> is a simple <a href="PythonLanguage.html">PythonLanguage</a> script that will save a remote wiki site as local files.  The links are translated as well. See <a href="http://www.cs.uu.nl/~hanwen/public/software/wiki-slurp.py.">http://www.cs.uu.nl/~hanwen/public/software/wiki-slurp.py.</a>
      </p>
      <p>
        <em>Why not just ask for a copy of the database?  Seems like it would cause a lot less load on the wiki server.  The admin would love you for that.</em>
      </p>
      <p>
        Next irritating question: what's the difference between slurping a normal Web site and slurping a wiki? They're both a forest of descending links, right?
      </p>
      <p>
        <em>They'd have to be if you're pulling from the outside.  It seems to me that a slurp rather than a copy of the DB would have the unfortunate result of excluding orphaned pages.</em>
      </p>
      <hr/>
      <p>
        For CategorizedRecentChanges, I wrote a script that downloaded every page on <a href="RecentChanges.html">RecentChanges</a> and then every change since the last fetch every five minutes. The bandwidth and server load it took was very high even after optimizing it. I would not recommend doing such a thing frequently. -- <a href="SunirShah.html">SunirShah</a>
      </p>
      <hr/>
      <p>
        Wiki has several stages of protection from denial of service attacks that can be and have been triggered by such programs. See <a href="WikiMirrors.html">WikiMirrors</a> for a much more site friendly approach bulk downloads. -- <a href="WardCunningham.html">WardCunningham</a>
      </p>
      <hr/>
      <p>
        I wrote wiki-slurp because it was so simple, and I needed the functionality. Its original purpose was to see how difficult it would be to convert the wiki site that I run with a close friend to local HTML. I haven't tested it in daily use.
      </p>
      <p>
        Perhaps it would be wiser to have the site run the script locally and let me mirror a .tar.gz of the Web pages.
      </p>
      <p>
        <em>Why not .tar.bz2?</em>
      </p>
      <p>
        The problem I see with copying the DB itself, is that I would have to set up and run the same <a href="WikiEngine.html">WikiEngine</a> as the original site itself (isn't that right?), which at the moment is not very convenient, and also feels like DuplicateFunctionality (or whatever it is called in the repository). -- <a href="HanWenNienhuys.html">HanWenNienhuys</a>
      </p>
    </div>
  </body>
</html>