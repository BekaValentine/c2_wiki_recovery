<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Tortoise And Hare
      </h1>
      <p>
        Nickname for "Floyd's Cycle-Finding Algorithm" invented by Robert W. Floyd in 1967; see my update to the wikipedia entry <a href="http://en.wikipedia.org/wiki/Floyd%27s_cycle-finding_algorithm.">http://en.wikipedia.org/wiki/Floyd%27s_cycle-finding_algorithm.</a>
      </p>
      <p>
        A simple technique for detecting infinite loops inside lists, symlinks, state machines, etc.
      </p>
      <p>
        For example, to detect a loop inside a linked list:
      </p>
      <ul>
        <li>
           make a pointer to the start of the list and call it the hare
        </li>
        <li>
           make a pointer to the start of the list and call it the tortoise
        </li>
        <li>
           advance the hare by two and the tortoise by one for every iteration
        </li>
        <li>
           there's a loop if the tortoise and the hare meet
        </li>
        <li>
           there's no loop if the hare reaches the end
        </li>
      </ul>
      <p>
        This algorithm is particularly interesting because it is O(n) time and O(1) space, which appears to be optimal; more obvious algorithms are O(n) space.
      </p>
      <hr/>
      <p>
        There is an alternative version in which the hare races ahead for <strong>2^n</strong> steps, then the tortoise teleports to him and <strong>n</strong> is increased by 1. More efficient (lower multiplicative constant) and only slightly more complex to code.
      </p>
      <p>
        <strong>Algorithm A, the original:</strong>
      </p>
      <code>
        tortoise = head<br/>
        rabbit = head<br/>
        length = 0<br/>
        if rabbit == null: return length<br/>
        rabbit, length = step(rabbit), length+1<br/>
        if rabbit == null: return length<br/>
        while rabbit != tortoise:<br/>
        rabbit, length = step(rabbit), length+1<br/>
        if rabbit==null: return length<br/>
        rabbit, length = step(rabbit), length+1<br/>
        if rabbit==null: return length<br/>
        tortoise = step(tortoise)<br/>
        return infinity<br/>
      </code>
      <p>
        <strong>Algorithm B, the leaping version:</strong>
      </p>
      <code>
        tortoise = head<br/>
        rabbit = head<br/>
        length,gap = 0, 0<br/>
        if rabbit == null: return "Finite list of length "+string(length)<br/>
        rabbit, length, gap = step(rabbit), length+1, gap+1<br/>
        if rabbit == null: return "Finite list of length "+string(length)<br/>
        limit = 1<br/>
        while rabbit != tortoise:<br/>
        if gap==limit: tortoise, gap, limit = rabbit, 0, limit*2<br/>
        rabbit, length, gap = step(rabbit), length+1, gap+1<br/>
        if rabbit==null: return "Finite list of length "+string(length)<br/>
        return "List with loop of size "+string(gap)<br/>
      </code>
      <p>
        In each, <strong>length</strong> is how long the list is so far as traversed by <strong>rabbit</strong>. In the first, if <strong>rabbit</strong> takes <strong>2n</strong> steps, <strong>tortoise</strong> takes <strong>n</strong> steps. In the second <strong>tortoise</strong> never takes any steps. In linked lists stepping to the next node isn't expensive, but this loop finding algorithm can be used in places where the list is implicit (such as the Pollard Rho and Elliptic Curve methods of factoring integers) and stepping is expensive. Algorithm B then wins by a substantial margin.
      </p>
      <p>
        Here's the complete analysis:
      </p>
      <p>
        Suppose there is no loop and the list is of length <strong>2n</strong>. Then algorithm A takes <strong>3n</strong> steps, and algorithm B takes <strong>2n</strong> steps. Now suppose there is a loop, and let <strong>t</strong> be the number of steps taken to reach the loop and let <strong>p</strong> be the size of the loop.
      </p>
      <p>
        Algorithm A:
      </p>
      <ul>
        <li>
           If <strong>t<=p</strong> then <strong>steps_A = 3p</strong>
        </li>
        <li>
           If <strong>t>=p</strong> then let <strong>d</strong> be such that <strong>d+t=k*p</strong>. Note that <strong>0<=d<p</strong>.
        </li>
        <li>
           Then <strong>steps_A = 3t + 3d</strong>, so that <strong>3t <= steps_A <= 3(t+p-1)</strong>
        </li>
      </ul>
      <p>
        Algorithm B:
      </p>
      <ul>
        <li>
           If <strong>t<=p</strong> then let <strong>P=2^n</strong> such that <strong>P <= p < 2P</strong>, and let <strong>e=2P-p</strong>, so that <strong>e<P<=p</strong>.
        </li>
        <li>
           Then <strong>steps_B = 2P + p = 2p+e <= 3p = steps_A</strong>
        </li>
      </ul>
      <ul>
        <li>
           If <strong>t>=p</strong> then let <strong>Q=2^m</strong> such that <strong>Q <= t < 2Q</strong>, and let <strong>f=2Q-t</strong>, so that <strong>f<Q<=t</strong>.
        </li>
        <li>
           Then <strong>time_B = 2Q+p = t+p+f <= 3t <= steps_A</strong>
        </li>
      </ul>
      <p>
        Therefore algorithm B never takes more steps than algorithm A and almost always takes fewer. Algorithm B is significantly harder to analyse, and should perhaps be introduced second in a teaching context.
      </p>
      <p>
        A comment about the tortoise in the second version: copying the rabbit's position into the tortoise (<em>aka</em> teleporting the tortoise to the rabbit) is about the same complexity as following a link in a linked list. However, the teleportation is <em>far</em> less expensive than a step in the context of integer factoring algorithms.
      </p>
      <p>
        <em>It occurs to me that I don't see a rationale for doubling as being the most efficient limit change. Assuming a multiplicative change, at least, is most efficient, then a heuristic argument would be that multiplying by e would be best, since infinity is too much and (1 + infinitesimal) is too small, and e is the usual minimum of such things.</em>
      </p>
      <ul>
        <li>
           Do you mean <em>e</em> as in the number whose natural logarithm is 1?  <strong>Of course.</strong> One wonders what a graph of depth <em>e</em> would like like.  Perhaps one could do it in <a href="CeePlusPlus.html">CeePlusPlus</a> with a few <a href="WildPointer.html">WildPointer</a>s......
        </li>
      </ul>
      <ul>
        <li>
           The constant e arises as a limit of an infinite process, naturally; any finite subsequence of such a process would involve a rational approximation to e rather than e itself. Thus a discrete data structure with depth 2 or depth 3 is such an approximation. Since we're talking about iteration, more exactly we're talking about approximating powers of e, such as e^2 ~= 7, e^3 ~= 20, e^4 ~= 55, etc, whereas the original algorithm was based on powers of two, 2^2 = 4, 2^3 = 8, 2^4 = 16, etc.
        </li>
      </ul>
      <ul>
        <li>
           Another option is to have more than two animals in the race.  Imagine a cheetah, a hare, a cat, a penguin, and a tortoise (traversing each at different speeds, in order to maximize the chances of a speedy meeting).  Any meeting between any two critters would signify a loop.  Analysis of this algorithm is left as an exercise to the reader.  :)
        </li>
      </ul>
      <ul>
        <li>
           That's similar to what I'm talking about, since the overhead increases as the number of animals increases, yet the chances of a meeting also increase. An infinite number of animals would have infinite overhead, so that's too much...
        </li>
      </ul>
      <p>
        Do the analysis. I've done the *2 case, surely you can work out what gives the minimum expected time.
      </p>
      <p>
        <em>That would be fair. But you're probably the better mathematician of us two, and it is typical of me to be lazy like this: to offer a heuristic motivation rather than an actual proof. :-) (This is undoubtedly how I ended up in the commercial world rather than academia. I do appreciate the importance of proofs, but I tend to suffer from the </em><a href="FrameProblem.html">FrameProblem</a>: getting tangled up in possible issues that an expert mathematician knows can simply be ignored.)<em></em>
      </p>
      <p>
        Well, if <strong>t</strong> is really small then we want the multiplier to be really big, and if <strong>t</strong> is really big then we want the multiplier to be really small. It depends on the assumptions you make about how the cycles have arisen, and therefore what the likely relationship is. For practical purposes a multiplier of 2 would seem a very good choice.
      </p>
      <ul>
        <li>
           Sure. The point I was trying to make is, what if you have no idea about the likely relationships? I basically was speculating that the limit tends towards <em>e</em> (yes, 2.718281828459045..., <strong>that</strong> <em>e</em>) over all random problems of arbitrary tail and cycle lengths. Now that I've said it more explicitly, let me modify that speculation to <strong>exp(N)</strong> in the number of nodes. This is just because <em>e</em> or <em>exp(N)</em> is very frequently involved in such limits, for instance, in a very precise sense, <em>e</em> is the optimum number base. But I haven't thought about the proof in the intervening months, I suppose I should. Or maybe doing some Monte Carlo runs would be more fun. :-)
        </li>
      </ul>
      <hr/>
      <p>
        Is there an analog to this algorithm for detecting cycles in an arbitrary graph?
      </p>
      <p>
        <em>If you have a deterministic algorithm for iterating through the nodes in your graph then just follow it and you'll find a cycle or a dead-end. If you find a dead-end you don't know that there are no cycles. If your graph can't be iterated in a deterministic manner then I think you're out of luck.</em>
      </p>
      <p>
        <em>You can always find cycles by taking the n-th power of the adjacency matrix and looking on the diagonal.</em>
      </p>
      <p>
        Are dead-ends a fundamental problem - wouldn't backtracking (e.g. depth-first) iteration avoid them? Are we talking about directed or undirected graphs?
      </p>
      <p>
        <em>The entire point of the algorithm on this page is that it doesn't use back-tracking. The question was whether this algorithm works. There are other algorithms, including some which back-track, but that wasn't the question. When you ask a question like that I no longer know how to be helpful. You either don't understand the algorithm or you're asking a different question entirely.</em>
      </p>
      <p>
        Sorry, I interpreted "analog" differently. The depth-first graph version seems analogous to me since it finds cycles in a data structure by "racing" to see if a hare overtakes a tortoise. At O(n) time and space it is still an efficient algorithm, but not quite as good as the original.
      </p>
      <p>
        <em>I can't think of a deterministic O(1) space algorithm to find cycles in graphs. I'd be interested to see one.</em>
      </p>
      <ul>
        <li>
           [I'm not the one who asked about an "analog" and I'm not sure where that question was going, FYI] On the other hand, the modification of Floyd's Cycle Finding Algorithm to random graphs is much closer to being deterministic and O(1) space than it is to being nondeterministic and O(lg N) space -- the nondeterminism tends to be sharply limited (effectively bounded by a constant) for all random graphs with average connectivity at or above the "network effect" level. It's true that, below that level, fully- but sparsely- connected graphs may be problematic in this regard...say, a graph consisting of nothing but many, many dead-ending cycle-free subtrees all connected at a single root. However such graphs are rare in most problem domains (and certainly are rare across random graphs). Then again, if what you've got are deep trees and you want to prove there are no cycles, that's a problem, yep. Knuth addresses this in O(1) space, note...it requires pointer arithmetic! Ha. Algorithmic efficiency growing out of manure; purists beware.
        </li>
        <li>
           However, let's re-frame the problem for clarity. <strong>First</strong> there is an issue of traversing the graph, and only <strong>second</strong> is there the further problem of finding cycles. If it is impossible to traverse the graph in O(1) space, then obviously you can't solve the whole problem in O(1) space. So let's just assume you have a traversal method that is O(1), without regard for its nature. Floyd cycle-finding can then be implemented on top of that (or any) traversal method in O(1) <em>additional</em>' space, just by traversing (somehow, anyway) at both half speed and full speed, and comparing the two positions. The cycle-finding is independent of the traversal method. (Unless, of course, traversal requires destroying the graph during the first traversal, such that there can be no second traversal, or at least, not until the first traversal finishes -- which is not hypothetical, again see Knuth). -- dm
        </li>
      </ul>
      <hr/>
      <p>
        <a href="CategoryAlgorithm.html">CategoryAlgorithm</a>
      </p>
    </div>
  </body>
</html>