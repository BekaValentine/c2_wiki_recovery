<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        What Is Information
      </h1>
      <p>
        There seem to be two strains of thought regarding 'information'. One is the ScientificField definition. the other is the <a href="InformaticsField.html">InformaticsField</a> definition.
      </p>
      <hr/>
      <hr/>
      <p>
        <strong></strong>ScientificField definition of information discussion<strong></strong>
      </p>
      <p>
        (from <a href="DefinitionOfLife.html">DefinitionOfLife</a>, which makes <a href="BioInformatics.html">BioInformatics</a> especially relevant)
      </p>
      <p>
        <strong>Meaning of information:</strong>
      </p>
      <p>
        <em>is "structure" to be understood as equivalent to "information content"?</em>
      </p>
      <p>
        physical information technically includes both local information and global information. Most local information is entropy, and most information is local information.
      </p>
      <ul>
        <li>
           technically speaking, locality refers to compact support. Analytic functions are non-compact; they are nonzero over an infinite range. Analytic functions which are known entirely over any finite range are also known over their entire infinite range. Conversely, any function which cannot be globally known from its local behavior is non-analytic. That is, only non-analytic functions can surprise you by crashing down on your head; analytic functions warn you ahead of time.
        </li>
      </ul>
      <p>
        So a system which maximizes physical information is maximizing entropy. Needless to say, that's not the desired meaning of information. Most people understand information as excluding entropy, as referring only to global information. Structure is a subset of that information. So the definition does <em>not</em> use information in the sense of Shannon's theory but more in its everyday sense. Perhaps logical information? So there you go, you can either interpret structure very broadly or information very narrowly.
      </p>
      <p>
        Would 'actively maintains its logical informational content' also apply to a computer? -- <a href="RobHarwood.html">RobHarwood</a> (honest question)
      </p>
      <p>
        Heh, you've just proved something I only suspected, that 'structure' is a strict subset of the everyday notion of information. The everyday concept of information encompasses both the physical structure of computers (which computers do not maintain) and their logical contents (which they do maintain). yet excludes entropy. Unfortunately, 'information' can get confused with physical information. Maybe order?
      </p>
      <p>
        <em>tangential comment: Yes, it's exactly true that (most) computers "actively maintain" their logical contents. Especially bits stored in DRAM, which fade away unless periodically refreshed many times per second.</em>
      </p>
      <p>
        <em>Sorry, but what exactly is that you are speaking about? What is "information content", "structure" or "internal order"? How do you suppose to reach an agreement about meanings of these words?</em>
      </p>
      <p>
        ...
      </p>
      <p>
        Information is a word with many meanings. There is high-level information (like in a crystal) and low-level information (usually called entropy). Too much information overloads people's signal processing (human brains) and so gets perceived as <strong>no</strong> information. A structure is an example of high-level information and entails lesser levels of low-level information. Entropy is the dominant part of low-level information and is intuitively conceived as disorder.
      </p>
      <p>
        The complications come from having more than two levels of information. In a computer, you have a lowest level made up of atoms and molecules, you have a higher level made up of wires, and you have a highest level made up of bits and logic switches. In this case, structure refers to the physical structure (middle level), information content refers to the highest level, and internal order refers to both higher levels. The lowest level of physical information is mostly heat which isn't conceived as information in the general sense of the word. -- <a href="RichardKulisz.html">RichardKulisz</a>
      </p>
      <hr/>
      <p>
        Any new and meaningfull signal is information. --<a href="HarryVanDerVelde.html">HarryVanDerVelde</a>
      </p>
      <p>
        Information is surprising.  The information in a signal can be measured by how unexpected (difficult to predict) it is.
      </p>
      <hr/>
      <p>
        I strongly recommend reading up on <a href="AlgorithmicInformationTheory.html">AlgorithmicInformationTheory</a>.  -- <a href="LucasAckerman.html">LucasAckerman</a>
      </p>
      <hr/>
      <hr/>
      <p>
        <strong></strong><a href="InformaticsField.html">InformaticsField</a> definition of information discussion<strong></strong>
      </p>
      <p>
        See:
        <a href="BioInformatics.html">BioInformatics</a>,
        <a href="InformationAsCurrentAnalogy.html">InformationAsCurrentAnalogy</a>,
        <a href="KnowledgeAndInformation.html">KnowledgeAndInformation</a>,
        <a href="TheFutureOfInformation.html">TheFutureOfInformation</a>,
        <a href="TheSocialLifeOfInformation.html">TheSocialLifeOfInformation</a>,
        <a href="ClaudeShannon.html">ClaudeShannon</a>
      </p>
      <p>
        (is <a href="OnlySearchableInformationExists.html">OnlySearchableInformationExists</a> relevant ?)
      </p>
      <p>
        (<a href="EditHint.html">EditHint</a>: should this page be re-named "DefinitionOfInformation" ?)
      </p>
      <hr/>
      <p>
        Moved from <a href="InformationVsData.html">InformationVsData</a>:
      </p>
      <p>
        Information is what people can understand. Knowledge is what they do understand. Information is the primary subject for Informatics and <a href="InformationScience.html">InformationScience</a>. 
      </p>
      <p>
        "Data is more like a gif. Information is more like a jpg."
      </p>
      <p>
        <em>I'm afraid I don't understand. Please explain?</em>
      </p>
      <p>
        <em>Isn't "information" just data that makes sense?</em>
      </p>
      <p>
        I completely agree, my own personal definition of information is that it is 'data that means something to people'. This is why it is so hard to define. This is because everyone has a different cognitive makeup and data that some people's brains can handle can not be handled by other people's brains. See <a href="BusinessTalentEndemeSet.html">BusinessTalentEndemeSet</a> for a breakdown of cognitive types as they apply to business.
      </p>
      <hr/>
      <p>
        <em>But if "information" is data that makes sense -- and we know human cognition is the "makes sense" part; it's not something inherent in the data -- then why would information programming differ from data programming?</em>
      </p>
      <p>
        This is a brilliant question. It is brilliant because I do not know the answer to it. I have years of experience related to this question, and I can intuit when programming is data oriented and when programming is information oriented 90% of the time. But I do not have a clear conversion of my intuition into a precise statement. One of the reasons why I am conversing here is to try to get at the heart of the answer to your question. I once wrote an article on the difference between data and information and I really really need to get that posted here so that we have something more to chew on.
      </p>
      <p>
        <em>I might be able to help you. Information got a very quantitative definition with </em><a href="ClaudeShannon.html">ClaudeShannon</a> and <a href="InformationTheory.html">InformationTheory</a>. Unfortunately for us English speakers, its precise definition is what you're essentially calling "data". So it's kind of a three-layered phenomenon. The field has clarified this ambiguity by referring to this former definition of information as "Shannon-Weaver Information", or simply <a href="ShannonInformation.html">ShannonInformation</a>.<em></em>
      </p>
      <p>
        <em>Yes, please post it.</em>
      </p>
      <p>
        {If something has an obvious and stable category(s) to it, then it's probably safe to rely on that for design decisions. If not, it's best <em>not to force categories</em> in my experience. Ambiguity or instability in category is a sign that it will be subject to change or multiple uses. Don't pin it down into something too specific if you don't have to. - t}
      </p>
      <hr/>
      <p>
        <a href="CategoryInformation.html">CategoryInformation</a>
      </p>
    </div>
  </body>
</html>