<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Software Development Improvement Paradigm Shift
      </h1>
      <p>
        (from <a href="TimeForaParadigmShift.html">TimeForaParadigmShift</a>)
      </p>
      <p>
        <strong>A simple faith:</strong>
        Science has a simple faith, which transcends utility. Nearly all men of science, all men of learning for that matter, and men of simple ways too, have it in some form and in some degree. It is the faith that it is the privilege of man to learn to understand, and that this is his mission. If we abandon that mission under stress we shall abandon it forever, for stress will not cease. Knowledge for the sake of understanding, not merely to prevail, that is the essence of our being. None can define its limits, or set its ultimate boundaries.
        <em>Vannevar Bush (1890 - 1974), U.S. electrical engineer, physicist. Science Is Not Enough, The Search for Understanding (1967).</em>
      </p>
      <p>
        <strong>Upon the shoulders:</strong>
        In science men have discovered an activity of the very highest value in which they are no longer, as in art, dependent for progress upon the appearance of continually greater genius, for in science the successors stand upon the shoulders of their predecessors; where one man of supreme genius has invented a method, a thousand lesser men can apply it.
        <em>Bertrand Russell (1872 - 1970), British philosopher, mathematician. A Free Man's Worship and Other Essays, ch. 3 (1976).</em>
      </p>
      <p>
        What if it was not the field of software development, but the field of <em>software development improvement</em> that needed a paradigm shift?
      </p>
      <p>
        I know of two ways to improve software development tools and practices:
      </p>
      <ol>
        <li>
           Controlled, repeatable experiments.
        </li>
        <li>
           Trial and error.
        </li>
      </ol>
      <p>
        In my opinion, neither one works all that well for improving software development (e.g. compared to hardware development, which improves by leaps and bounds). What if there were a better, third way to improve software development? Something like controlled trial and error? -- <a href="StanSilver.html">StanSilver</a>
      </p>
      <hr/>
      <p>
        <strong>Pro experiment comments</strong>
      </p>
      <hr/>
      <p>
        I like your use of the words "software development improvement" it expresses more correctly the point I am trying to make with this page. The whole way of thinking about improving the way we use computers and how we think about and acquire new knowledge. The extreme concept of <a href="WorkingInPairs.html">WorkingInPairs</a> is a form of collaboration, which I believe is a powerful concept, perhaps the most powerful of the extreme community. 
      </p>
      <p>
        "To do experiments to determine how best to do this" - good suggestion - some steps may be underway - Would you consider the work in <strong></strong><a href="ExtremeProgramming.html">ExtremeProgramming</a><strong> a software development improvement experiment of sorts?</strong>
      </p>
      <p>
        The existence and use of this Wiki - could this also be an experiment in communication and collaboration? While neither may fit a laboratory definition of an experiment, they may be in the broadest sense be "experiments". 
      </p>
      <p>
        Certainly more directed and specific experiments are in order, but what form could they take? Who would do them, and who would pay for them? 
      </p>
      <hr/>
      <p>
        Stan, I think you have some common misconceptions about what scientific experiments are for. First off, I believe your above statements form a false dichotomy; your first point addresses experimental method, your second addresses one of the goals of research - all research is 'trying new things' to some degree. We do controlled, repeatable experiments because it is orders of magnitude more effective than anything else. However, experiments don't 'prove' anything, this is not the purpose of experiment. Without sharing of results, there is no science. A move away from good experimental method will just mean that you will be much less likely to find true relationships: you may end up with effective models, you may not --- but you won't know why. If you refrain from sharing conclusions, you are making a step back into the pre-scientific era of knowledge, not a step forward.
      </p>
      <hr/>
      <p>
        In my opinion, the failure to improve software development practices by leaps and bounds is primarily due to a single cause: Trying to engineer without sufficient underlying theory. I don't believe that your 'third way' exists; systemic methods of knowledge improvement will always require reproducibility, and control is necessary for any successful experiment. But we are perhaps focusing too much on the word 'experiment'.
      </p>
      <p>
        It seems that many view C.S. as a branch of mathematics - no experimentation needed. I believe this is false; that some amount of experiment is needed - however, the vast majority of practitioners don't know how to conduct an experiment properly. On the other hand, we have the 'Software as an engineering discipline' camp - who often seem completely oblivious to the massive gaps in their underlying knowledge. Some of these people have "cargo-cult engineering" stamped all over them: pretending that process alone will get you over gaps in fundamental knowledge.
      </p>
      <p>
        So we have two problems. On one side, we have people pretending to be engineers in the same sense that a mechanical engineer is, for example - but this is not possible today. That doesn't mean there are not valuable lessons and techniques to be learned from the engineers, but they must be coupled with an understanding of what makes engineering work (and when it fails).
      </p>
      <p>
        On the other side, we have a computer science community. There are huge gaps in the understanding of C.S.; these need to be delineated and communicated to the outside world. There are many areas of current C.S. that demand experiment, but many CS people either don't believe this, or do it in a half-baked way, never learning how to do this properly (e.g., it isn't taught in many CS graduate programs, as it would be in say Physics). Many people with little formal CS training (or perhaps a bachelors) end up trying to conduct experiments where they see the need, but they often don't know how to go about it either. Furthermore, CS people have to start doing science again. In north america at least, many of them have stopped. The influx of university-as-a-business and similarly broken ideas, plus the waxing of IP laws has lead researchers to play their hands 'close to the chest'. Every time I read a 'peer reviewed' article that does not contain enough information to reproduce the work, I shake my head. How can someone claim to have 'reviewed' it? In mathematics, the idea of leaving out details like this would be laughable.
      </p>
      <p>
        Bottom line: much of the area I think you are covering with the statement 'where they don't apply' has a much simpler explanation: the work has never been attempted.
      </p>
      <hr/>
      <p>
        <strong>Pro trial and error or third way comments</strong>
      </p>
      <hr/>
      <p>
        You may want to look into the science of Sociology. A lot of Sociology is performed without <em>controlled</em> experiments, but rather by systematically observing social behaviour -- and using statistics to make and test hypotheses. Indeed, much of traditional software engineering research uses the methods of Sociology (e.g. the CMM, <a href="WattsHumphrey.html">WattsHumphrey</a>, and people of that ilk).
      </p>
      <p>
        On the other hand, I don't think the paradigm shift here is in the methods of improving software development, but in the underlying assumptions about <em>what is </em>'wrong<em>' with software development</em>, and hence, what needs to be improved. Traditional methodologists assume that since the cost of change increases exponentially over time, then we need to control change. XP pulls out the Aikido approach and says that instead we need to be able to <em>roll with</em> change as efficiently as possible.
      </p>
      <hr/>
      <p>
        Let me jump in and make a few comments.
      </p>
      <p>
        First, I am not sure "controlled, repeatable" experiments are valuable in software. Once you make the experiment controlled and repeatable, it is no longer possible to generalize the result. To make a statistically significant measurement requires a large amount of tests. In trying to recall my college statistics, I believe 20 - 30 test results each for the control and each variation. To be repeatable, you must have a fixed population of people to draw from and a single or fixed set of software programs to run the test with. To be feasible to complete, you are required to test single person programs that can be completed in a couple of days at most. Will these results provide insights into multiple person development which goes on for years?
      </p>
      <p>
        Second, most talk about software development improvement is more focused on improving the management of software development and rarely discusses actual improvement of software development.
      </p>
      <p>
        Any thoughts?
      </p>
      <p>
        <a href="WayneMack.html">WayneMack</a>
      </p>
      <hr/>
      <p>
        In "controlled, repeatable" experiments, the degree of control is tailored to the situation. Think of how little you have to control to demonstrate that the boiling point of water is 100 degrees C at sea level. Is the reaction to "controlled, repeatable" on this page coming out of an assumption of over-control, I wonder?
      </p>
      <p>
        <em>Look at what is restricted in this experiment. Atmospheric pressure (sea level) and presumably the volume for expansion (near infinite) and the purity of the water, if you get 100 degrees C as the result. The boiling point of pure water is a complex function and when you start throwing in impurities, the actual boiling point for a sample of water is almost impossible to predict. Using 100 degrees C as the boiling point of water is a useful approximation in many instances, but is not helpful when cooking in mountain tops or in pressure cookers.</em>
      </p>
      <p>
        <em>To relate this back to software, it is not just a matter of creating a "controlled, repeatable" experiment, but the degree to which the results of the experiment can be generalized. Results of the experiment may not apply outside the bounds of the experiment.</em>
      </p>
      <p>
        Precisely, and yet too precisely at the same time. And hence the problem, and so the answer to my question, apparently, is YES.
      </p>
      <p>
        The boiling point of water is presumably a continuous function of its (continuous) variables (the level of each type of impurity, the volume for expansion, the atmospheric pressure etc.) so that with a few controlled or at least closely monitored/recorded experiments we can then use the results to predict the results in a range of cases between (via interpolation) and beyond (via extrapolation) those tested in the experiments.
      </p>
      <p>
        Software (and Software Development Processes) is more complex and its important characteristics (performance, availability; productivity, quality) are functions of huge numbers of variables, the variables may be discrete rather than continuous, and the characteristic functions may not be continuous over those variables.
      </p>
      <p>
        This makes interpolation harder, and extrapolation much harder. 
      </p>
      <hr/>
      <p>
        This entire page is founded on an unspoken premise: That the software development process is a repeatable, improvable discipline (i.e. say, like engineering).
      </p>
      <p>
        If you think that what we do is more akin to writing great novels, or coming up with new proofs in mathematics, or tending a garden and helping it grow in the most aesthetically pleasing direction, then all this talk of "controlled experiments" immediately starts to sound like nonsense.
      </p>
      <p>
        <strong>DO</strong> you believe that the software process is repeatable and improvable? I suspect not, but I'd be happy to discuss this point.
      </p>
      <p>
        -- <a href="AlainPicard.html">AlainPicard</a> (p.s. I was a professional scientist for quite a while, which is why this page interests me. Now my business card says "system architect", and my bosses think I'm a "software engineer", but I don't really know what it is I do on a day to day basis. :-)
      </p>
      <dl>
        <dt> </dt>
        <dd><strong>Yes</strong>, the software process is repeatable and improvable. This is implicit in the concept of experience. What is unclear is what are the key aspects of software development and how do we teach them. -- <a href="WayneMack.html">WayneMack</a></dd>
      </dl>
      <dl>
        <dt> </dt>
        <dd><strong>No</strong>, I do not believe the software development process is repeatable and improvable in the same way a physical system or manufacturing system is. <strong>But</strong>, I believe it is repeatable in some other way that we do not yet know how to work with. We use the same set of tools on each project; we just use them in a different way each time. So our tool set should be able to be systematically improved - I just don't know how to express what is recurring or how to improve it yet. Hence, this page. -- <a href="StanSilver.html">StanSilver</a></dd>
      </dl>
      <dl>
        <dt> </dt>
        <dd><em>Are you disagreeing with the "repeatable and improvable" concept or the analogy with a "manufacturing system?" -- Wayne</em></dd>
      </dl>
      <dl>
        <dt> </dt>
        <dd><em>The latter. Software projects do not repeat like a manufacturing assembly line. Software projects do not repeat like a reproducible experiment. But something about them repeats. So I agree with both Alain and you. Maybe we need a word other than "repeat" that means "that which recurs, and how it recurs, in every software project". I am pretty sure it is not a sequence of steps that recurs, like a recipe. It is more like the same problems and same solutions recur, but in different sequences and different guises. -- Stan</em></dd>
      </dl>
      <dl>
        <dt> </dt>
        <dd>This can be posed as a question: <a href="WhatRecursEverySoftwareProject.html">WhatRecursEverySoftwareProject</a>?</dd>
      </dl>
      <p>
        <em>See also: </em><a href="ProgrammingAintManufacturing.html">ProgrammingAintManufacturing</a><em></em>
      </p>
      <hr/>
      <p>
        <strong>Other comments</strong>
      </p>
      <hr/>
      <p>
        I still haven't digested all of the above, so I don't have anything intelligent to say about it yet - but the bit about "standing on the shoulders" moved me to speak:
      </p>
      <p>
        In software, we tend to repeatedly build on top of the systems produced by others - who, in turn, built their systems on top of previous systems. We think we get incremental improvement this way.
      </p>
      <p>
        Radical computer scientist <a href="ChuckMoore.html">ChuckMoore</a> would disagree; he'd say we get horribly slow, horribly wasteful, horribly complicated systems this way.
      </p>
      <p>
        The important "shoulders" to stand on are not the programs written by our predecessors, but the knowledge gained from those systems. And sometimes that knowledge should lead us to toss those systems overboard and start anew. (Moore seems to <em>always</em> start anew; <a href="AlanKay.html">AlanKay</a> is wont to <a href="BurnTheDiskpacks.html">BurnTheDiskpacks</a>.)
      </p>
      <p>
        If "<a href="BuildingOnlyUpward.html">BuildingOnlyUpward</a>" is a paradigm of current software development, maybe we need to shift to a <a href="CreativeDestruction.html">CreativeDestruction</a> paradigm.
      </p>
      <p>
        -- <a href="GeorgePaci.html">GeorgePaci</a>
      </p>
      <hr/>
      <p>
        George, you might be interested in the book <a href="TheStructureOfScientificRevolutions.html">TheStructureOfScientificRevolutions</a> by <a href="ThomasKuhn.html">ThomasKuhn</a>. Kuhn talks about "normal science" and "revolutionary science" (<a href="PleaseComment.html">PleaseComment</a>: I'm not sure if these are the exact words he uses). Normal science is the <a href="BuildingOnlyUpward.html">BuildingOnlyUpward</a> phase where people use the understanding of what is known to see how far they can push it. Revolutionary science starts to happen when the younger crowd starts to see inconsistencies. They start to come up with new theories to explain the inconsistencies, and when a good one is found, the <a href="KuhnParadigmShift.html">KuhnParadigmShift</a> begins. This seems to be analogous to your <a href="CreativeDestruction.html">CreativeDestruction</a> phase.
      </p>
      <hr/>
      <p>
        Perhaps the majority of this page could be moved to a place for discussing the nature and merits of the scientific method and experimental techniques? Interesting topic in and of itself, but slightly divergent from a discussion of what a paradigm shift might look like. -- <a href="StevenNewton.html">StevenNewton</a>
      </p>
      <hr/>
      <p>
        A <a href="KuhnParadigmShift.html">KuhnParadigmShift</a> starts with a phase (normal science) where its mainstream theory is explored in enough detail that its flaws start to become apparent. The established scientists try to explain the flaws by making slight modifications or additions to the established theory, but because the flaws are fundamental to the theory, this doesn't work. I think this is what Stan is trying to point out with this page. Because of the nature of software development, improvement efforts based on the old models of process improvement are fundamentally flawed. Hence the need for (or rather, the impending probability of) a paradigm shift in software development improvement.
      </p>
      <hr/>
      <p>
        Paradigm Shifts (Please add to the list)
      </p>
      <ol>
        <li>
           Software development is an ongoing process, not a single bounded project or even a sequence of bounded projects. Think of a 10 year time frame, not a 2 year or less time frame.
        </li>
      </ol>
      <ol>
        <li>
           Software is a simulation of a fuzzy human process. It will never be a perfect model (i.e., no perfect software), just an ever improving approximation. It will always require users to adapt their ways of working, to some degree. This also implies there are no software requirements, only an improving understanding of the process.
        </li>
      </ol>
      <ol>
        <li>
           Software development improvements start to happen systematically and methodically, instead of randomly and magically by some smart person coming down off a mountain with new words of wisdom (like <a href="AlanKay.html">AlanKay</a> and Smalltalk, or <a href="KentBeck.html">KentBeck</a> and XP, or ...). How the heck did <a href="AlanKay.html">AlanKay</a> and <a href="KentBeck.html">KentBeck</a> do what they did? Was it magic? Or can mortals duplicate what they did? How?
        </li>
      </ol>
      <ol>
        <li>
           Software development does not exist, so it cannot be improved:  it is analogous to Chemistry.  No offense, guys, you're doing a great job (chemists and software developers alike).  Is it not true, though, that very little of what chemists do is Chemistry and/or very little of the benefit Chemistry provides is actually provided by chemists (still less by chemists doing Chemistry)?
        </li>
      </ol>
      <hr/>
      <p>
        Hmmm. The first two proposed paradigm shifts seem to be wisdom gained from years of coding that they don't teach in school. In other words, the newbies repeat the same mistakes over and over until they, too, gain wisdom. So a question the third proposed paradigm shift brings up might be how do we <a href="CodifyAcquiredSoftwareWisdom.html">CodifyAcquiredSoftwareWisdom</a>?
      </p>
      <p>
        I would question the third item. I don't believe you can have a systematic and methodical paradigm shift. Yes, we need systematic and methodic improvements, but major shifts do tend to come "magically" from an outside source.
      </p>
      <hr/>
      <p>
        <a href="CategoryPhilosophy.html">CategoryPhilosophy</a>
      </p>
    </div>
  </body>
</html>