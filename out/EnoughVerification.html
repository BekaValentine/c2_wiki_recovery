<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Enough Verification
      </h1>
      <p>
        I feel there was a lot in common with the way I do software, and some of the descriptions of engineering here. I do have a theory, at least for the components. I can reason about the behaviour of a stack. I use <a href="AbstractDataType.html">AbstractDataType</a>s. I use LoopInvariants and LoopVariants - generally informally, but they could be made formal. So my software is more theory-based than you might think. Contrariwise, engineering is less theory-based than you might think.
      </p>
      <p>
        This touches on the major software schism between static analysis and dynamic analysis. XP is very much on the dynamic side. XP'ers seem to prefer testing over theory, doing over thinking, dynamically checked languages like Smalltalk over statically checked languages like Eiffel, <a href="UnitTest.html">UnitTest</a>s over <a href="DesignByContract.html">DesignByContract</a>. Personally I lean towards the static side, and I occasionally have to readjust my expectations when interacting with this Wiki because of it.
      </p>
      <p>
        For me it is not really beautiful unless I understand it. It's not enough for me to know that something works: I want to know why and when, and under what circumstances it can fail. The program reached this breakpoint - will it always do so, or might it sometimes go down another path? It passed a <a href="UnitTest.html">UnitTest</a> but that only exercised a finite number of cases - what about all the others?
      </p>
      <p>
        -- <a href="DaveHarris.html">DaveHarris</a>
      </p>
      <hr/>
      <p>
        I don't agree with the static analysis/dynamic analysis distinction. You could do a project extreme and write assertions out the wazoo. I read a paper <a href="BertrandMeyer.html">BertrandMeyer</a> wrote a couple of years ago where he talks about a <a href="UnitTest.html">UnitTest</a> discipline indistinguishable from XPs.
      </p>
      <p>
        What would make a project not extreme is if you did more assertion checking than you needed, or if you let the presence of the assertions keep you from refactoring. How hard is it to radically restructure code with lots of assertions?
      </p>
      <p>
        -- <a href="KentBeck.html">KentBeck</a>
      </p>
      <hr/>
      <p>
        Static versus Dynamic is not either/or - you can have both. But IMHO the tension between them is one of the great themes of software.
      </p>
      <p>
        I think assertion tools could benefit from some work if they are to be integrated with XP. For example, they could do with being propagated from callee routines to caller. This is similar to static type-checking. I would expect a statically typed system based on type inference to be easier to refactor than one based on static but manifest types. (See <a href="TypeChecking.html">TypeChecking</a> for a discussion of terms.)
      </p>
      <p>
        Apart from that, when you restructure code you have to review the assertions. Some of them will survive the restructuring and, like <a href="UnitTest.html">UnitTest</a>s, help ensure that the final code still has the original expected properties. This is especially true of the assertions <strong>outside</strong> the classes being restructured.
      </p>
      <p>
        Other assertions may refer to properties that the restructuring has removed, and must be changed or deleted. In fact that's one motivation for refactoring: some assertions, like some comments, are a hint that your code is wrong and should be rewritten to eliminate them.
      </p>
      <p>
        I'm not sure about "more assertion checking than you needed". Assertions test for impossible conditions. I am always tempted to leave them out on the grounds that they are obvious, or that they can never fail. And sometimes (if I put them in anyway), these are exactly the ones that do fail. 50% of the money spent on advertising is wasted, but we don't know which 50%.
      </p>
      <p>
        -- <a href="DaveHarris.html">DaveHarris</a>
      </p>
      <hr/>
      <p>
        If you are using <a href="TestFirstDesign.html">TestFirstDesign</a>, the <a href="ProgrammerTest.html">ProgrammerTest</a>s define what is <a href="EnoughVerification.html">EnoughVerification</a>.  Code is only added to pass a test, implying there should be no untested code.  The code may have characteristics not tested, but these characteristics can be viewed as implementation details only.  
      </p>
      <p>
        If, sometime later, these implementation details become significant, then a decision needs to be made on how to address it.  You may have to split a class into two or more that support different results for a characteristic.  For example, you may have to split a date class into a class that handles complete dates and a class that handles partial dates such as year and month only.  <a href="ProgrammerTest.html">ProgrammerTest</a>s would also be added to explicitly state what the desired operation is to be.
      </p>
      <p>
        <a href="EnoughVerification.html">EnoughVerification</a> is determined by when you stop writing <a href="ProgrammerTest.html">ProgrammerTest</a>s and modifying code to support them.  It is a subjective decision and the decision can be revisited to add additional verification or occasionally prune unnecessary verification.
      </p>
      <p>
        -- <a href="WayneMack.html">WayneMack</a>
        See also <a href="UnitTestingIsDesign.html">UnitTestingIsDesign</a>
      </p>
    </div>
  </body>
</html>