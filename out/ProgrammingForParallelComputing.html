<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Programming For Parallel Computing
      </h1>
      <p>
        <em></em><a href="ProgrammingForParallelComputing.html">ProgrammingForParallelComputing</a> proposed new name for much of what I had put on <a href="ParallelProgramming.html">ParallelProgramming</a>.<em> -- </em><a href="JohnFletcher.html">JohnFletcher</a>
      </p>
      <hr/>
      <p>
        Programming for Parallel Computing is programming to run on several processors at the same time. This is something which has been possible for many years. Now it is a real possibility but there are great barriers for programmers wanting to take advantage but having to grasp new ideas to be sure that the results are reliable in the new situation. There are several libraries which do the basic <a href="InterProcessCommunication.html">InterProcessCommunication</a> and also a lot of higher-level code for particular problems such as <a href="LinearAlgebra.html">LinearAlgebra</a>. What many people need is a roadmap of how to get something running. -- <a href="JohnFletcher.html">JohnFletcher</a>
      </p>
      <hr/>
      <p>
        If a program is to run in parallel, there must be at least one task running on each processor, and they must be able to communicate through <a href="InterProcessCommunication.html">InterProcessCommunication</a>.
      </p>
      <p>
        Programming for this involves a choice of <a href="ParallelProgrammingModel.html">ParallelProgrammingModel</a>. 
      </p>
      <p>
        <em>Discussion of </em><a href="ParallelProgrammingModel.html">ParallelProgrammingModel</a> moved to that page.<em> -- </em><a href="JohnFletcher.html">JohnFletcher</a>
      </p>
      <hr/>
      <p>
        Not only is it "a real possibility", but it will increasingly become a necessity if one wishes to fully leverage the capabilities of multithreaded and multicore CPUs. -- <a href="MikeSmith.html">MikeSmith</a>
      </p>
      <hr/>
      <p>
        Q. Is there an accepted delineation (or term) between parallel computing on single machine systems (i.e. where one presumably has command over the [synchronous] execution path) vs. p2p parallel computing done over several machines where there is not synchronous, controlled execution (like the SETI-at-home project)?  --<a href="MarkJanssen.html">MarkJanssen</a>
      </p>
      <p>
        A. The former is typically known as multi-threaded programming or multiprocessing, the latter is typically called distributed computing or parallel distributed computing or grid-based computing.
      </p>
      <p>
        A2. Most 'single machine systems' these days have at least two cores in the CPU which are sharing the main memory but not faster cache memory directly attached to each core. This means that compute intensive tasks are most efficiently carried out by software which is able to adapt to the structures available. There is even software which is able to find out what is the structure of the CPUs and their caches and provide that information in ways which can be used by higher level software.
      </p>
      <hr/>
      <p>
        See also <a href="ParallelProgramming.html">ParallelProgramming</a> <a href="ParallelProgrammingDiscussion.html">ParallelProgrammingDiscussion</a>, <a href="ExascaleComputing.html">ExascaleComputing</a>
      </p>
      <hr/>
      <p>
        <a href="CategoryProgrammingLanguage.html">CategoryProgrammingLanguage</a>
      </p>
    </div>
  </body>
</html>