<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Xp Is Marxism
      </h1>
      <p>
        Is XP like Marxism: it can't be falsified?  Does it matter?  What would <a href="KarlPopper.html">KarlPopper</a> say?
      </p>
      <p>
        <em>In short, XP is just as falsifiable as any other software methodologies.</em>
      </p>
      <hr/>
      <p>
        <strong>It cuts both (all) ways</strong>
      </p>
      <p>
        The complaint has been made that the XP community has made XP unfalsifiable. Each time someone reports a failure while attempting XP, it's claimed, the XP community rapidly comes up with reasons why that attempt doesn't count.
      </p>
      <p>
        The complaint works the other way round too. XP has been in use for a while now, and there's a growing body of stories about projects that used XP and were a success, but the XP critics seem to have no problems coming up with reasons why these successes don't count, either.
      </p>
      <p>
        The same is true of all other software-development methods.  You tried Structured Analysis/Structured Design on a project and it went down in flames?  You must have done it wrong.  It did work?  Then you must have been doing something trivial or already fully explored.
      </p>
      <hr/>
      <p>
        <strong>The demand for data</strong>
      </p>
      <p>
        People sometimes demand 'data' to show that XP works.  "Where is the data?  You XP people are so unscientific, making claims without data."  But the same is true of all other software-development methods.  Where is the data that SA/SD works?  That RUP works?  Where are the controlled experiments?  Show me some data, dammit.
      </p>
      <p>
        The truth is that software development involves far too many variables for any simple empirical test to validate or invalidate any generalization about which method "works".  The major factors that affect the success of a software project include things like:
      </p>
      <ul>
        <li>
           how cooperative the customer is
        </li>
        <li>
           whether the customer is playing to win or playing to lose
        </li>
        <li>
           the customer's political support within his own organization
        </li>
        <li>
           what technical know-how the programmers have
        </li>
        <li>
           what people skills the programmers have
        </li>
        <li>
           whether the programmers have gelled as a team
        </li>
        <li>
           whether the programmers <em>want</em> to do the method
        </li>
        <li>
           and, among many, many other things, whether people practice techniques to reduce risk and manage requirements change.
        </li>
      </ul>
      <p>
        XP addresses mostly the last item on that list.  It contributes to some extent to developing cameraderie and cooperation among the programmers, and mostly assumes that the all-important political elements are in place.  In that latter respect, it is little different from most software development methods.
      </p>
      <p>
        When people demand 'data' to support a method of developing software, they are just announcing that they don't want to hear what you have to say.  The use of the term 'scientific' is just a debating trick.
      </p>
      <p>
        -- <a href="BenKovitz.html">BenKovitz</a>
      </p>
      <hr/>
      <p>
        <strong>What would </strong><a href="KarlPopper.html">KarlPopper</a> say?<strong></strong>
      </p>
      <p>
        <a href="KarlPopper.html">KarlPopper</a> would say that XP offers a number of 'bold hypotheses'.  He would say that like all generalizations, like all the hypotheses that drive science, they transcend all available empirical data that could support them.
      </p>
      <p>
        Like all generalizations in science, XP's claims go far beyond what could ever be tested, and certainly far beyond 'supporting' evidence.  <a href="KarlPopper.html">KarlPopper</a> would say that if you want to treat XP scientifically, and possibly open up new and even better hypotheses, you should look for ways to put its hypotheses to the test.  If a hypothesis passes the test, that proves nothing; there is no way to prove such a hypothesis.  But if it fails, then you've learned something about its limits and maybe that will give you an idea for a new hypothesis.
      </p>
      <p>
        What are the bold hypotheses of XP that could conceivably be tested?
      </p>
      <p>
        'XP is always the best method and ensures a successful project in every case' is surely not among them.
      </p>
      <hr/>
      <p>
        <em>So in a high-feedback situation, pointing out the faulty parts is easy. What's wrong with that metric? Don't answer ;-)</em> --<a href="PhlIp.html">PhlIp</a>
      </p>
      <p>
        We need some examples of this behavior to judge it.  The XP "failures" may indeed have resulted from inappropriate application of XP principles.  Or, XP could be defined too narrowly.  We can't decide if we have no experience. -- <a href="BrentNewhall.html">BrentNewhall</a>
      </p>
      <hr/>
      <p>
        XP is not science, it is engineering. So it is not important whether or not it is falsifiable. 
      </p>
      <p>
        <em>XP is falsifiable, it's just too expensive to do a proper scientific experiment. It's more practical and economical to just try it out for yourself and make an educated decision.</em>
      </p>
      <hr/>
      <p>
        See <a href="XpIsFreeMarket.html">XpIsFreeMarket</a>
      </p>
    </div>
  </body>
</html>