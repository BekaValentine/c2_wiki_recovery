<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Wiki Needs Trust Metrics
      </h1>
      <p>
        Someone suggested that Wiki needs a killfile, but kill-files don't work here. <a href="AdvoGato.html">AdvoGato</a> uses <a href="TrustMetric.html">TrustMetric</a>s for much the same purpose. How could that be made to work here?
      </p>
      <hr/>
      <ul>
        <li>
           If something is true, constructive and helpful ...
          <ul>
            <li>
               ... leave it in and
            </li>
            <li>
               ... edit it to make it fit the document better.
            </li>
          </ul>
        </li>
        <li>
           If something is false, destructive or unhelpful ...
          <ul>
            <li>
               ... extract what lessons you can and then
            </li>
            <li>
               ... delete it.
            </li>
          </ul>
        </li>
      </ul>
      <p>
        [If this were done, there would be constant deletions and restorations, deletions and restorations, over and over again. For example, all the scurrilous insulting of <a href="LinusTorvalds.html">LinusTorvalds</a>, <a href="DennisRitchie.html">DennisRitchie</a>, <a href="RichardStallman.html">RichardStallman</a>, AndyTanenbaum, <a href="TedNelson.html">TedNelson</a>, would be deleted. Then it would be put back. Etc. No, the suggestions below are the correct way to go, I believe. But they are not the <a href="WikiWay.html">WikiWay</a>...]
      </p>
      <p>
        You are claiming that it is not true that the vast majority of the people are trust-worthy and responsible. I believe you are wrong, and that the current brush-fire will die out and the wiki will survive.
      </p>
      <p>
        [Sorry, I misread you before. I am not claiming what you say I am at all. My <em>claim</em> is that by the above criteria, it would be <em>correct</em> to delete that material. I then am assuming that Kulisz will keep putting it back. This is not a current thing, much of that material has been there for a long time and people have just let it go. But I think there are some people now who will <em>not</em> let it go. So unless Kulisz doesn't keep putting it back, we could have the kind of activity I describe above going on for a long time <em>despite</em> the fact the vast majority of people here <em>are</em> trusty-worthy and responsible. The other problem is that amongst that irrational, vicious, vitriol, there are some legitimate criticisms. It is difficult to delete it, difficult to let it stay, but difficult to edit it without a total rewrite, which is thankless since Kulisz will then just restore the original stuff or close enough to it.]
      </p>
      <hr/>
      <p>
        We'd have to abandon <a href="SoftSecurity.html">SoftSecurity</a>. People would need to create accounts and login with them. But there'd be no requirement for account names to map to real names. Instead we'd have consistent pseudonyms so that over time you'd come to have expectations of people.
      </p>
      <p>
        Every user would get a homepage based on their account name and kept in a different namespace to regular wiki pages (much like <a href="WikiPedia.html">WikiPedia</a>'s /Talk). As soon as an account is created your 'juice' or trust would be set to <em>T</em>. This would be the threshold value. If your juice falls below that value (say because you upset sufficient people with high levels of juice) you'd lose the ability to edit any pages other than your homepage.
      </p>
      <p>
        At the same time people would be able to sponsor new wikizens by certifying them. Negative behaviour by those you sponsor would of course reduce your own juice.
      </p>
      <p>
        This way, we'd always know who had said what and no newbie could go on a rampage that caused massive amounts of harm over a prolonged period of time.
      </p>
      <p>
        Of course, all of this has already been explained so much better over on <a href="http://www.advogato.com.">http://www.advogato.com.</a> Take a special look at <a href="http://www.advogato.org/trust-metric.html">http://www.advogato.org/trust-metric.html</a> and <a href="http://www.advogato.org/person/bytesplit/">http://www.advogato.org/person/bytesplit/</a> (who represents precisely the kind of attack that the trust metric was designed to thwart) and of course the FAQ: <a href="http://www.oshineye.com/texts/advogatoFaq.html.">http://www.oshineye.com/texts/advogatoFaq.html.</a>
      </p>
      <p>
        <em>Isn't it a demonstration of a broken trust metric if bytesplit is certified at the Journeyer level?</em>
      </p>
      <p>
        Nope, Advogato is using a two-dimensional trust-metric (see this page for a diagram: <a href="http://www.oshineye.com/software/advoSpaceOutput.html)">http://www.oshineye.com/software/advoSpaceOutput.html)</a> where someone can be a highly-rated open sourcerer but have their diary rated beneath the visibility threshold.
      </p>
      <p>
        Actually, it would seem that the certification metric is irrelevant for trust. The diary rating is the metric that controls visibility of posts. Why have the certification at all, unless the diary rating is also related to the certification level in some non-obvious way?
      </p>
      <p>
        <em>Yep, the diary rating and the certification metric are indeed related. The idea is that diary ratings from people you've directly or indirectly certified count for more than those of random members of the community.</em>
      </p>
      <hr/>
      <p>
        Wiki Scoring Proposal, from <a href="StevenNewton.html">StevenNewton</a>
      </p>
      <ul>
        <li>
           Every page on a wiki has a score, say 0, 1, or 2
        </li>
        <li>
           Every visitor can browse at a certain threshold
        </li>
        <li>
           Some pages are marked as "seed" pages, and get a score of 3 always
        </li>
        <li>
           Other pages are scored based on how many links they have to them, and the score of the linking pages. Ideally, a page with just a few links from pages scored a 2 would have a higher ranking than a page with many links from pages all ranked 0.
        </li>
        <li>
           A page is scored by looked at pages link to it, counting any seed pages as 3. For the non-seed pages, the system looks back another layer, and counts any seed pages there, and scores that page as the avg ((3*seed pages)+pages). Now all the pages that link to the original pages are scored, either as a 3, as a score, or 0 for pages that have no pages linking to them. The base pages is then scored as the average of the scores of all the pages that link to it.
        </li>
      </ul>
      <p>
        Illustration of scoring (example - I don't know if these counts are really right):
      </p>
      <p>
        To score Once and Only Once:
      </p>
      <p>
        Seed pages, score 3:
        <a href="FrontPage.html">FrontPage</a>
        <a href="WikiWikiWebFaq.html">WikiWikiWebFaq</a>
      </p>
      <p>
        Pages linking to Once and Only Once:
        <a href="WikiWikiWebFaq.html">WikiWikiWebFaq</a>
        <a href="WikiEngines.html">WikiEngines</a>
        <a href="RecentEdits.html">RecentEdits</a>
      </p>
      <p>
        Score for <a href="RecentEdits.html">RecentEdits</a> is 1
      </p>
      <p>
        Score for <a href="WikiEngines.html">WikiEngines</a> is:
        (3 for <a href="WikiWikiWebFaq.html">WikiWikiWebFaq</a> + 1 for <a href="RecentEdits.html">RecentEdits</a>)/2 = 2
      </p>
      <p>
        Score for Once and Only Once is:
        (3 for <a href="WikiWikiWebFaq.html">WikiWikiWebFaq</a> + score for <a href="WikiEngines.html">WikiEngines</a> + score for <a href="RecentEdits.html">RecentEdits</a>)/3
        (3 + 2 + 1) / 3 = 2
      </p>
      <p>
        Thinking about a second dimension of trust, now. A visitor can attach an arbitrary score of 0-3 to any page, but that score is for that user only. In other words, if a visitor thinks <a href="WikiEngines.html">WikiEngines</a> is a boring topic, he or she can score it as 0, and that score will be used in place of the community score, <strong>for that visitor only</strong>, which presumes that there must be some way for wiki to identify a particular visitor. Just to be clear, for that visitor the score of 0 not only applies to that page, but to the scores assigned to other pages based on links from it. So the score for Once and Only Once becomes (3 + 0 + 1) / 3 = 1.33...
      </p>
      <p>
        <em>While this is no doubt a technical </em>tour de force<em> it scarcely counts as the </em><a href="SimplestThingThatCouldPossiblyWork.html">SimplestThingThatCouldPossiblyWork</a>. <a href="WardsWiki.html">WardsWiki</a> has survived for years largely undamaged. Let's see how things progress without such technical changes.<em></em>
      </p>
      <p>
        Quite right. The <em>tour de force</em> is a thought experiment by me, not a serious proposal that I wish to pursue here. I have adjusted it to suggest a second dimension of trust - visitor self-scoring, based on the discussion on this page.
      </p>
      <p>
        <a href="SoftSecurity.html">SoftSecurity</a> is a good thing. TrustMetrics are interesting, too. Both are attempts to mirror in a technical way real human community interactions. For example, in any trust metric, I would expect friendship (trust) to be unidirectional. That is, if I trust you, you are not required to trust me. Also, it should not be the case that friends of my friend are automatically my friends as well.
      </p>
      <p>
        -- <a href="StevenNewton.html">StevenNewton</a>
      </p>
      <hr/>
      <p>
        The above assumes that trust is unitary. So someone who writes only pablum would get a high trust value while anyone controversial would get a low value. And of course, there is no way to distinguish between, say, a controversial left-winger (eg, <a href="NoamChomsky.html">NoamChomsky</a>) from a controversial right-winger (eg, <a href="CostinCozianu.html">CostinCozianu</a>). Rather, it conflates individuals' trust in people into "the community's trust" in someone. That just gets you into the problem of aggregation of preferences. Specifically, it is <em>impossible</em> to aggregate preferences in any non-arbitrary way.
      </p>
      <p>
        [The latter claim is highly debatable, however, the weaker claim that we don't know much of an idea of how to do it now, if it <em>is</em> possible, is enough to establish that this proposed "tour de force" will not work. It enforces community solidarity over the honest individuals search for truth. An individual interested truly in the search for truth could quite like get shut out of such a system. I am working on a program that might get around this problem, but it would never search as a replacement for Wiki even if it was good, because of network and lockin effects]
      </p>
      <p>
        Actually this would work assuming you had a multi-dimensional trust metric so that users could be rated in as many dimensions as the system support. Some of those rating dimensions would be collaborative and some would be based on <em>your</em> particular perspective. Advogato already does this. Certifications are collaborative whilst diary ratings are perspective-based.
      </p>
      <p>
        Also we'd also want to start new users just <em>below</em> the edit threshold otherwise persistent trolls could just keep creating new accounts. That particular problem has plagued <a href="WikiPedia.html">WikiPedia</a> for a while now.
      </p>
      <p>
        [What I think you want (what my program alluded to above is intended to do) is to let everyone edit everything, but allow users to have filters based on their own evaluations of reputation. These filters could be arbitrarily finegrained, allowing them to see additions from certain people only on certain subjects, and they could be turned up or down depending upon the users mood - does he want to get down to business and really learn something, or instead slum around with what he considers various interesting crackpots? Both are possible.]
      </p>
      <p>
        The program alluded to by our friend in the square brackets sounds very much like PurpleWiki (available at: <a href="http://www.blueoxen.org/tools/purplewiki/)">http://www.blueoxen.org/tools/purplewiki/)</a> which has a model of wiki that tracks fine-grained elements (finer than the page level that is) and as such offers <a href="TransClusion.html">TransClusion</a>s.
      </p>
      <p>
        [Yes, my (only partly designed and implemented) program is at the very fine-grained level (alas, in some ways it is <em>not</em> as fine-grained as wiki, since because of expediency in programming (the use of Tk canvas widgets to handle justified text formatting on the desktop version), it does not allow structure inside each individual "paragraph" (which is the unit of granularity)). In fact there are no "pages" in the wiki sense, but pages are created dynamically from focal items (what are here <a href="WikiNames.html">WikiNames</a>), based on selecting a set of items from the set of all items that are explicitly related to it. These items could thus potentially be in many "pages" at once. If you know Ecco or Lotus Agenda there are similarities there. Unlike PurpleWiki, the item level of granularity is there from the beginning. I've been "working" on this stupid program since before I heard of wikis or indeed before they existed, but I am not a superman like Kulisz so I haven't made a lot of progress.]
      </p>
      <p>
        The downside to letting everyone edit everything (like the downside of killfiling people on <a href="UseNet.html">UseNet</a>) is that you'd still see the side-effects. If someone you like or have whitelisted responds to flame-bait then you end up having to wade though the ensuing flame-war. The beauty of collaborative filtering is that the system filters based on the totality of your preferences. It should be possible to make an objectionable person 'disappear' from your perspective and reduce the visibility of anything they touch just by applying a rating. Thus if I rate someone (called Alice) as 0 then anyone who rates Alice as 10 or who Alice rates highly up ends up with with a lowered rating from my perspective. Allow the community's perspective to be an input to your trust metric (if I rate Bob as a 10 and Bob rates Charlie as a 0 then the system tentatively assumes that I have a low opinion of Charlie) and I can filter out most of the undesirables (present and future) just by rating a few individuals. 
      </p>
      <p>
        [I didn't mean <em>everything</em>, but they should be able to add comments everywhere as they wish. The hope is that they can be filtered out using various mechanisms such as those you describe. Maybe this is hopeless. My main interest now for my own program is in the single user version, which people can use to develop their own worldview over time, and then expose parts of it for commentary from and to help others. The collaboration model is more along the lines, to use a political/ethical metaphor, of Kant's kingdom of ends rather than Rousseau's body politic. Each person would have his own sovereign structure which they could consider separately from the totality at any time.]
      </p>
      <p>
        The difficulty with any security system is that it must deal reasonably with a variety of different kinds of 'attacks'. These range from newbies who commit social blunders, to WikiSociopaths all the way up to people bent upon the deletion/corruption of the entire wiki. As such the system has to be based on feedback so that the more serious the transgression the more severe the response. It should be easier to lose trust than to gain it that way people have a disincentive to misbehave or tolerate misbehaviour. The downside is that someone who is sufficiently unpopular get visibly shunned and becomes a voice ranting in the darkness. The upside is that unpopular people ranting in the darkness are out of sight and so can safely be put from one's mind.
      </p>
      <hr/>
      <p>
        In 2004 the community was infected by a small number of troublemakers (fewer than five - probably two or three). These troublemakers were sociopaths who the community had to deal with in some fashion. Once these particular troublemakers had gone, things settled down, - until the next set of high-strung Wikizens come along and start another bar room brawl. Remember that most of the people referred to as "troublemakers" are, in fact, people whose intentions are nothing but the highest. They see themselves as Keepers Of The Truth, etc., and are willing to cause all manner of "disruption" to see their goals met. Folk like that are not unlike religious extremists - except that they don't strap on Semtex vests. Well, not yet, anyway.
      </p>
      <p>
        <em>The combination of system support for </em><a href="TheTimeOutStrategy.html">TheTimeOutStrategy</a> and community resolution on the matter extinguished the winter 2003/2004 brushfire. The solution may not be "long term", but it seems to be working. It took nine years for serious troublemakers to emerge. If an episode like this happens no more than every three or four years, that's good enough for me -- especially if the learnings from this one prove helpful in addressing the next recurrence.<em></em>
      </p>
      <hr/>
      <p>
        For a solution to the required login see <a href="AccountlessUserIdentification.html">AccountlessUserIdentification</a>.
      </p>
      <hr/>
      <p>
        Let's be frank: Wiki "needs", or at least could make good use of, a whole lot of things. None of which <a href="WardsWiki.html">WardsWiki</a> is likely to provide. There are literally dozens (perhaps over a hundred) wiki codebases, most of which have more features than this one. Call it minimalism, elegance, pragmatism, apathy, negligence, whatever you want, but you're just not going to see drastic changes made to the codebase of this wiki to support things like trust metrics, karma, moderation, and so forth. 
      </p>
      <p>
        There are parallels here between c2 and other online communities. A few cranks and/or stubborn vocal types take over discourse and either game or otherwise stoke up ill will around whatever technical mechanisms of social control exist. It happened all the time on usenet groups (especially when propagation was more reminiscent of uucp peer-to-peer and not large hubs like giganews), it happened on <a href="LambdaMoo.html">LambdaMoo</a> with a few characters and the arbitration system (those that were there know who, no point in naming them), and doubtless many more communities where I've not witnessed it myself. See WikiDeclineLament. One might experiment with socio-technical mechanisms ElseWiki, but they're likely to find little traction here.
      </p>
      <hr/>
      <p>
        The <a href="BookMarklet.html">BookMarklet</a> at <a href="UserName.html">UserName</a> is a kind of <a href="SoftSecurity.html">SoftSecurity</a> and is better than none. Even a consistently used <a href="DramaticIdentity.html">DramaticIdentity</a> helps to signify the general nature of posts being made, and is particularly useful for people with multiple <a href="IpUsername.html">IpUsername</a> (e.g. dialup, or switch between work and home).
      </p>
      <ul>
        <li>
           This is, of course, marred by fake use of another person's <a href="DramaticIdentity.html">DramaticIdentity</a>, but generally this has not occurred much.
        </li>
        <li>
           If most regulars resume the use of <a href="UserName.html">UserName</a>, naked <a href="IpUsername.html">IpUsername</a> will help to identify spammers that do not visit here often.
        </li>
      </ul>
      <hr/>
      <p>
        It seems we've been working on these problem, in parallel, but independently.  
      </p>
      <p>
        I think you can separate out the two orthogonal issues and desires best with <a href="UserRanking.html">UserRanking</a> (like <a href="AdvoGato.html">AdvoGato</a>'s <a href="TrustMetric.html">TrustMetric</a>) and <a href="PerItemVoting.html">PerItemVoting</a> (rather than ranking a user's blog quality). You don't need multi-scale voting, just a simple + or - 1 will do. The rest is handled though counting intrawiki linking. Or, on <a href="AdvoGato.html">AdvoGato</a>, how often users link to other users pages or content (each link can count as a vote). Ultimately, what this is all pining towards, I'll argue, is the <a href="GlassBeadGame.html">GlassBeadGame</a> and a new Internet that embodies the <a href="WikiWay.html">WikiWay</a>. -- <a href="MarkJanssen.html">MarkJanssen</a>
      </p>
      <hr/>
      <p>
        The practical bottom line is that if you want a heavier <a href="GateKeeper.html">GateKeeper</a> approach to an IT wiki, you'll probably have to build it yourself(s) how you think it "should be" and hope people come. The <a href="WikiZens.html">WikiZens</a> of this wiki will probably want to keep things more or less the same because it's a <strong>self-selecting group</strong> in that those who really hate the "open" style probably stopped coming, meaning the existing <a href="WikiZens.html">WikiZens</a> will tend to favor the status quo. It's roughly comparable to a rock band changing their style: existing fans want things as-is because they are fans precisely because they like the band's "sound", and it takes a while to build a new fan base if they switch. In general it's far easier to lose fans than gain them. Perhaps it's better to focus on incremental fixes. - t
      </p>
      <hr/>
      <p>
        See also <a href="EditsRequireKarma.html">EditsRequireKarma</a>
      </p>
    </div>
  </body>
</html>