<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Prisoners Dilemma
      </h1>
      <p>
        A two-person situation in which (1) Player A stands to gain more in a transaction by acting selfishly towards Player B if B offers to cooperate, and vice-versa ; yet (2) A and B together could gain a higher total reward by cooperating  than by both acting selfishly. The standard outcome for a Prisoner's Dilemma is that both players behave selfishly, leading to the worst mutual outcome.
      </p>
      <p>
        This occurs because for each player, attempting to cooperate exposes that player to risk of defection by the other, and in this dilemma, being defected against while offering to cooperate is worse than mutual defection. As a result, though the ideal outcome is mutual cooperation, without mechanisms for both coordination and for trust, the equilibrium state is mutual defection. (See <a href="http://plato.stanford.edu/entries/prisoner-dilemma/">http://plato.stanford.edu/entries/prisoner-dilemma/</a> for a survey of the subject.)
      </p>
      <p>
        This situation, a classic of both game theory and ethics, provides food for thought. Some advocates see it as clear justification for human selfishness (rational self-interest), while other advocates see it as the best mathematical argument for unselfishness. Still others see it as irrelevant to human relations, claiming that it's based on poor assumptions about human nature and selfishness.
      </p>
      <hr/>
      <p>
        <strong>The Dilemma</strong>
      </p>
      <p>
        The following table shows one particular form of Prisoner's Dilemma. Each player chooses (not knowing the other player's choice) to "cooperate" or "defect", and then each player gets paid according to the box they jointly land in; the two numbers are what P1 gets, then what P2 gets.
      </p>
      <code>
        Player 1<br/>
        C	  D<br/>
        +-----+-----+<br/>
        C| 3,3 | 0,5 |<br/>
        Player 2	+-----+-----+<br/>
        D| 5,0 | 1,1 |<br/>
        +-----+-----+<br/>
      </code>
      <p>
        In each row (i.e., for each possible choice by P2) P1's best outcome is in the <em>D</em> column. In each column (i.e., for each possible choice by P1) P2's best outcome is in the <em>D</em> row. So defection seems to be rational for each player. And yet they do much better if they both cooperate than if they both defect.
      </p>
      <p>
        There's an excellent book about this, Robert Axelrod's <em>The Evolution of Cooperation</em> [ISBN: 0465021212]. In part of the book he describes how he asked academics for different strategies, then ran those strategies against each other in tournaments. His tournaments, and some theoretical analysis, indicate that to do well, a strategy should have four qualities:
      </p>
      <ol>
        <li>
           it should be <strong>nice</strong>, that is, cooperate when the other party does (and at the start)
        </li>
      </ol>
      <ol>
        <li>
           it should be <strong>provokable</strong>, that is, punish non-cooperation or otherwise try to keep non-cooperation from happening
        </li>
      </ol>
      <ol>
        <li>
           it should be <strong>forgiving</strong>, that is, after punishing non-cooperation, immediately return to cooperation
        </li>
      </ol>
      <ol>
        <li>
           it should be <strong>clear</strong>, that is, behave in a way the other party can understand (so that the other player learns quickly that cooperating with you is the way to do well)
        </li>
      </ol>
      <p>
        The strategy that did spectacularly well in the tournament is called "Tit for Tat". (That's not a link because there's already a Wiki page with that name on an unrelated topic). Its behavior is dead simple: cooperate on the first turn, and thereafter on each turn do what the other player did last turn.
      </p>
      <p>
        Here are some of the simplest strategies.
      </p>
      <p>
        <strong></strong><a href="TitForTat.html">TitForTat</a><strong> (aka An eye for an eye, TFT)</strong>
      </p>
      <code>
        Other<br/>
        C   D<br/>
        +---+---+<br/>
        M	C| C | D |<br/>
        e	 +---+---+<br/>
        D| C | D |<br/>
        +---+---+<br/>
      </code>
      <p>
        <strong></strong><a href="PavlovStrategy.html">PavlovStrategy</a><strong> (aka Win Stay Lose Change, PAVLOV)</strong>
      </p>
      <code>
        Other<br/>
        C   D<br/>
        +---+---+<br/>
        M	C| C | D |<br/>
        e	 +---+---+<br/>
        D| D | C |<br/>
        +---+---+<br/>
      </code>
      <p>
        <strong></strong>AlwaysDefect<strong> (aka ALL-D)</strong>
      </p>
      <code>
        Other<br/>
        C   D<br/>
        +---+---+<br/>
        M	C| D | D |<br/>
        e	 +---+---+<br/>
        D| D | D |<br/>
        +---+---+<br/>
      </code>
      <p>
        <strong></strong>AlwaysCooperate<strong> (aka ALL-C)</strong>
      </p>
      <code>
        Other<br/>
        C   D<br/>
        +---+---+<br/>
        M	C| C | C |<br/>
        e	 +---+---+<br/>
        D| C | C |<br/>
        +---+---+<br/>
      </code>
      <p>
        Playing the <a href="PrisonersDilemma.html">PrisonersDilemma</a> more than once is known as the <a href="IteratedPrisonersDilemma.html">IteratedPrisonersDilemma</a>. One of Axelrod's conclusions is that cooperation is not the best strategy for a one-round <a href="PrisonersDilemma.html">PrisonersDilemma</a>, but in an <a href="IteratedPrisonersDilemma.html">IteratedPrisonersDilemma</a> the concomitant "shadow of the future" (i.e., the expectation that you'll interact with the other person again) encourages cooperation among players. (Loosely speaking, players establish reputations, and how you interact with other players will be based on their reputations.)
      </p>
      <p>
        Both <a href="TitForTat.html">TitForTat</a> and <a href="PavlovStrategy.html">PavlovStrategy</a> are strong competitors in the <a href="IteratedPrisonersDilemma.html">IteratedPrisonersDilemma</a>. Each has its strengths and weaknesses, which are explored in <a href="TitForTatVsPavlov.html">TitForTatVsPavlov</a>. Despite the impressive showings of these strategies, Axelrod demonstrates that no single strategy outperforms (or ties) all opponents. The best strategy is always determined by its context. See <em>The Evolution of Cooperation</em> for more on this point.
      </p>
      <p>
        Read the variations at <<a href="http://www.spectacle.org/995/index.html">http://www.spectacle.org/995/index.html</a>> to see why a particular strategy may or may not work.
      </p>
      <p>
        Note that the terms "cooperate" and "defect" may set off alarm bells in those versed in <a href="GameTheory.html">GameTheory</a>. In this case, they're just names for the two different choices each player can make. Despite the name "cooperate", all choices are made independently by each player, with no collusion allowed.
      </p>
      <p>
        [I can't find a link, but... related to the Sissy Fight strategy below, a program was able to "take over" one of these game theory experiments, beating out the traditional tit-for-tat strategy, by communicating via certain patterns of behavior and operating as a team. As in so many games, the best way to win is to cheat.]
      </p>
      <hr/>
      <p>
        The most interesting example of this that <a href="MichaelChermside.html">MichaelChermside</a> ever saw was an experiment (I wish I remembered the reference... Scientific American?) in which the standard <a href="PrisonersDilemma.html">PrisonersDilemma</a> was modified by introducing the concept of communication errors... a certain (small) percentage of the time, the players would be mis-informed about the results of their last round. Then the whole thing was set up on what was basically an evolutionary basis: a pool of many interacting players with different strategies was created and allowed to play each other. Then a new pool (the "next generation") was created, with strategies that did better getting a larger share of the "gene pool" in the new generation.
      </p>
      <p>
        The results were fascinating. They were able to get the system to a stable state where the following happened. First, in a population of mixed strategies, <a href="TitForTat.html">TitForTat</a> proliferated until they took over nearly all of the population. Then AlwaysCooperate began to take root, because unlike <a href="TitForTat.html">TitForTat</a> it didn't have a tendency to get into a state of alternating defecting after a random communication error, so it cooperated with everyone. AlwaysCooperate grew, until it had taken over most of the population, at which point AlwaysDefect appeared. It quickly took advantage of AlwaysCooperate, and took over nearly all of the population. Then <a href="TitForTat.html">TitForTat</a> began to appear... it  wouldn't allow AlwaysDefect to take advantage of it, but two  <a href="TitForTat.html">TitForTat</a>s were able to cooperate. So <a href="TitForTat.html">TitForTat</a> took over, and the cycle started again. This could be made stable.
      </p>
      <hr/>
      <p>
        The most interesting (to me) thing about the <a href="GameTheory.html">GameTheory</a> view of <a href="PrisonersDilemma.html">PrisonersDilemma</a> is that it shows how cooperation can emerge without intelligence. If you aren't forced to assume intelligence, you can make a lot more conclusions about the system. Making a single combined move forces us to assume intelligence (the two players agreeing to make this move together), and that really doesn't shed any light on cooperation; we've known that intelligent people cooperate for thousands of years. The most interesting fact is that this same level of cooperation would arise even if we weren't intelligent creatures. Another interesting fact is that these strategies are stable. It doesn't matter how scheming or devious the other strategies are, or how complicated their hidden agendas are, a simple TFT strategy will still beat them in the long run. What happens when you <em>have</em> to interact with someone or something that you don't have control over? Without understanding the game of two independent players, you won't get many insights. -- <a href="AnonymousDonor.html">AnonymousDonor</a>
      </p>
      <p>
        <em>Please note that the term "cooperate" as used in the context of game theory is probably different from what is intend in the phrase above "...we've known that intelligent people cooperate..."  In game theory no coordination or collusion is assumed or permitted.  There are other significant simplifying assumptions that drive the conclusions of game theory, but make the application of the results to the real world questionable.  Collaboration is a far more productive approach in the real world.</em>
      </p>
      <hr/>
      <p>
        Could this be summed up like this? If the players don't know they're playing, the best 'choice' is to defect. If they do, they're screwed by the circular logic.
      </p>
      <p>
        <em>No. If they don't know they're playing, the best choice is "defect". If they do, both players can choose "cooperate", and both will make out better than they would if they chose "defect".</em>
      </p>
      <hr/>
      <p>
        The evolutionary basis for <a href="DramaticIdentity.html">DramaticIdentity</a>.
      </p>
      <p>
        <em>Selfishness is not part of the prisoner's dilemma (or general game theory).  The supposition is that there are two players making simultaneous decisions without knowledge of the other. Cooperation is precluded by the definition and is not dependent upon "selfishness."</em>
      </p>
      <hr/>
      <p>
        There is an online game called "Sissy Fight 2000" at <a href="http://www.sissyfight.com">http://www.sissyfight.com</a> that appears to be related to the <a href="PrisonersDilemma.html">PrisonersDilemma</a>. In the game, you take on the role of a young school girl intent on ruining other's popularity and self-esteem.  
      </p>
      <p>
        It works like this: Three to six girls are playing in the schoolyard.  You start out with 10 self-esteem points.  The goal is to humiliate and abuse others and reduce their points to zero.  Each turn, you can choose one action:
      </p>
      <dl>
        <dt>	Scratch</dt>
        <dd>	The girl you scratch loses 1 point.</dd>
      </dl>
      <dl>
        <dt>	Tease</dt>
        <dd>  If you tease a girl with someone else, she loses 2 points for each girl that teases.  If you tease by yourself, it doesn't do anything (except take up one of your turns).</dd>
      </dl>
      <dl>
        <dt>	Grab</dt>
        <dd>	When you grab a girl, she can't scratch, grab, or tease anyone else on that turn.  If someone scratches the girl your grabbing, the scratch is twice as effective.  If more than one person grabs a girl in the same turn, she loses a point.</dd>
      </dl>
      <dl>
        <dt>	Tattle</dt>
        <dd> This makes every other girl in the game lose three points, but you only can do it twice in a game -- and if other girls tattle in the same turn, the teacher doesn't believe anyone and all tattlers lose three points.</dd>
      </dl>
      <dl>
        <dt>	Cower</dt>
        <dd>  It lets you dodge a <em>single</em> grab or scratch.  It also makes you look innocent, so that tattles don't affect you.  If you cower too much, you lose points.</dd>
      </dl>
      <dl>
        <dt>	Lick Your Lollypop</dt>
        <dd>	This gives you two points and makes you look innocent, which protects you from tattles.  But if you are scratched or grabbed while licking your lollypop, you'll choke on it and suffer double damage.  You only get three lollypop licks per game.</dd>
      </dl>
      <p>
        As you can see, these rules add quite a lot of complexity to the scoring. If you want to get anywhere in the game, you have to cooperate with others. But even that has a twist. You can use the chat command to speak to the other girls, but they can see what you type. So two girls wanting to back stab another girl have to make their plans known in public.  And girls who want to gang up are obvious. Because the victims can see what's going on, they can react as well to the information.
      </p>
      <p>
        Perhaps someone with a deeper understanding of game theory can comment on the game's dynamics, or maybe even analyze a winning strategy.
      </p>
      <p>
        <em>The best way to win at sissy fight is to open an alternate chat channel (i.e. cheat) and/or to go in to the game as a team (i.e. also cheat).</em>
      </p>
      <p>
        -- <a href="JohnPassaniti.html">JohnPassaniti</a>
      </p>
      <hr/>
      <p>
        Winning by destruction: Consider the final line in the movie "War Games" made by the computer after having examined all the outcomes of destructive competition (A Global Nuclear War Scenario):
      </p>
      <code>
        A STRANGE GAME. <br/>
        THE ONLY WINNING MOVE IS <br/>
        NOT TO PLAY. <br/>
      </code>
      <code>
        HOW ABOUT A NICE GAME OF CHESS? <br/>
      </code>
      <p>
        Related to <a href="PrisonersDilemma.html">PrisonersDilemma</a>, <a href="GameTheory.html">GameTheory</a> helped determine the strategy of the <a href="ColdWar.html">ColdWar</a>, called MAD or <a href="MutuallyAssuredDestruction.html">MutuallyAssuredDestruction</a>. Indeed, the only winning move is to take no offensive action.
      </p>
      <p>
        There was one other option that was also considered at the start of the Cold War, the pre-emptive strike. This is even more frightening in the context of very recent history.
      </p>
      <p>
        The movie <a href="DrStrangelove.html">DrStrangelove</a> has a great scene where the Russian diplomat explains about the existence of their automatic doomsday device, and Dr. Strangelove says something along the lines of, "That is the whole idea of this machine, you know. Deterrence is the art of producing in the mind of the enemy... the <em>fear</em> to attack. And so, because of the automated and irrevocable decision making process which rules out human meddling, the doomsday machine is terrifying. It's simple to understand. And completely credible, and convincing. [...] Yes, but the... whole point of the doomsday machine... is lost... IF YOU KEEP IT A SECRET! Why didn't you tell the world, eh?"
      </p>
      <p>
        <em>That's what happens when you don't </em><a href="LetTheHumanPullTheTrigger.html">LetTheHumanPullTheTrigger</a>.<em></em>
      </p>
      <p>
        To be fair, the Russian diplomat explains that they were about to announce it to the world when the trouble with Gen. Ripper started, which in a way is an argument against <a href="LetTheHumanPullTheTrigger.html">LetTheHumanPullTheTrigger</a>, or at least, 'let a single human pull the trigger'.
      </p>
      <hr/>
      <p>
        See <a href="TragedyOfTheCommons.html">TragedyOfTheCommons</a>, <a href="DollarAuction.html">DollarAuction</a>
      </p>
    </div>
  </body>
</html>