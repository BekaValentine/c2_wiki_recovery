<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Parallel Programming Model
      </h1>
      <p>
        A Parallel Programming Model has to be chosen in order to do <a href="ProgrammingForParallelComputing.html">ProgrammingForParallelComputing</a>.
      </p>
      <hr/>
      <p>
        <em>On the page </em><a href="http://en.wikipedia.org/wiki/Parallel_computing.''">http://en.wikipedia.org/wiki/Parallel_computing.''</a>
      </p>
      <p>
        A <strong>parallel programming model</strong> is a set of software technologies to express parallel algorithms and match applications with the underlying parallel systems. It encloses the areas of applications, languages, compilers, libraries, communication systems, and parallel I/O. People have to choose a proper parallel programming model or a form of mixture of them to develop their parallel applications on a particular platform.
      </p>
      <hr/>
      <p>
        Much of the work which has been done in this area has been in <a href="FortranLanguage.html">FortranLanguage</a>, as this is the underlying language of many of the libraries. There is also an interface to <a href="CeePlusPlus.html">CeePlusPlus</a> and some interesting software called the <a href="ObjectOrientedMessagePassingInterface.html">ObjectOrientedMessagePassingInterface</a> (OOMPI), which provides a class-based approach to the <a href="MessagePassingInterface.html">MessagePassingInterface</a>. This makes it much easier to write the programs as it is possible to hide nearly all of the parameters of MPI with skillful use of overloading of member functions.
      </p>
      <p>
        The low-level choices about how to implement a Parallel Programming Model are part of <a href="ParallelProgramming.html">ParallelProgramming</a> and <a href="DistributedComputing.html">DistributedComputing</a>.
      </p>
      <p>
        Higher-level choices involve choosing among implementations which provide a program interface for the user to obtain the facilities needed for ParallelComputing, that is for different tasks to run concurrently. Examples of these are:
      </p>
      <ul>
        <li>
           <a href="MessagePassingInterface.html">MessagePassingInterface</a> (MPI)
        </li>
        <li>
           <a href="ObjectOrientedMessagePassingInterface.html">ObjectOrientedMessagePassingInterface</a>
        </li>
        <li>
           <a href="ParallelVirtualMachine.html">ParallelVirtualMachine</a> (pvm)
        </li>
        <li>
           <a href="OpenMultiProcessing.html">OpenMultiProcessing</a> (OpenMP)
        </li>
        <li>
           <a href="GeneralPurposeGraphicsProcessUnits.html">GeneralPurposeGraphicsProcessUnits</a> (GPGPU) with <a href="ComputeUnifiedDeviceArchitecture.html">ComputeUnifiedDeviceArchitecture</a> (CUDA) software or <a href="OpenCl.html">OpenCl</a> software.
        </li>
        <li>
           <a href="CudaMpi.html">CudaMpi</a> (more than one GPGPU communicating)
        </li>
      </ul>
      <p>
        If the task to be carried out involves <a href="LinearAlgebra.html">LinearAlgebra</a>, one route to take is to use <a href="ScaLapack.html">ScaLapack</a>, which hides most of the details of the <a href="InterProcessCommunication.html">InterProcessCommunication</a>.
      </p>
      <hr/>
      <p>
        An alternative is to provide an extension to a language to permit concurrency: 
      </p>
      <ul>
        <li>
           <a href="MuCeePlusPlus.html">MuCeePlusPlus</a> (extends <a href="CeePlusPlus.html">CeePlusPlus</a>)
        </li>
      </ul>
      <hr/>
      <p>
        <strong>Books</strong>
      </p>
      <ul>
        <li>
           <a href="ObjectOrientedMultithreadingUsingCpp.html">ObjectOrientedMultithreadingUsingCpp</a>
        </li>
        <li>
           <a href="ParallelAndDistributedProgrammingUsingCpp.html">ParallelAndDistributedProgrammingUsingCpp</a>
        </li>
        <li>
           <a href="PatternsForParallelProgramming.html">PatternsForParallelProgramming</a> which defines a <a href="PatternLanguageForParallelProgramming.html">PatternLanguageForParallelProgramming</a>
        </li>
      </ul>
      <hr/>
      <p>
        <strong>Job Classification</strong>
      </p>
      <dl>
        <dt><strong>SingleProgramMultipleData (SPMD)</strong></dt>
        <dd>the same program runs on multiple processors, although not necessarily in sync (example: SETI@Home)</dd>
      </dl>
      <dl>
        <dt><strong>MultipleProgramMultipleData (MPMD)</strong></dt>
        <dd>different programs run on different processors.</dd>
      </dl>
      <dl>
        <dt><strong>SingleInstructionMultipleData (SIMD)</strong></dt>
        <dd>the exact same instruction stream is fed simultaneously to all processors. Often each instruction is conditional on the flags in the processor, so each processor may or may not execute the instruction. The <a href="ConnectionMachine.html">ConnectionMachine</a>s CM-1 and CM-2, or the Goodyear MPP are examples of this.</dd>
      </dl>
      <dl>
        <dt><strong>MultipleInstructionMultipleData (MIMD)</strong></dt>
        <dd> One of SPMD or MPMD, depending on precise usage.</dd>
      </dl>
      <hr/>
      <p>
        <strong>Practical Experience</strong>
      </p>
      <p>
        <strong></strong><a href="ParallelVirtualMachine.html">ParallelVirtualMachine</a> (pvm)<strong></strong>
      </p>
      <p>
        This was a few years ago the <em>defacto standard</em>, but is now being replaced by the <a href="MessagePassingInterface.html">MessagePassingInterface</a>. I have used it on a cluster of PC's to run programs using <a href="ScaLapack.html">ScaLapack</a>. It was necessary for the user to nominate the computers to be used for each run, either at run time or in a configuration file. It is possible to run both SPMD and MPMD jobs and to spool extra tasks while the job is running.
      </p>
      <p>
        <strong></strong><a href="MessagePassingInterface.html">MessagePassingInterface</a> (mpi)<strong></strong>
      </p>
      <p>
        There are a number of different implementations of the MPI standard for the data interface. The common one on Linux systems is MPICH, although there is also LAM MPI. MPI is often used on systems where there is a batch scheduling system to run SPMD tasks which need a number of processors known in advance.
      </p>
      <p>
        <strong>Changing from pvm to MPI</strong>
      </p>
      <p>
        This has been quite easy for SPMD programs written in <a href="FortranLanguage.html">FortranLanguage</a> and using <a href="ScaLapack.html">ScaLapack</a>. A few changes were required in the set up and close down sections, but the main calls were unchanged. The makefile needed adjusting to link different libraries. 
      </p>
      <hr/>
      <p>
        See also <a href="ParallelProgrammingDiscussion.html">ParallelProgrammingDiscussion</a>
      </p>
      <hr/>
      <p>
        <a href="CategoryProgrammingLanguage.html">CategoryProgrammingLanguage</a>
      </p>
    </div>
  </body>
</html>