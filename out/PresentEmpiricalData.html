<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Present Empirical Data
      </h1>
      <p>
        Often when faced with someone recommending change, people ask the agent to <a href="PresentEmpiricalData.html">PresentEmpiricalData</a>.
      </p>
      <p>
        This is rarely - I am tempted to say never - a request for empirical data. With rare exceptions, empirical data does not convince, let alone induce change.
      </p>
      <p>
        Asking someone to <a href="PresentEmpiricalData.html">PresentEmpiricalData</a> is what sales people call an "objection", and they teach that the first objection raised usually isn't even a real one. The real meaning of an initial reaction of "<a href="PresentEmpiricalData.html">PresentEmpiricalData</a>" is, all too often, "Go away".
      </p>
      <p>
        <strong>Therefore...</strong>
      </p>
      <p>
        The first technique in dealing with sales objections is to ignore them. When they aren't the real objection, they tend not to be repeated, and this is often sufficient to move on to the real issue. Sometimes you have to deal more directly with the objection ... and almost invariably, when you do, you find another objection.
      </p>
      <p>
        One way to test whether "<a href="PresentEmpiricalData.html">PresentEmpiricalData</a>" is a real objection is to invite the objector to do the experiment himself. If he answers "I already know it won't work", you can be sure that he expected no empirical data, and that if and when it was presented, he'd "already know" why it wasn't applicable to him.
      </p>
      <p>
        Of course it would be really good to have empirical data, no denying that. Just recognize that when you get it, you won't instantly close any more sales, you'll probably just get to the next objection. Keep on drillin'.
      </p>
      <p>
        <strong>But...</strong>
      </p>
      <p>
        <em>Often when faced with someone recommending change, people ask the agent to </em><a href="PresentEmpiricalData.html">PresentEmpiricalData</a>.<em></em>
      </p>
      <p>
        This deserves clarification - specifically, is that to be read as "recommending change in general" (there's something wrong that should be changed because going on as we are will lead to disaster) or "recommending a specific kind of change to address a specific problem" (here is how we might do things to improve the situation).
      </p>
      <p>
        For the second kind of concern - you are recommending a specific change to address a specific problem acknowledged as such - I'd argue that empirical data by itself isn't sufficient. It needs to be presented as supporting a given set of objectives, or as a method for achieving the necessary improvements, or whatever; in short, fact without theory is worthless.
      </p>
      <p>
        Empirical data by itself is primarily useful in justifying the first kind of concern. <a href="PresentEmpiricalData.html">PresentEmpiricalData</a> to the effect that a problem exists, and you have justified the need for change.
      </p>
      <p>
        For that, though, you need to see things from the point of view of the people to whom you are recommending change - would <em>they</em> agree that your data signals a problem, according to their own definition of their objectives? (See <a href="FirstCreateTheMailbox.html">FirstCreateTheMailbox</a>.)
      </p>
      <p>
        If you have been asked to solve a problem and are recommending a specific solution, and they ask you to <a href="PresentEmpiricalData.html">PresentEmpiricalData</a> - it's a stall. Brush it aside and move on to the real issues.
      </p>
      <p>
        If you are just butting in where no-one has asked you to solve a problem, and recommending change in general "just because", <a href="PresentEmpiricalData.html">PresentEmpiricalData</a> may be a way to say <em>Go away</em>. But possibly it's as good a way as any other.
      </p>
      <hr/>
      <hr/>
      <p>
        <strong>Further buts...</strong>
      </p>
      <p>
        In my opinion, the above is a very black and white way of looking at the world. How do you know exactly what someone else's agenda or thoughts are? We are taught as Engineers to work with data and the Scientific Method. It is perfectly valid for someone to ask to <a href="PresentEmpiricalData.html">PresentEmpiricalData</a>. If it works, you should have rigorous EmpiricalData to back up your claims.
      </p>
      <p>
        We have seen great things with <a href="ExtremeProgramming.html">ExtremeProgramming</a>. It is the pretty much the only methodology that makes sense to me after 22 years of everything from <a href="CowboyCoding.html">CowboyCoding</a> to CMM. However, I think even Kent would agree that more data needs to be collected. I would disagree with what you said above. As <a href="HalClement.html">HalClement</a> taught me, "A theory in Science is not accepted as a theory unless there is EmpiricalData to back it up." It should be the same for XP. Hand-waving is not acceptable.
      </p>
      <p>
        -- sg
      </p>
      <p>
        <em>Please read it again, Sam, then say more about why it's black and white. Note that it says it would be nice to have data: perhaps that's not strong enough. What percentage of folks who see the data do </em><strong>you</strong><em> think will then move immediately to doing XP? And please in your reply discuss how you wound up trying XP after so long on "the other side". Thanks!</em>
      </p>
      <p>
        I'm not sure how many people would be "sold". But I know a lot of Engineering people make decisions on things based on this kind of data. <a href="SteveMcConnell.html">SteveMcConnell</a> is one of them. And there are others. I'm not saying we have to measure everything like PSP does. Gosh no. But we should be able to collect some form of EmpiricalData and present it.
      </p>
      <p>
        How did I come to try XP? In my twenty plus years of software development, I have seen many "heavy" processes that just did not work. I started my career working for the DOD and using DOD2167 and seeing the results. The results were that we wrote specs for months and months and did little else. The specs were completely wrong when it came time to do anything. The same thing at Raytheon with projects actually getting canceled after months of spec writing and no code. Digital was slightly better, but with their emphasis on a "Phase Review Process" at the end of each of the four "phases", it was clearly a waterfall model that broke down constantly and no-one ever read the specs anyway (we know because we put certain things in it to get a reaction!).
      </p>
      <p>
        Throughout all of this, I always saw the customer as totally unhappy and the developers "fed up". There was no quality. I also worked with ISO9000 and CMM at various companies and it was a horrid way to do anything. Documents upon documents. There were documents just to change a line of code! It was clear to me that to survive in this software business, companies would have to adopt something much more lightweight but more rigorous than the <a href="CowboyCoding.html">CowboyCoding</a>.
      </p>
      <p>
        I found XP to be it. I had a lot of skepticism when I tried to cling onto my Rational diagrams and <a href="BigDesignUpFront.html">BigDesignUpFront</a>. But interactions with cards and customers convinced me that XP was the better way. I try to do everything XP now. I am introducing <a href="UnitTest.html">UnitTest</a>s at my current company. Is this what you wanted?
      </p>
      <p>
        -- sg
      </p>
      <p>
        <em>Yes, it is. An observation, then a question. Observation: You moved to XP, by your account, because you saw what wasn't working and were looking for something new. You chose it based on gut feel, and then on the data that really matters, your own experience. I think that's how things really happen, not decisions based on data.</em>
      </p>
      <p>
        <em>Question: How do you know </em><a href="SteveMcConnell.html">SteveMcConnell</a> makes decisions based on empirical data? Because he asked for some about XP? That doesn't convince me. And I don't see a lot of empirical data in his (very wonderful) books. So ... why do you say he bases decisions on empirical data? Thanks!<em></em>
      </p>
      <hr/>
      <p>
        <em>We might also wonder about the adoption of pair programming, for which there is some decent evidence. (I'd observe, though, that it's not like PP showed up as 10X better in the experiments, so maybe even if we were data-driven we'd not be convinced as yet.) Still think most of us are not data-driven, but I could be wrong. Anyone got any data on that?</em>
      </p>
      <hr/>
      <p>
        <em>How do you know </em><a href="SteveMcConnell.html">SteveMcConnell</a> makes decisions based on empirical data?<em></em>
      </p>
      <p>
        You've obviously never read <a href="CodeComplete.html">CodeComplete</a>. But I find it interesting that you are using hand waving arguments about <em>how another person feels</em> when there are clear data points that show the opposite, both in his writing and in <em>his own testimonial</em>. I don't think this speaks to the weakness of empirical data in general, but more to your own weakness of discriminatory selection of premises. I understand your argument that most people think this way, but that's just more handwaving. The circularity is cutting. -- <a href="SunirShah.html">SunirShah</a>
      </p>
      <p>
        <em>As for what's obvious, I have in fact read </em><a href="CodeComplete.html">CodeComplete</a>. As for "hand waving arguments about how another person feels", I make no arguments here whatsoever about what people feel, hand waving or otherwise. I <strong><em>observe</em></strong> that in the majority of cases where I provide evidence in response to the request for empirical data, all it does is uncover another objection. And I respectfully suggest that you review <a href="BowToYourFellowPractitioners.html">BowToYourFellowPractitioners</a>, and adjust your tone of discourse.<em></em>
      </p>
      <p>
        Let's keep the discourse civil. I agree.
      </p>
      <p>
        -- sg
      </p>
      <p>
        The opening posting suggests that a request for empirical data is not always what it seems, especially when presented as a first objection. It points out that one way to find out if it is a real objection is to ignore it and see if it comes back, and it points out that another way is to suggest and experiment and see if the answer is "I already know ...". It suggests that when you <em>do</em> get the empirical data, you'll likely just find another objection, and that you should keep on looking for new ones. I fail to see why the posting has generated so much heat. -- <a href="RonJeffries.html">RonJeffries</a>
      </p>
      <p>
        What I wonder about is what happens when XP advocates are presented with empirical data that contradicts their rhetoric. I was asked on <a href="UnitTestsReconsidered.html">UnitTestsReconsidered</a> (specifically <a href="BugsInTheTests.html">BugsInTheTests</a>) to present empirical evidence of my claims, so I did (using <a href="CodeComplete.html">CodeComplete</a> no less!). The response from the XP advocates was at best confusion, sadly. I was hoping for more, like how to deal with the semantic bug problem.
      </p>
      <p>
        Nonetheless, when presented with empirical data that disputes XP's rhetoric, XP advocates must respond by either refuting the empirical data's source, producing more empirical data to the contrary, changing their rhetoric, or correcting their methodology to fit with the new information. 
      </p>
      <p>
        Unfortunately, the XP advocates involved chose to change the rhetoric (see <a href="XpVsStandardDefinitionOfUnitTest.html">XpVsStandardDefinitionOfUnitTest</a>). This is the weakest of all responses as it suggests that a) you don't know what you're doing, b) a) you don't know why you're doing it, c) all you can do is hand wave. But I know you know what you're doing - though I doubt you know why - and I definitely think you can do better than hand wave. Anecdotal evidence would be far better, for instance.
      </p>
      <p>
        I would have been much happier with practical solutions to the problems I face. I don't care whether you call the tests drivers or contracts or whatever, there will still be <a href="BugsInTheTests.html">BugsInTheTests</a>. -- <a href="SunirShah.html">SunirShah</a>
      </p>
      <p>
        <em>I didn't mean the above to be inflammatory. Maybe a little disappointment. I was trying to suggest constructive ways to respond, rather than the current path chosen. If you can clean it up, bonus. If you can't, delete it. By the way, I was careful to suggest it was only the local response that was flawed, and not XP itself. I like XP; I don't like "XP advocacy" in the sense that I like Linux; I don't like "Linux advocacy". -- ss</em>
      </p>
      <hr/>
      <p>
        In my observation, there are <a href="TooManyVariablesForScience.html">TooManyVariablesForScience</a>. Thus a different approach needs to be taken. But note that if one demands empirical evidence for A, you can also demand empirical evidence for the alternatives to A. It works both ways. In the end comparisons are usually not easy and simple (barring a show-stopping flaw), and if one expects them to be, they are usually naive.
      </p>
      <hr/>
      <p>
        <a href="CategoryEvidence.html">CategoryEvidence</a>
      </p>
    </div>
  </body>
</html>