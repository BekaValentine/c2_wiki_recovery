<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Economy Of Execution
      </h1>
      <p>
        Runtime performance is one of many desirable properties of a computer language. How fast will programs written in the language run? This page explores two assertions about language design with respect to economy of execution...
      </p>
      <ul>
        <li>
           Many object-oriented languages perform badly in this regard. Poor economy of execution has been listed among the <a href="BadEngineeringPropertiesOfOoLanguages.html">BadEngineeringPropertiesOfOoLanguages</a>.
        </li>
      </ul>
      <ul>
        <li>
           Static type information is essential for performance. The more information a compiler/optimizer has about data, the more it can produce highly efficient code. Many people deemphasize performance when complaining about <a href="StaticTyping.html">StaticTyping</a>. 
        </li>
      </ul>
      <hr/>
      <p>
        Typical examples of object-oriented languages that perform well are C++ and Ada'95 where one can have highly flexible structures (templates, parameterized classes) without paying in execution performance. 
      </p>
      <p>
        On the other end you have Java , Smalltalk, Python. These languages defer type based decisions until runtime. Although just in time compilers are supposed to fix this issue in theory, in practice it doesn't quite happen. 
      </p>
      <p>
        Statement level performance varies over many orders of magnitude from VHDL (which compiles to hardware) to shell script (that forks a task to add two numbers). Performance can be so important that people will even argue over rather small increments as in the discussion on this page.
      </p>
      <ul>
        <li>
           Smalltalk = 1/4 C++
        </li>
        <li>
           C++ = 1/2 Fortran
        </li>
        <li>
           Fortran = 1/2 hand assembler
        </li>
      </ul>
      <p>
        <em></em><a href="EconomyOfExecution.html">EconomyOfExecution</a> is growing less and less important (slowly, of course, and there are admittedly exceptions). More important is EconomyOfImplementation, something that is quaintly overlooked in the paper <a href="BadEngineeringPropertiesOfOoLanguages.html">BadEngineeringPropertiesOfOoLanguages</a>.<em></em>
      </p>
      <hr/>
      <p>
        My experience, after 20 years or so of application development experience in Smalltalk, C-based OO languages (C++, Objective-C, ObjectC, etc), Java, and so on, is that the claimed performance advantages of "strict static type checking" are less compelling when deployed than those cited here. Here's why:
      </p>
      <ol>
        <li>
           <em>Static typing often just moves the performance problem.</em> While it's sometimes (but not always) true that a good C++ compiler will outperform a bad Smalltalk implementation on a microscopic benchmark (like adding two floats), this approach usually does not measure the performance impact resulting from the often tortuous path the programmer must follow to ensure that the arguments, as delivered, really ARE floats. Try writing a complex number package in java as an exercise. Measure its performance (<a href="ComplexNumberTest.html">ComplexNumberTest</a>). Write the same package in Smalltalk. Measure its performance. Ask the best Java, C++ and Smalltalk programmers you know to do the same. Compare their results. Please bear in mind that if I need fast complex numbers in Smalltalk, I'll probably write and install primitives written in C (NOT C++). The net result will be that the "performance critical" parts will be quite similar, and the "glue" around it will be what differentiates the C++ from the Smalltalk. Static typing won't make that glue go any faster AT ALL -- in fact, it might even slow it down (see 3 below).
        </li>
      </ol>
      <ol>
        <li>
           <em>Static typed compilers often ignore or can't use the static typing information any more effectively than their loosely-typed (or untyped) counterparts.</em> Much of the alleged benefits of static typing are not used at all by real compilers solving real problems. Try compiling your favorite piece of real C++ code and measuring its performance with optimizing turned off. Use conventional coding tactics to improve its performance by hand until you can't make it go any faster (a factor of two is usually easy for most unoptimized working code). Now turn on the compiler optimization and measure the improvement. Write Smalltalk that does the same thing, and optimize the Smalltalk. Ask the best Smalltalker you know to do the same. Compare the results.
        </li>
      </ol>
      <ol>
        <li>
           <em>I can write fast code too if it doesn't have to work.</em> Static typed compilers (especially C++) often force the programmer into such convoluted code that typical problems (especially in environments where the requirements are always changing) either cannot be solved at all or where the working code is painfully slow, so slow that any performance benefits gained from static typing are totally swamped by the losses resulting from the confused and confusing code.
        </li>
      </ol>
      <p>
        The commercial Smalltalk market, as small as it is, exists (especially in the financial and insurance domains) because a huge portion of the C++ development projects died after 24-36 months, millions of dollars expended, and with no working results at all. Ask BankersTrust about LS1, their first effort to write a commercial Loan Syndication system. Ask Fidelity about its C++ projects. Ask ex-Easel folks about Easel Workbench II (the follow-on to Easel Workbench). Smalltalk succeeded in these environments 
        because shops were able to get something running quickly and were then able to speed up the slow parts, based on real measurements of real problems.
      </p>
      <dl>
        <dt> </dt>
        <dd><em>1. Whoa, cowboy. If you declare the parameter as a float, the parameter is bloody hell a float. What you're complaining about is why </em><a href="DefensiveProgramming.html">DefensiveProgramming</a> is stupid. No kidding, eh. On the other hand, if you are talking about DaveUngar's amazing type pipelining system, well, that's DaveUngar and he's amazing. You won't find that in Smalltalk.<em></em></dd>
      </dl>
      <dl>
        <dt> </dt>
        <dd><em>When "the parameter is bloody hell a float", then I have to write and maintain other code to handle the three other types it might be -- not because of </em><a href="DefensiveProgramming.html">DefensiveProgramming</a>, but because the parameter can legitimately be one of four types. And I also have to write and maintain the code that tests and dispatches to the right place based on the type. The WORST offender for this is Java, but see below. In the complex number example I cited above, what happens to your code when the complex numbers can have coefficients of each numeric type?<em></em></dd>
      </dl>
      <dl>
        <dt> </dt>
        <dd><em>2. You are flat out wrong. Trust me. I </em><a href="ZenSlap.html">ZenSlap</a> the compiler every day with casting.<em></em></dd>
      </dl>
      <dl>
        <dt> </dt>
        <dd><em>So do I whenever I have to use a strongly-typed language. That's my point.</em></dd>
      </dl>
      <dl>
        <dt> </dt>
        <dd><em>3. Yes, this is the whole argument in favour of dynamic typing over static typing. Yawn. A better answer is go sideways in the discussion and </em><a href="AlternateHardAndSoftLayers.html">AlternateHardAndSoftLayers</a>, or do what we did and write a <a href="CodeGenerator.html">CodeGenerator</a> in Perl that emits optimized C++ by extending the C++ syntax. -- <a href="SunirShah.html">SunirShah</a><em></em></dd>
      </dl>
      <dl>
        <dt> </dt>
        <dd> <em>Agreed, </em><a href="AlternateHardAndSoftLayers.html">AlternateHardAndSoftLayers</a> is my favorite approach.<em></em></dd>
      </dl>
      <dl>
        <dt> </dt>
        <dd><em>P.S. Don't ever again use Java as an example of anything except poor design. Why not go whole hog and use </em><a href="VisualBasic.html">VisualBasic</a> as an example of a dynamically typed language?<em></em></dd>
      </dl>
      <dl>
        <dt> </dt>
        <dd> <em>Also agreed, about both Java and </em><a href="VisualBasic.html">VisualBasic</a>. -- <a href="TomStambaugh.html">TomStambaugh</a><em></em></dd>
      </dl>
      <p>
        <em>I partially subscribe to your view, although I'm no Smalltalk fan, and I wished I was doing C++ rather than Java at this time. I think that </em><a href="LucaCardelli.html">LucaCardelli</a> points out that C++ fares pretty badly in areas other than <a href="EconomyOfExecution.html">EconomyOfExecution</a>. But the fact is <a href="EconomyOfExecution.html">EconomyOfExecution</a> is a problem of Smalltalk and other OO languages (liek Java), whether you like it or not. What you're missing here is that not everybody does banking applications, and the fact that Smalltalk is good enough in certain domains is more related to the fact that <a href="EconomyOfExecution.html">EconomyOfExecution</a> is less important there. Try to implement encryption or any other performance critical algorithms in Smalltalk, those are real problems to be solved too. Well I did it in Java and wasn't happy at all, I take the liberty to presume Smalltalk is similar. <em> </em>
      </p>
      <p>
        <em>On the other hand, I don't think there's enough convincing evidence of a corelation between the </em><a href="EconomyOfExecution.html">EconomyOfExecution</a> and either EconomyOfSmallScaleDevelopment or EconomyOfLargeScaleDevelopment (the two notions Cardleli uses for EconomyOfImplementation). What you mentioned here is anecdotical evidence. The reason for <a href="BadEngineeringPropertiesOfOoLanguages.html">BadEngineeringPropertiesOfOoLanguages</a> was to call for more research and implementations towards better languages, not to set existing ones in stone. I don't think that adopting a "Smalltalk is all we need and all there ever needs to be" approach is particularly helpful or justified. The reason for this page wasn't to bash Smalltalk nor to glorify C++. -- <a href="CostinCozianu.html">CostinCozianu</a><em></em>
      </p>
      <p>
        Regarding anecdotal evidence: Take a second read of the article. It's one big anecdote.
      </p>
      <hr/>
      <p>
        Mostly type information is used for instruction selection. C++ can make use of a floating point register for a temp because it knows the temp will always be a float. Smalltalk moves the temp data back and forth from memory. (Some lisps select instructions just as well but they have been rejected for reasons discussed elsewhere. See <a href="WhyWeHateLisp.html">WhyWeHateLisp</a>) Type information can also improve performance because...
      </p>
      <ul>
        <li>
           optimizers work better if they have more information.
        </li>
        <li>
           run-time checks can be replaced by compile-time proofs.
        </li>
      </ul>
      <p>
        Slower languages exist because they excell in other dimentions such as ease of use. As <a href="GeraldWeinberg.html">GeraldWeinberg</a> said: "I can ALWAYS build faster code if it doesn't have to work." There are definite trade-offs with <a href="StaticTyping.html">StaticTyping</a> that cannot be ignored in the real world.
      </p>
      <p>
        There are different trade-offs that various statically-typed languages took, Java being a prime example. For example the project disasters related to C++ are mainly due to the liberal use of pointers and memory management, which are related to <a href="EconomyOfExecution.html">EconomyOfExecution</a>, but are not related to <a href="StaticTyping.html">StaticTyping</a>. 
      </p>
      <p>
        But it is still unclear if there's anything inherent to <a href="StaticTyping.html">StaticTyping</a> that prevents whatever desirable properties you see in dynamically typed languages. Languages such as ML and its variants tend to prove the opposite.
      </p>
      <p>
        Java has static type checking but <a href="JavaTypingWasSimple.html">JavaTypingWasSimple</a>. This has all kind of benefits, but it also leads to loss of typing (ask yourself how many times have you used the cast operator lately) and consequently it hurts <a href="EconomyOfExecution.html">EconomyOfExecution</a> because all kinds of checkings have to happen at run-time and because the optimizer (even if it is inside the just in time compiler/<a href="HotSpot.html">HotSpot</a> ) is prevented from doing its job well.
      </p>
      <p>
        <em>A cast operator is part of every static-typed language I'm aware of (even Eiffel has one), and is one of the reasons why so few commercial static-typed language compilers actually use the static typing information for anything useful. The presence of the cast operator means that runtime checks have to happen anyway, and the runtime checks of C++ or Java are not inherently any faster than those of Smalltalk. -- </em><a href="TomStambaugh.html">TomStambaugh</a><em></em>
      </p>
      <p>
        The fact that there is a cast operator, doesn't mean you have to use it. In Java, yes, you really have to. But in C++ you have to deal with exceptionally rare situations to have to actually use a dynamic_cast, some authors even question the usefulness of the dynamic_cast operator. The same applies to Ada and many other languages.
      </p>
      <p>
        <em>Let me beat the carcass of this dead horse a little more. The fact that there is a cast operator means that the compiler has to take it into account, whether or not the developer uses it. Thus, the compiler has to emit code that does something predictable at, for example, code position "A" (such as an assignment) when an incorrect casts occur at code position "B" (such as inside an independently compiled module). The effort to reliably sort out what can and can't occur at runtime causes most commercial vendors to walk away from the whole problem. The result is that the overall </em><a href="EconomyOfExecution.html">EconomyOfExecution</a> for real applications that work is about the same for modern commercial implementations of Smalltalk, C++, and Java. -- <a href="TomStambaugh.html">TomStambaugh</a><em></em>
      </p>
      <p>
        Tom, I think the burden of proof is on you. Come up with a practical example, cite a relevant study or something, but don't just state something you believe to be true, while it's common sense that it isn't. C++ compiler doesn't insert anything at point A to catch a bad cast at pont B, on the contrary, the most likely behaviour in such a situation is a total program crash. The <em>only</em>' thing that might slow down a C++ program is setting up exception traps, which can be disabled anyway, by a simple compiler switch. There's also the problem of object allocation and garbage collection, and tons and tons of other things  - like the 23 machine instructions that I got in <a href="ComplexNumberTest.html">ComplexNumberTest</a>. Can you back your claim with some real evidence ? Just spear us of the <em>real applications that work</em>. I've done a few <em>real applications that work</em> in C, C++, <a href="DelphiLanguage.html">DelphiLanguage</a> and BorlandPascal, developing in any of these languages (without garbage collection, dynamic typing and the goodies of Smalltalk) ain't the nightmare that smalltalkers would like us to believe it is. On the contrary, it is very enjoyable. -- CC
      </p>
      <p>
        <em>Excuse me, Costin, but I'm responding to two assertions, from above:</em>
      </p>
      <dl>
        <dt> </dt>
        <dd>* optimizers work better if they have more information.</dd>
      </dl>
      <dl>
        <dt> </dt>
        <dd>* run-time checks can be replaced by compile-time proofs.</dd>
      </dl>
      <p>
        <em>If anything is to be "proven", it is these assertions, together with the commonly asserted (but in my experience mythical) implication "Applications written in static typed languages are faster". I will not fall into the semantic trap of attempting to prove a negative.</em>
      </p>
      <p>
        Tom, what you call assertions are common places in <a href="ComputerScience.html">ComputerScience</a>. There's no point in debating these with you unless you come up with new evidence or new ideas. Who's talking of proving the negative ? What is your experience, what have you measured ?
      </p>
      <p>
        <em>I don't doubt that you have written real applications in C, C++, </em><a href="DelphiLanguage.html">DelphiLanguage</a>, and BorlandPascal. So have I. The claim I dispute is YOUR implied claim that those applications would have had worse performance had they been written in Smalltalk (I agree with you that Java would have KILLED them... :)). If the strongly typed compiler of your choice in a language that includes casts really DOES blindly execute an operation without checking types as you argue it should, I argue that for an application of any complexity, you will see a large amount of unexplained, apparently random application crashes -- precisely the reliability problem that strong-typing is alleged to "solve". And if you ALSO skipped the garbage collection, I'll bet your applications also had core leaks and a whole different set of apparently random failures caused by dereferencing stale pointers created the effort to avoid the "overhead" of the garbage collector. If you REALLY mean that you ship code with BOTH unchecked casts AND manual storage management, I would respond that you make my argument for me more effectively than I can make it myself. tms<em></em>
      </p>
      <p>
        You can dispute no matter how much you want, but please try to come up with 23 machine instructions in <a href="ComplexNumberTest.html">ComplexNumberTest</a>. 
      </p>
      <p>
        <em>I agree that we should continue this discussion in </em><a href="ComplexNumberTest.html">ComplexNumberTest</a>. Please see my additions to that page. -- <a href="TomStambaugh.html">TomStambaugh</a><em></em>
      </p>
      <p>
        And about my compiler, it doesn't execute blindly, it <strong>proves</strong> at <strong>compile time</strong>, because you usually don't use dynamic_cast in C++. Even in case you do use dynamic_cast, only yhen it inserts a check at runtime. Let's get serious and delete this crap, shall we ?
      </p>
      <p>
        <em>Using dynamic_cast<> whenever I might be tempted to use a C-style cast was ingrained into me by days upon days of wasted time debugging what turned out to be "slicing" problems on a largish C++/MFC project. I find it hard to believe that the contributors on this page who speak from experience with C++ won't at least acknowledge that C-style casting can lead the unsuspecting OO programmer into serious trouble.</em>
      </p>
      <p>
        A C-style cast has the effect of adding an axiom to the compiler's "proof" system. If you add a false axiom, then you should expect to get false conclusions. <a href="DontDoThat.html">DontDoThat</a>!
      </p>
      <hr/>
      <p>
        Tom, I respect your opinion in matters of Smalltalk usually, but you are really wanking it here. Most Smalltalks are one tenth the speed of C. DaveUngar, because he's amazing, pushed Self to one half the speed of C years and years ago. Worse, C++ is faster than C because it has more tricks to pull. And your whole claim that casting costs in C++ is totally bogus. I know how much a cast costs because I spend all day measuring the impact of various C++ expressions against a variety of compilers on a variety of platforms using a variety of processors. The only thing you have to worry about when casting is integer extension and calls to emulated 32-bit multiplies and divides. Unless you are using dynamic_cast<>, but dynamic_cast<> is evil. -- <a href="SunirShah.html">SunirShah</a>
      </p>
      <p>
        <em>Let's continue this discussion at </em><a href="ComplexNumberTest.html">ComplexNumberTest</a>, and focus it on real stuff. -- <a href="TomStambaugh.html">TomStambaugh</a><em></em>
      </p>
      <hr/>
      <p>
        <em>I spend all day measuring the impact of various C++ expressions...</em>
      </p>
      <p>
        That leave you much time for implementing things with <a href="BusinessValue.html">BusinessValue</a>? ;)
      </p>
      <hr/>
      <p>
        Somebody asserted during a refactoring of this page that: <em>Object-oriented principles run counter to performance. </em>
      </p>
      <p>
        It wasn't clear if the statement itself was asserted, or if the assertion was that the discussion on this page was about that. I believe there's no evidence to affirm that "Object-oriented principles run counter to performance". Please clarify.
      </p>
      <p>
        <em>While I didn't say that, it's definitely true. Objects boil down to extra level of indirection. Every level of indirection slows you down. -- </em><a href="SunirShah.html">SunirShah</a><em> (unless it's resolved at compile-time, as for C++ templates -- </em><a href="DaveWhipp.html">DaveWhipp</a>).
      </p>
      <p>
        And speeds you up as well - <a href="SlowDownToSpeedUp.html">SlowDownToSpeedUp</a>.
      </p>
      <hr/>
      <p>
        <a href="CategoryCpp.html">CategoryCpp</a>
      </p>
    </div>
  </body>
</html>