<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Paralleli Zation
      </h1>
      <p>
        <a href="EditHint.html">EditHint</a>: this page title is <a href="UgLy.html">UgLy</a>.  Maybe move?
      </p>
      <hr/>
      <p>
        <em>From </em><a href="SynchronizationStrategies.html">SynchronizationStrategies</a>:<em></em>
      </p>
      <p>
        One way to look at parallelization is to consider the serial program to already be a parallel program, but one that acquires a single
        global lock at the start of execution, then releases it just before terminating.
      </p>
      <p>
        From this perspective (sometimes called ``Giant Locks<em>), the idea is to move toward finer-grained locking until you reach the point</em>
        where where the <a href="SpeedUp.html">SpeedUp</a> and <a href="ConTention.html">ConTention</a> forces balance the <a href="OverHead.html">OverHead</a>, Economics, and <a href="DevMaintCost.html">DevMaintCost</a> forces.  Each finer ``grain<em>, or</em>
        <a href="CriticalSection.html">CriticalSection</a>, are usually identified by looking at the data structures used by the program, since the data is what must be
        protected from concurrent access. One approach is to provide a lock for the code accessing the data (<a href="CodeLocking.html">CodeLocking</a>), and another is to
        provide locks for the data items themselves (<a href="ParTition.html">ParTition</a>).
      </p>
      <p>
        Other approaches (e.g., AtomicInstructions, <a href="WaitFreeSynchronization.html">WaitFreeSynchronization</a>, <a href="DataOwnership.html">DataOwnership</a>, <a href="ReadWriteLock.html">ReadWriteLock</a>) are also possible. These
        approaches are subject to other forces from the application (such as <a href="ReadToWriteRatio.html">ReadToWriteRatio</a>).
      </p>
      <p>
        Parallelization efforts will often be subject to LaTency, BandWidth, and CacheArchitecture forces (or maybe they are contexts, I am
        never sure, is there a <a href="ForceContextDuality.html">ForceContextDuality</a>?) from the underlying machine.  In addition, parallelization efforts can easily lead to well-hidden <a href="DeadLock.html">DeadLock</a>.
      </p>
      <p>
        <a href="CurtSchimmel.html">CurtSchimmel</a> presents an excellent and thorough coverage of CacheArchitecture forces in his book on parallelizing Unix kernels.
      </p>
      <p>
        The <em></em>GuideToParallelProgramming<em>, available from Sequent gives a good overview of software aspects of parallelization.</em>
      </p>
      <p>
        One need in patterns for parallelism, as (IMHO) patterns everywhere is a good way to index into them -- to find the pattern you
        need based on the context of the problem that you are trying to solve or the attributes of the job you are trying to get
        done.  One possible index displays the hierarchy of concurrency primitives for a given computer system and application.  The hierarchies that I have looked at tend to have few branch-points and therefore resemble saguaro cacti, hence, <a href="CactiOfConcurrency.html">CactiOfConcurrency</a>.
      </p>
      <p>
        Book: <a href="PatternsForParallelProgramming.html">PatternsForParallelProgramming</a>
      </p>
      <hr/>
      <p>
        <a href="CategoryConcurrency.html">CategoryConcurrency</a> <a href="CategoryConcurrencyPatterns.html">CategoryConcurrencyPatterns</a>
      </p>
    </div>
  </body>
</html>