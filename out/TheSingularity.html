<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        The Singularity
      </h1>
      <p>
        Read this first: it's all here for the 'layman': <a href="http://yudkowsky.net/tmol-faq/meaningoflife.html">http://yudkowsky.net/tmol-faq/meaningoflife.html</a> 
        (and it's pretty darn important to your life).
      </p>
      <ul>
        <li>
           <a href="EditHint.html">EditHint</a>: bad link?
        </li>
      </ul>
      <p>
        The above link was later called by Yudkowsky "The greatest mistake he ever made". Unfortunately he doesn't seem to believe in taking them down, but there's a nifty warning at the top. Oh, the singularity is still a Big Deal, but if you think AIs would *automatically* be nice to us you're sadly (and fatally) deluded.
      </p>
      <p>
        Also, <a href="http://www.aleph.se/Trans/Global/Singularity/">http://www.aleph.se/Trans/Global/Singularity/</a> , <a href="http://www.singularitywatch.com/">http://www.singularitywatch.com/</a>
      </p>
      <hr/>
      <p>
        Someone said: A millennialist religious idea expounded by some followers of <a href="TransHumanism.html">TransHumanism</a>.
      </p>
      <p>
        I disagree: Millennialism is the belief that just because we have reached a round number in an arbitrary base ten number system, something very significant is going to happen. Also <a href="TransHumanism.html">TransHumanism</a> is pretty rejective of religion. Read the Meaning of Life link above to see what I mean.
      </p>
      <p>
        <em>Millenialism is often applied to cases where the final number isn't round, and I think the gist of the comment was that transhumanism itself is a religion, at least in such extreme forms (in which case it's obviously going to reject other religions).  Compare the guide linked above.</em>
      </p>
      <hr/>
      <p>
        The basic gist,
      </p>
      <p>
        At some time in the not too distant future (<em>A lot of people and cultures believe [</em><a href="http://www.2013.com">http://www.2013.com</a> 2012]<em>), technological change will accelerate so much and humanity will be so linked together and so able to communicate with itself that in a timeless moment of transcendence we will pop into a higher plane of existence in a puff of boundless optimism.</em>
      </p>
      <p>
        The idea has antecedents, especially <a href="TeilhardDeChardin.html">TeilhardDeChardin</a>'s OmegaPoint; but the modern version is generally credited to the <a href="ScienceFiction.html">ScienceFiction</a> of <a href="VernorVinge.html">VernorVinge</a>.
      </p>
      <p>
        Actually the gist in <a href="VernorVinge.html">VernorVinge</a>'s essay on <a href="TheSingularity.html">TheSingularity</a> <a href="http://www.ugcs.caltech.edu/~phoenix/vinge/vinge-sing.html,">http://www.ugcs.caltech.edu/~phoenix/vinge/vinge-sing.html,</a> and in recent works of "speculative fiction" by RayKurzweil [ISBN: 0140282025], HansMoravec [ISBN: 0195136306] and <a href="KenMacLeod.html">KenMacLeod</a> [ISBN: 0765305038], seems to carry more implications for our simply being surpassed and abandoned? ignored? tolerated? eliminated? by our superintelligent machine progeny. Here's an excerpt from Vinge's essay:
      </p>
      <dl>
        <dt> </dt>
        <dd>In the 1960s there was recognition of some of the implications of superhuman intelligence. I. J. Good wrote:</dd>
      </dl>
      <dl>
        <dt> </dt>
        <dd>Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an "intelligence explosion," and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the _last_ invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is more probable than not that, within the twentieth century, an ultraintelligent machine will be built and that it will be the last invention that man need make.</dd>
      </dl>
      <dl>
        <dt> </dt>
        <dd>Good has captured the essence of the runaway, but does not pursue its most disturbing consequences. Any intelligent machine of the sort he describes would not be humankind's "tool" -- any more than humans are the tools of rabbits or robins or chimpanzees.</dd>
      </dl>
      <p>
        Opinions differ:
      </p>
      <hr/>
      <p>
        As a big Vinge fan, I have to say that I believe his inclusion of The Singularity in 'Marooned in Realtime' was primarily so that the storyline was possible and not due to any particular belief in it or any specific concept of what it was. Characters in the book were explicit on the point that it wasn't intended to be anything specific, though many possibilities were raised. Of course, it is a legitimate concept; I just felt that this comment was worth making. -- <a href="DanielKnapp.html">DanielKnapp</a>
      </p>
      <p>
        He's been writing about the Singularity in one way or another for some time. In one of the explanatory essays in <em>True Names and Other Dangers</em> (a collection of his Singularity-related short stories), he talks about a story he was working on for John Campbell about some kind of super-intelligence. Campbell told him he couldn't write that story, because he simply couldn't grok that much intelligence. Since then he has focused on stories in which for some reason or another the characters are near but not over the line of the Singularity.
      </p>
      <hr/>
      <p>
        The books of <a href="IainBanks.html">IainBanks</a> explore life after The Singularity, and how people interact with such ultraintelligent machines.
      </p>
      <p>
        Singularity, The. The Techno-Rapture. A black hole in the Extropian worldview whose gravity is so intense that no light can be shed on what lies beyond it.
        From <em>Godling's Glossary</em> by David Victor de Transend.
      </p>
      <p>
        The Singularity is a common matter of discussion in <a href="TransHumanism.html">TransHumanism</a> circles. There is no clear definition, but usually the Singularity is meant as a future time when societal, scientific and economic change is so fast we cannot even imagine what will happen from our present perspective, and when humanity will become posthumanity. Another definition is used in the Extropians FAQ, where it denotes the singular time when technological development will be at its fastest. Of course, there are some who think the whole idea is just technocalyptic dreaming. -- sg
      </p>
      <p>
        Indeed, singularity is not just about technology. <a href="TeilhardDeChardin.html">TeilhardDeChardin</a>: "Someday, after we have mastered the winds, the waves, the tides and gravity, we shall harness for God the energies of love. Then, for the second time in the history of the world, [hu]mankind will have discovered fire." Further, to speak of "the" singularity is pretty us-centric. The transition from non-life to life (in de Chardin's terms, geosphere to biosphere) is a shift as profound as anything we are approaching. (Well it would be, except that as far as we can tell, life ought to be abundant anywhere there are planets of the appropriate size and temperature. Certainly organic molecules are abundant in interstellar space.)
      </p>
      <hr/>
      <p>
        I think that a more likely outcome than The Singularity is what one might call "The Dissolution": we will be more and more dependent upon technology and it will become more and more complex until, finally, it will all collapse. Some essential system will crash, and everything else will crash or malfunction as a consequence. It will be impossible to get the whole thing running again, because it will be so complicated, decentralized, and interdependent that there is no correct order for starting it all up again piece by piece.
      </p>
      <p>
        So we'll just have to start over again with fire, the plow, and the wheel.
      </p>
      <p>
        The idea that superhuman intelligence will just inevitably "happen" as a natural consequence of technological progress seems ludicrous to me.
      </p>
      <p>
        -- <a href="KrisJohnson.html">KrisJohnson</a>
      </p>
      <p>
        If you replace "technology" with "biology" does it sound so ludicrous? Is there a difference? -- <a href="AndyPierce.html">AndyPierce</a>
      </p>
      <p>
        I'm not sure what the timeline is like for the development of the notion of <a href="TheSingularity.html">TheSingularity</a>, but <a href="TerenceMcKenna.html">TerenceMcKenna</a> wrote quite a bit about it in his books. The view he held was neither technologically nor biologically based, but stemmed from the fact that the increase in human knowledge has been growing at an exponential rate, and positing that based on the rate of increase there will be some point (I think forecasted for 2026 or so) <em>[</em><a href="http://www.2013.com">http://www.2013.com</a> 2012]<em> where the rate of knowledge increase will exceed the passage of time. This is the moment that humankind could possibly enter a new phase of existence. It's a neat idea, and one that you can have lots of fun speculating about. It could be a point at which our notion of time is redefined to more minute scale, it could be the point at which we begin to perceive more than 4-dimensions, it could be </em>TheRapture, etc.... Fun! Some information regarding this idea from <a href="TerenceMcKenna.html">TerenceMcKenna</a> at <a href="http://www.sculptors.com/~salsbury/Articles/singularity.html">http://www.sculptors.com/~salsbury/Articles/singularity.html</a> -- <a href="SeanMcNamara.html">SeanMcNamara</a>
      </p>
      <p>
        <em>Clearly, knowledge will </em>'not<em>' grow exponentially for very long.</em>
      </p>
      <p>
        <em>Clearly, population will </em>'not<strong> grow exponentially for very long either. knowledge and people are related. Knowledge is assimilated by people who by necessity must specialize given the rapidity of development. Thus knowledge can not really grow any faster than an assimilating population (unless you are talking about knowledge as </strong><a href="AutomatedIntelligence.html">AutomatedIntelligence</a>).<em> </em>
      </p>
      <p>
        Certainly, a possible outcome is that we won't be able to assimilate knowledge, so the rate of new knowledge will slow. This seems quite plausible from a behaviorist/synthesists point of view. If, however, you consider the possibility of a collective mind from which knowledge is revealed, then the possibility of <a href="AutomatedIntelligence.html">AutomatedIntelligence</a> exists. This is an interesting possibility since if we accept time to be a dimension that we merely perceive as serial in nature, then everything that ever will be invented, has already been invented. If this is the case then wouldn't this be grounds for the collective consciousness from whence all thought springs? I'm not saying any of this is the case, only that it is a fruitful ground for speculation.
      </p>
      <hr/>
      <p>
        As may have been gathered by now there are many varieties of "the Singularity".  The simplest and most brutally literal, harking back to Vinge's stories, is "the point at which science fiction authors stop being able to tell stories"; this is correlated with the rise of greater-than-human intelligence.  This idea goes back to <a href="LarryNiven.html">LarryNiven</a>, actually, who had superhuman Pak protectors (but with extremely constrained values) and maybe some stuff in <em>A World Out of Time</em>.
      </p>
      <p>
        All the stuff about rates of progress going to infinity or analogies to mathematical functions are, in my opinion, from people taking "Singularity" too literally and trying to do something mathematical with it.  This is just wrong.
      </p>
      <p>
        <a href="KenMacLeod.html">KenMacLeod</a>'s <em>The Sky Road</em> has a brief bit about being post-Singularity; this plus the Culture show a usage referring as much to the presence of nanotech and biological immortality as to superhuman intelligence.  Basically the "whoah, weird" point.
      </p>
      <p>
        There's questions about whether we'd notice the Singularity happening if we live through it -- some people prefer the term "Horizon", something which recedes as you approach it, and echoes event horizon, and echoes <a href="RobertHeinlein.html">RobertHeinlein</a>'s <em></em>BeyondThisHorizon<em> too.  Others wonder if a Singularity hasn't already happened, with the acceleration of scientific progress since 1800.</em>
      </p>
      <p>
        -- DamienSullivan (author of the <a href="VernorVinge.html">VernorVinge</a> page)
      </p>
      <hr/>
      <p>
        SL4 Wiki is specifically intended for Singularity-related discussion: <a href="http://sl4.org/wiki">http://sl4.org/wiki</a>
      </p>
      <hr/>
      <p>
        Could somebody tell me what makes singularitarians think that increasing <em>intelligence</em> will make it easier to invent a "correct" meaning of life?
      </p>
      <p>
        <em>The meaning of life is something people think about. Increasing "intelligence" helps people think better about everything. Therefore, increasing "intelligence" helps people think better about the meaning of life. Yes ?</em>
      </p>
      <p>
        From <a href="http://yudkowsky.net/tmol-faq/meaningoflife.html">http://yudkowsky.net/tmol-faq/meaningoflife.html</a> :
      </p>
      <p>
        The more intelligent you are, the better your chance of discovering the true meaning of life, the more power you have to achieve it, and the greater your probability of acting on it (16).
      </p>
      <p>
        <em>This assumes that life has meaning.  My vast and superior intelligence has deduced that life is meaningless, aside from the meaning we assign to life in order to amuse ourselves.</em>
      </p>
      <ul>
        <li>
           Your intelligence would indeed have to be vast to make such an observation.  Those of us in the subset of Life, don't have the authority to determine the <a href="MeaningOfLife.html">MeaningOfLife</a>.  Only the Being responsible for creating life has that authority.  Of course, we are ChasingRabbits here, as the topic is <a href="TheSingularity.html">TheSingularity</a> rather than <a href="TheMeaningOfLife.html">TheMeaningOfLife</a>. -- <a href="BrucePennington.html">BrucePennington</a>
        </li>
      </ul>
      <p>
        You may quite well just have reached a local maximum and can't get to the real answer until your search strikes far enough in an unlikely direction to find a steeper climb.
      </p>
      <ul>
        <li>
           Indeed so; anyone who stops at this point is merely demonstrating that, despite their self-assessed intelligence, they fall well short of the thinkers preceding them. The well-known answer to go beyond the above stage is to say "fine, say there is no externally-imposed meaning of life; well and good, then we have individual freedom to invent <strong>our own</strong> meanings for our own lives", and the debate continutes unabated from there. (All of which is simply adding details to the already-stated point that such a view is a local maximum, not a global maximum.)
        </li>
      </ul>
      <p>
        ...
        # 16:  Some people disagree with that last part.  They are, in fact, wrong.  (17).  But even so, very few people think that being more intelligent makes you intrinsically less moral.  So, when you run the model through the algebraic goal system, it's enough to create the differential of desirability that lets you make choices (see below).
      </p>
      <p>
        # 17:  Intelligence isn't just high-speed arithmetic, or a better memory, or winning at chess, or other stereotypical party tricks.  Intelligence is self-awareness, and wisdom, and the ability to not be stupid, and other things that alter every aspect of the personality.
      </p>
      <hr/>
      <p>
        Could somebody tell me what makes singularitarians think that utilitarianism is the ultimate value philosophy?
      </p>
      <p>
        <em>Why do you think we think that ?</em>
      </p>
      <hr/>
      <p>
        The webcomic A Miracle of Science (<a href="http://www.project-apollo.net/mos/)">http://www.project-apollo.net/mos/)</a> depicts two possible aspects of <a href="TheSingularity.html">TheSingularity</a>: the 
        Martian 'collective intelligence' on the one hand, and the rise of MadScientists (see <a href="ScienceRelatedMemeticDisorder.html">ScienceRelatedMemeticDisorder</a>) on the other. - <a href="JayOsako.html">JayOsako</a>
      </p>
      <p>
        Someone here, who has read <a href="TheSingularityIsNear.html">TheSingularityIsNear</a> and wants to elaborate on this nearly empty page? -- fp
      </p>
      <p>
        Sure: I don't think that there is/will be a singularity of that kind. My argument runes the same way as on <a href="SimulationArgument.html">SimulationArgument</a>: 
        I think that there are (and we we will discover) inherent complexity limits due to energy required to keep the complexity that make the hypothetical "planet computers" etc. infeasable. Just have a look at our MostComplexSystemThatEverWorked. I think though that AI is possible - after all BI is possible so there are not theoretical complexity limits for that,
      </p>
      <hr/>
      <p>
        Robots are probably gonna kick our ass because they have one advantage over biological systems: they copy the best minds instead of have to start over again for each new generation.
      </p>
      <p>
        How could we fix this? Thoughts?
      </p>
      <hr/>
      <p>
        <a href="CategoryFuture.html">CategoryFuture</a>
      </p>
    </div>
  </body>
</html>