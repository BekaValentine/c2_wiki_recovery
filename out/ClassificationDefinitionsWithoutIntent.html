<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Classification Definitions Without Intent
      </h1>
      <p>
        It's been suggested in [asdflkasd] that definitions of "classification" must involve "intent". This topic explores the possibility of definitions that don't involve intent or related concepts such as "goal" and "purpose". It's an important issue because it relates to other issues, such as the definition of "types".
      </p>
      <hr/>
      <p>
        <strong>Classification is compression</strong>. For example, to a regular fish, "moving", "large", "big teeth" is compressed into "dangerous". Multiple attributes are compressed into a single attribute. 
      </p>
      <code>
        if (object.size > threshold and object.ismoving and object.hasTeeth) <br/>
        then alarm_flag = true;<br/>
      </code>
      <p>
        The fish doesn't even need to be conscious of this, it may be instinct. Imagine similar behavior in a tiny worm or bacterium, which has no "intent", but is acting <strong>merely on instinct</strong>, yet still does classification: certain sensory input patterns trigger a response or condition. A single neuron can perform such classification: if the summed signal from enough of the inputs exceeds a threshold, then the output fires a signal.  (An analog version is slightly more involved, but the concept is the same.)
      </p>
      <p>
        In practice its usually compression for local or task-specific needs. We use it to simplify our models. But this is the "why", and does not need to be part of the definition.
      </p>
      <p>
        --top
      </p>
      <hr/>
      <p>
        <strong>Classification is Mapping</strong> (candidate definition)
      </p>
      <p>
        This is a more general view than compression in that reduction of information is not assumed.  Mapping turns one set of variables into a different set, but these are still influenced by the values of the original but are not identical (in all cases). In the fish example, "large", "moving", and "teeth", are remapped into "danger".
      </p>
      <p>
        <em>Are you claiming that ALL functional mappings are classifications?  If not, how do you tell the difference?</em>
      </p>
      <p>
        I know of no exception at this point, but won't rule one out.
      </p>
      <p>
        <em>You appeal to your ignorance?</em>
      </p>
      <p>
        It's called keeping an open mind. You should try it sometime.
      </p>
      <p>
        <em>Keeping an open mind requires I give new ideas a fair trial as I attempt to shoot them down, not that I embrace them without filter, give them pet names, then fight to defend them until somebody else kills them.  I mean, there's "open" and then there's "Johnny on the Spot".</em>
      </p>
      <p>
        <em>Then I shall offer you the opportunity to explore and shoot down your own ideas.  Please justify how each of these either are not mappings or are classifications: integer representations, diagonalization of rational numbers, language translations, language compilation, MP3 encodings, integer arithmetic, and HTML displays.</em>
      </p>
      <p>
        You have not identified the input and output mappings in many cases. 
      </p>
      <p>
        <em>I trust you are able to ask questions if you are unfamiliar with these cases.</em>
      </p>
      <p>
        As far as language translation, we probably would not call it "classification" per se, although classification is probably involved in the process. In ordinary speech, there is usually compression of information for classification. But I couldn't say there is always compression. It may be comparable to the <a href="DefinitionOfLife.html">DefinitionOfLife</a>. No single criteria may be enough.
      </p>
      <hr/>
      <p>
        (Regarding "Classification is Compression")
      </p>
      <p>
        <em>Are you claiming that ANY compression algorithm qualifies as classification?  If not, how do you tell the difference?</em>
      </p>
      <p>
        Perhaps only loss-full compression.
      </p>
      <p>
        <em>So you are claiming that any LOSSY compression is classification.  Can you please explain this in the context of my MP3 collection?</em>
      </p>
      <p>
        It re-classifies sound waves into a frequency realm. It also rids what we humans cannot perceive such as quiet sounds close to the frequency of loud ones, thus making a human-centric abstraction of sound. 
      </p>
      <p>
        <em>I can hardly believe a logical and astute fellow such as yourself would regress to 're-classify' as an explanation of 'classification'.  And it seems from the second sentence that you know what 'lossy compression' means, but I wouldn't have brought up MP3s if I didn't know that much.  Be honest: are you just rambling in hopes you'll hit a good answer?</em>
      </p>
      <hr/>
      <p>
        <em>Also, since you wish to get away from "intent or related concepts such as 'goal' and 'purpose'", are you trying to say that instinct has no purpose?</em>
      </p>
      <p>
        That is a very interesting philosophical question.  Generally I consider "purpose" part of an observer's perspective. Is an amoeba reacting to stimuli any more "purposeful" than a water droplet hitting a pond? It's almost a religious question. We tend to anthropomorphise life much more than inanimate objects.
      </p>
      <p>
        <em>If you consider issues of computation to be religious discussions, is it any wonder you get into so many holy wars?</em>
      </p>
      <p>
        Or vise versa. Until "intent" is measurable in a consensus way, it will continue to be a problematic concept to tie definitions to.
      </p>
      <p>
        ["Intent" is measurable in a consensus way.  The objection appears to be yours and yours alone.]
      </p>
      <p>
        Bullsh8t! "I know it when I see it and that's good enough" is not good enough.
      </p>
      <p>
        [Are you claiming that the intent of <a href="QuickSort.html">QuickSort</a> is not to put items in sorted order?  Or that the intent of opening a Berkeley socket is not to establish a connection with another socket?  You appear to be equating "intent" with "conscious will."]
      </p>
      <p>
        It may be to win a Turing Award for the inventor. I don't really know for sure. I can give a guess (see probability below), but it is only a guess based on a somewhat arbitrary (non-rigorous but experience-based) model of human behavior that I construct in my mind to try to figure out people. We don't always know the exact steps our mind takes to come up with a conclusion. It's "intuition" that is somewhat distance from science-quality scrutiny. Even in the lab where the guts can be monitored, some neural nets have grown too complicated for the researchers to dissect the steps for the result.
      </p>
      <p>
        [You are referring to the intent of the inventor and not the invention itself.]
      </p>
      <p>
        The "intent" of the program? WTF? It's just following mechanical rules. Does a snowflake crystal seed have intent to form a snowflake?
      </p>
      <p>
        [Certainly.  This is further evidence that you are equating "intent" with "conscious will", which it is not.] (written prior to 'snowflake' addendum)
      </p>
      <p>
        This appears to contradict your <a href="DefinitionsThatRelyOnIntent.html">DefinitionsThatRelyOnIntent</a> <a href="PageAnchor.html">PageAnchor</a> Same_Design_Different_Intent claim. The "intent" would then be independent of the author. If a monkey accidentally types quick-sort code, then it can still have intent?
      </p>
      <ul>
        <li>
           [I have not written anything on <a href="DefinitionsThatRelyOnIntent.html">DefinitionsThatRelyOnIntent</a>.  You are confusing me with someone else.]
        </li>
        <li>
           You were mis-classified, eh? How ironic. You guys really need to invent some kind of consistent handle. Make up a number if you have to.
        </li>
        <li>
           [No need.  We mostly agree.  Not entirely, because I've had some lengthy debates with Correspondent #2 re mutability in databases, definitions of "data" and suchlike, but for the sake of this topic and related ones regarding types and so forth, you can regard us as essentially interchangeable.]
        </li>
      </ul>
      <p>
        <em>Technically, for </em>'anything<strong> a monkey writes, one can invent a language for which that monkey-typing qualifies as quick sort code.  This goes the other direction: if everything a monkey types can be quick-sort code in some language or another, how do you judge?  You make assumptions as to the language, that's how.  And by doing so, you make assumptions as to the semantics, and therefore to the purpose and intent of the program.  You cannot logically make that assumption about semantics without </strong>implicitly<em>' taking a leap of faith to intent and purpose.  Unless you make that assumption, the monkey-typing is just that: monkey-typing.  It isn't even a program.</em>
      </p>
      <p>
        Maybe <a href="EverythingIsRelative.html">EverythingIsRelative</a>. But for practical considerations we are not dealing with all possible potential languages. Usually we are interested in something more specific. We can create an SQL-detector (classifier), for example, without having to mention "intent".
      </p>
      <p>
        <em>By resorting to claims of 'practical considerations' in a debate that is fundamentally philosophical, it seems what you really want to do treat 'intent' as 'the-word-that-shall-not-be-named' and sweep it under a rug with your other unmentionables.  For philosophical considerations, we ARE dealing with all possible potential languages.  We can </em>assume<em> something more specific, and in making that assumption we introduce semantics and, therefore, intent. We repeat the process for the program output: you can't even say an SQL-in garbage-out program is not an SQL detector unless you </em>assume<em> a particular language for the output.  I don't know about you, but I do not believe it proper to ignore the assumptions you are being forced to make and pretend they are non-consequential.  It's like hiding that "then a miracle happens here" step in (bad) math or logic.</em> -- #2 
      </p>
      <hr/>
      <p>
        Classification isn't a compression, so much as it is a <strong>let</strong>-assignment.  Assuming the same context of the fish above, something <em>is</em> dangerous if it is moving, has teeth, and is large.  That is:
      </p>
      <code>
        given self as fish:<br/>
        let dangerous other = (hasTeeth other) /\ (isLarge other) /\ (isMoving other)<br/>
        in  ...etc...<br/>
      </code>
      <p>
        <em>I'm sure that most </em><a href="EverythingIsa.html">EverythingIsa</a>'s have a take on it. --top<em></em>
      </p>
      <hr/>
      <p>
        <em>RE: We tend to anthropomorphise life much more than inanimate objects.</em>
      </p>
      <p>
        Perhaps you do.  I tend to objectify humans just as much as other animate objects.  Which, if either, of these sounds logical to you: (1) "Every human I know may be categorized animate constructs of physical matter."  (2) "Every animate construct of physical matter I know may be categorized as human."  I don't know which logic you use to make decisions, but to me (1) is logical (barring dead humans), and (2) is not, being contradicted by such things as plastic wind-up clapping monkeys.
      </p>
      <p>
        Where you keep saying "intent" and "purpose" requires humanizing things, I see you spouting a bunch of religious mumbo-jumbo.  It's as though you're claiming that "intent" can't be represented in any computational system <em>except</em> a human brain, but you just expect me to take you on your word.  If human brains can represent intent, do we have <em>any</em> reason to believe that other computational systems may not?  Are you going to invoke "free will"?
      </p>
      <ul>
        <li>
           <em>I don't care which kind of animal, thing, blob, tomato, or robot you assign "intent" to. Just come up with a consistent way to measure its presents beyond "I just know it when I see it".</em>
        </li>
        <li>
           P(~Intent|Observation) = P(~Intent n Observation)/P(Observation) = P((Accident or Random Event) n Observation) / P(Observation).  It derives of the same basic equation we (loosely) use to judge everything else (even "baseball-ness" and "SQL-ness").  For example, consider this sentence: the probability of this particular sentence being placed on the <a href="WikiWiki.html">WikiWiki</a> by random event can be assumed to be zero (I've never heard of text added by random event, much less in an English grammatical structure), so you must judge the relative probability of it being an accident.  What do you believe is the relative probability of this sentence being appearing in a topic like '<a href="ClassificationDefinitionsWithoutIntent.html">ClassificationDefinitionsWithoutIntent</a>' by accident?  Could it be a copy-and-paste error? a really, really long speeling eror?
        </li>
        <li>
           <em>If we introduce probability into the mix, then definitions that rely on intent, such as "types", will be probability based. We couldn't point to source code and say, "this is a type but this is not", but rather only give it probabilities. (And the estimations still are not based on any real rigor.)</em>
        </li>
        <li>
           It only means that <em>judgements of definitions that rely on intent based on real world observations</em> are probability based <strong>just like EVERY other judgement based on real world observations</strong>.  This is why intent is <strong>just as</strong> objective as everything else.  You say: "We couldn't point to source code and say, 'this is a type but this is not'", but you are forgetting the context: you can't even point at a page of text and say "this is source code and that is not", except by probability.  And, yes, <strong>human estimations</strong> generally lack rigor.  The equation, however, can be applied rigorously.
        </li>
        <li>
           <em>Sure, one can make up an equation, an internally-consistent model, but that does not necessarily mean it is a good model. </em>'There will be more chance of dispute if you base a definition on people's brains instead of the nature of the object itself.<em>' </em>
        </li>
        <li>
           I can only imagine that you slept through your math and statistics courses: that equation (P(A|B)P(B) = P(A n B)) is very well proven, both mathematically and for making useful and verifiable deductions in the real world.  It is probably even used in your spam filter.  Extraordinary claims, such as arguing this being a 'special' case where that equation won't work, require extraordinary evidence - which you ain't got.  And you would do well to follow your own advice and stop creating <em>unnecessary</em> attachments to people's gray matter.  I <em>already</em> look at intent instead as being based in "computation systems", which for <em>both</em> computer programs and brains is "the nature of the object itself".
        </li>
        <li>
           <em>And my spam filter screws up too often. Anyhow, if the givens are bad, the results are bad. GIGO.</em>
        </li>
        <li>
           Nice of you to take a tangent and run with it.
        </li>
      </ul>
      <p>
        When a human is cause for an event, even just tightening a fist, that event is either intentional or unintentional.  However, intent has limited extent.  It may be that you intended to tighten your fist, but that you did not intend to pull the trigger.  It may be that you intended to pull the trigger, but did not intend to fire a bullet.  It may be that you intended to fire a bullet, but did not intend to shoot yourself in the foot.  It may even be that you intended to shoot yourself in the foot.  If human intent has limited extent, is there any reason to believe that actions caused by lesser computational systems may not possess intent with limited extent?  
      </p>
      <p>
        Is the intent of the visual cortex not to classify imagery?  Is the intent of the neuron not to fire in response to the correct balance of chemical stimuli? What happens when these computational processes begin delivering signals unintentionally?
      </p>
      <p>
        If intent has limited extent, should you not judge intent in part by examining its extent?  How would you handle a man who claims: "I intended to load my revolver, point my piece at his head, and pull the trigger... but I did not intend to fire a bullet."  This man is claiming a certain extent to his intent.  How would you judge it, and by what reasoning?
      </p>
      <p>
        What happens if you consistently apply this reasoning to judging other claims of intent?  How do you compare this reasoning to the reasoning by which you detect 'baseball-ness'?  Are you willing to even try this introspection?
      </p>
      <p>
        I feel that much of our conflict about 'intent' comes from your anthropomorphizing of it and me <em>not</em> doing so.  Perhaps if you consider intent without anthropomorphic features, getting rid of that 'unnecessary' dependency on human-ness, you'll have a better understanding of my past discussions.
      </p>
      <hr/>
      <p>
        Classification is applying a distinction for the purpose of the distinction. See also <a href="DifferenceThatMakesNoDifference.html">DifferenceThatMakesNoDifference</a>.
      </p>
      <ul>
        <li>
           <em>Note that "for the purpose of" is closely related to "intent".</em>
        </li>
      </ul>
      <p>
        This may be because 
      </p>
      <ul>
        <li>
           You have discovered that the distinction has temporal or spatial structure.
        </li>
        <li>
           You have discovered that the distinction correlates with other features.
        </li>
        <li>
           You have (dis)advantages from the distinction because your behaviour correlates with it somehow.
        </li>
        <li>
           You can gain advantage from exploiting the distinction by knowingly aligning your behaviour with it somehow.
        </li>
        <li>
           You understand what brings the distinction about.
        </li>
        <li>
           You understand what follows from the distinction for other distinctions.
        </li>
      </ul>
      <p>
        Classification is related to compression in so far as any significant correlation with other distinctions implies a redundancy that can be exploited technically for space.
      </p>
    </div>
  </body>
</html>