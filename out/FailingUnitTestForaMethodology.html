<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Failing Unit Test Fora Methodology
      </h1>
      <p>
        Someone asked me recently what they should do differently in their work. They said there were just two of them in a 4-person company, all in one room. They wrote work lists on the board, picked them off, coded them, showed to bosses, released, etc. I asked, "What seems not to be working for you?" They answered, "Nothing, at the moment. It just seems that we're not doing enough of something, and want to know what that is."  My comment was then, and still is, "If it's working well for you, then you don't need to change anything. (If you are about to hire 4 more people, then we can discuss the impact of that change.)"
      </p>
      <p>
        The similarity with XP was striking... There are so many ways of working that work that I am reluctant to make changes in other people's setups (I make changes in my own all the time, as part of constant experimentation, but that's my personality, not theirs).  Therefore, I like to see what amounts to a "unit test" for their methodology that is failing - - - Name something that is not going well, and look to see how to improve that.  Just as XP says, Don't add code without a failing unit test, I'm starting to request approximately the same thing for methodologies: Show me a <a href="FailingUnitTestForaMethodology.html">FailingUnitTestForaMethodology</a> before requesting a repair or extension.  Methodologies are already too heavy.
      </p>
      <p>
        (This is, btw, different from experimenting, which is a great habit. First, get something working, then when you have a safe fallback position, experiment to see what you can make better. If it's not better, fall back, then find the next experiment.) -- <a href="AlistairCockburn.html">AlistairCockburn</a>
      </p>
      <hr/>
      <p>
        <em>I asked, "What seems not to be working for you?" They answered, "Nothing, at the moment. It just seems that we're not doing enough of something, and want to know what that is."</em>
      </p>
      <p>
        Just a thought : things are working, but they have a problem... if we go by <a href="JerryWeinberg.html">JerryWeinberg</a>'s definition of "problem" as the discrepancy between a desired situation and a perception of the actual situation. Did you ever have the opportunity to find out what they felt they were not doing enough of ?
      </p>
      <p>
        <em>That was the point - they couldn't say what they felt they should do more of, just that guilt was eating at them as though they weren't doing something they should be doing (each had come from a heavyweight process org before to this startup where they consciously did the least possible.) -- Alistair</em>
      </p>
      <p>
        Another : this seems consistent with the XP approach to adopting XP - a) <a href="IdentifyTheWorstProblem.html">IdentifyTheWorstProblem</a>, b) solve that problem the XP way, c) when it's no longer the worst problem, repeat. But possibly the XP approach is too "macho" - who cares if you solve the <em>worst</em> problem, as long as you're solving problems ? (You could just do what <a href="RefactorLowHangingFruit.html">RefactorLowHangingFruit</a> suggests - applied to the process.)
      </p>
      <p>
        <em>If no one can identify any problems (ar at least shortcomings), how can anyone determine whether a change is an improvement?  As noted above, I think we spend too much time making random process changes without any idea of whether anything has improved when we are done.</em>
      </p>
      <hr/>
      <p>
        I assume that they generally do the "most important" things on the lists (if completing the list will take a non-trivial amount of time). If not, then they should work on setting priorities.
      </p>
      <p>
        Looks like their biggest problem is that they need to accept that it's not really necessary to produce lots of heavy weight documentation. My personal (non-XP) style is to focus on a few high-value diagrams or documents; something that will help me maintain a good "overall view" of the system, or the subsystem I'm working with. This may or may not work for them.
      </p>
      <p>
        Ask them about testing. Are they satisfied that their current level of testing is the best it could be? Perhaps they should consider Automated <a href="RegressionTesting.html">RegressionTesting</a>.  (IE: <a href="UnitTest.html">UnitTest</a>s.)
      </p>
      <p>
        -- <a href="JeffGrigg.html">JeffGrigg</a>
      </p>
      <p>
        <em>Right. The point is - they couldn't produce any broken "unit test" for their way of working. Nothing wasn't working. So rather than say, "Well, you really </em> ought <em>to do X, Y or Z, I found myself shrugging my shoulders: Well, then maybe there's nothing to change. Kind of like going to the doctor and saying everything's going well and her/him saying to keep doing whatever you're doing. -- </em><a href="AlistairCockburn.html">AlistairCockburn</a><em></em>
      </p>
      <hr/>
      <p>
        On the experimentation part - they aren't likely to experiment anytime soon, either. They've heard of XP, but since everything is currently going well, they have little incentive to give up their relaxed way of working and taking on an intense, high-octane approach.  So I suspect they'll just keep on doing what they're doing until something wrong shows up. -- Alistair
      </p>
    </div>
  </body>
</html>