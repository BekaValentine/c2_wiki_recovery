<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Notes On The Synthesis Of Form
      </h1>
      <p>
        ""Notes On the Synthesis of Form " by <a href="ChristopherAlexander.html">ChristopherAlexander</a>
      </p>
      <p>
        <img src="http://images.amazon.com/images/P/0674627512.01._PE_PI_SCMZZZZZZZ_.jpg" />
      </p>
      <p>
        [ISBN 0674627512] Early book in the development of patterns (Alexander's dissertation, I believe). Has several compelling lessons for software development, and still (though the details and concrete instantiations of patterns were yet to come) proves invaluable as background and rationale. Reading this made me ask <a href="IsXpAnUnselfconsciousProcess.html">IsXpAnUnselfconsciousProcess</a>.
      </p>
      <p>
        -- <a href="DavidHarvey.html">DavidHarvey</a>
      </p>
      <hr/>
      <p>
        Design is the process of resolving conflicting constraints. The absolute best explanation for why evolutionary is better than heroic design is the book <a href="NotesOnTheSynthesisOfForm.html">NotesOnTheSynthesisOfForm</a> by <a href="ChristopherAlexander.html">ChristopherAlexander</a>.   
      </p>
      <p>
        To cast what that book says into programming terms, consider a tall stack of conflicting requirements. Suppose programmer A took the whole stack, read each one, selected the most-conflicting requirements as a subset, and "thought about" how to design such that the majority of the conflicts went away. Then programmer A commits this design to source code, and then starts adding all the minor requirements to that design - the ones A assumed did not conflict.
      </p>
      <p>
        Now suppose programmer B sorts the requirements into a random order, then pops a requirement off the top of the stack and writes the >simplest< possible design that could satisfy that requirement. Then B pops the next requirement, and makes the simplest possible change to that design to accommodate this requirement. Then B reviews the entire project to ensure that it is as simple as it can be for the current set of committed requirements. B removes any duplication found at this point. Then B repeats the process for each feature in the stack.
      </p>
      <p>
        Years of research and practice have shown many people that programmer A adopted the correct technique. Programmer A deferred risk by taking a "most risk first" approach.
      </p>
      <p>
        But the first technique is, in fact, deceptive. It adds risk. Programmer A took a flying leap all the way from no design to a very big design with lots of parts. B, however, took the design incrementally from nothing up a slope of minimum possible complexity. If programmer A leapt too far, the "extra design" that now burdens the project is hidden and terribly difficult to find and take out.
      </p>
      <p>
        Further, we live in the real world, where "stakeholders" need to know we are not burning up their money. A's technique ran "invisible" for a long time, with no >accurate< way for a stakeholder to unobtrusively inspect any intermediate product.
      </p>
      <p>
        Programmer B can get the current feature set reviewed at any time. Short iterations. If the project is in trouble, the stakeholder knows in less than 2 weeks, not in more than 2 months. Further, because programmer B needs the requirements list prioritized, the stakeholder can assign the risk to business value order.
      </p>
      <p>
        That is the secret behind this evolutionary methodology:
      </p>
      <code>
        <a href="BusinessValueOrientedProgramming.html">BusinessValueOrientedProgramming</a><br/>
      </code>
      <p>
        -- <a href="PhlIp.html">PhlIp</a>
      </p>
      <hr/>
      <p>
        I recently read the book for the first time. It has two parts. The first part is a very good analysis of why design is so hard. The second part is a proposed methodology. Alexander says that it is NOT a methodology, but I know one when I see one. Anyway, I bet that the reason that he does not like the book is because of the second part. But it was probably better than the average software methodology. The first part is definitely worth reading, and the second part is interesting to those who are curious. -- <a href="RalphJohnson.html">RalphJohnson</a>
      </p>
      <hr/>
      <p>
        I'm struck by the resemblance between the rule of <a href="CouplingAndCohesion.html">CouplingAndCohesion</a> and Alexander's formal treatment of the "main" problem of design - hierarchical decomposition of the set M of misfit variables (the "things which might possibly break") into subsets which
      </p>
      <ul>
        <li>
           maximize the connections between their components (high cohesion) but also
        </li>
        <li>
           are minimally connected with each other (low coupling)
        </li>
      </ul>
      <p>
        In other words, the issue of <a href="CouplingAndCohesion.html">CouplingAndCohesion</a> isn't just central to software design - it is central to <em>all</em> design, if Alexander is to be believed.
      </p>
      <hr/>
      <p>
        ConstantineAndYourdon made reference to Alexander in their 1978 Structured Design,
        so it is likely that Constantine's ideas of <a href="CouplingAndCohesion.html">CouplingAndCohesion</a> were influenced by Alexander's work.
      </p>
      <dl>
        <dt> </dt>
        <dd>(p. 104) ... Attempts have been made to quantify the strength of various types of coupling.* ...</dd>
      </dl>
      <dl>
        <dt> </dt>
        <dd>*See, for example, Glenford J. Meyers, Reliable Software through Composite Design (1975) or Christopher Alexander, Notes on the Synthesis of Form (1971 [sic])</dd>
      </dl>
      <hr/>
      <p>
        It has changed my way of thinking about design; and stopped me struggling to find a Perfect Design. Alexander cleanly and convincingly presents that Good Design is the absence of Bad Design, and once you understand that, all those BDUF meetings seem an achingly pointless waste of time. 
      </p>
      <hr/>
      <p>
        Several of the summaries here, notably the first one, are inaccurate. Alexander's work is, roughly speaking, a natural philosophy of design. His treatment of "self-conscious" design: the problem of figuring out what the problem is, has a lot of structural similarities with Dave Parnas' "On the Criteria to Be Used in Decomposing Systems into Modules".
      </p>
      <hr/>
      <p>
        <a href="CategoryBook.html">CategoryBook</a>
      </p>
    </div>
  </body>
</html>