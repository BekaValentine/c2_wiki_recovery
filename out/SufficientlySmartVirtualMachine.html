<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Sufficiently Smart Virtual Machine
      </h1>
      <p>
        Extracted from <a href="SufficientlySmartCompiler.html">SufficientlySmartCompiler</a>:
      </p>
      <p>
        A related issue is the <a href="SufficientlySmartVirtualMachine.html">SufficientlySmartVirtualMachine</a>; early <a href="JavaLanguage.html">JavaLanguage</a> proponents hypothesized about ultra-advanced <a href="VirtualMachine.html">VirtualMachine</a>s which, by employing various adaptive strategies (profile-directed optimizations and such), would achieve runtimes <a href="AsFastAsCee.html">AsFastAsCee</a> or faster.  The logic was that C/C++, being languages which are compiled in advance, could only use static optimizations whereas the Java code could tailor the optimizations being performed to each running instance of the program/class.
      </p>
      <p>
        Certainly Sun and others have developed some rather advanced VM technology; but none of 'em are <a href="AsFastAsCee.html">AsFastAsCee</a> yet.  Given that many optimization strategies employed by C/C++ compilers are <a href="NpHard.html">NpHard</a>, I suspect that a runtime optimizer will not be able to avail itself of many of them.
      </p>
      <p>
        Actually, there's no reason a JIT compiler can't use whatever algorithms it wants. By default the JVM interprets bytecode and only uses compilation where it sees fit (i.e. in the "hot spots", hence HotSpotVM. Compilation can also run in background threads out of band with normal execution, and can even avail itself of idle CPUs when they aren't being consumed by the running program.
      </p>
      <hr/>
      <p>
        <em>Hrm. Was the Java-specific CPU ever released? I remember hearing about it years ago - that it would "run Java not at </em><a href="VirtualMachine.html">VirtualMachine</a> speed, but at hardware speed". . . never saw anything come of it though.<em></em>
      </p>
      <p>
        There are quite a few CPUs, especially the various ARM derivatives used in PDAs, which have "Java accelerators" in them.  However, <a href="JavaByteCode.html">JavaByteCode</a> is not a particularly good instruction set for efficient silicon implementation. Java's bytecode structure is optimized for loading into a compiler, not a processor.
      </p>
      <ul>
        <li>
           An interesting approach is the one of Azul - a general purpose RISC with a few primitives for excellent JVM support. Cliff Click is one of the best JVM engineers around, and he works for Azul. Here are a couple of links about this architecture:
          <ul>
            <li>
               <a href="http://www.azulsystems.com/blog/cliff-click/2008-11-18-brief-conversation-david-moon">http://www.azulsystems.com/blog/cliff-click/2008-11-18-brief-conversation-david-moon</a> (description of his architecture, comparing it with a Lisp Machine)
            </li>
            <li>
               <a href="http://www.azulsystems.com/blog/cliff-click/2010-04-21-un-bear-able,">http://www.azulsystems.com/blog/cliff-click/2010-04-21-un-bear-able,</a> a discussion about why running Java bytecode in hardware is a bad idea, compared to JITting
            </li>
          </ul>
        </li>
      </ul>
      <p>
        Actually the traditional x86 machine code is also 'not a particularly good instruction set for efficient silicon implementation'. That is the reason why our beloved Intel processors internally decode (read: compile) it into a (very simplified and optimized) data flow machine instruction set and execute that. I don't see any reason why this coudn't be done with bytecode too. After all the <a href="http://en.wikipedia.org/wiki/Efficeon">http://en.wikipedia.org/wiki/Efficeon</a> (aka Crusoe) also does this. So it is quite possible. It's just not done for historical reasons.
      </p>
      <hr/>
      <p>
        As far as I have seen in a former project (in a team that built a 32bit VM) the "Java accelerators" don't usually help very much.
        The ones available at the time only helped to speed up the inner interpreter loop - i.e. looking up an address for a bytecode.
        Some of them provide some stack manipulation shortcuts, possibly even simple mathematical computations using a stack, but as soon as you have to touch a 'real' object or perform a more complex function, you are on your own. 
        Which in turn means that
      </p>
      <ul>
        <li>
           either you have to construct a "Java accelerator" that imposes a specific object model (and associated data structures in memory) 
        </li>
        <li>
           or you don't gain much performance from using these accelerators -- looking up an address in a function table does not take very long.
        </li>
      </ul>
      <p>
        So - at least for common general-purpose processors - I doubt that a performance-boosting "Java accelerator" in hardware is possible.
        Of course this does not rule out specific architectures that are tailor-made for efficient Java execution.
      </p>
      <hr/>
      <p>
        But sometimes the time of a <a href="SufficientlySmartVirtualMachine.html">SufficientlySmartVirtualMachine</a> comes. An example is <a href="GarbageCollection.html">GarbageCollection</a>. There are now a few studies that show that programs with automatic <a href="GarbageCollection.html">GarbageCollection</a> are faster as their more complex counterparts with manual collection.
      </p>
      <p>
        <em>Which is related to the virtual machine because ...?</em>
        Because a garbage collector is a part of the hardware abstraction that makes up a VM.
      </p>
      <ul>
        <li>
           I'm surprised this hasn't generated a <a href="DemandForEvidence.html">DemandForEvidence</a>.
          <ul>
            <li>
               Yes. I remember <a href="HansBoehm.html">HansBoehm</a> saying that sometimes GC beats malloc because keeping the free-lists up-to-date may be expensive. Trying to find a reference (<a href="http://www.hpl.hp.com/personal/Hans_Boehm/gc/)">http://www.hpl.hp.com/personal/Hans_Boehm/gc/)</a> 
            </li>
            <li>
               Here's one: <a href="http://citeseer.ist.psu.edu/appel87garbage.html">http://citeseer.ist.psu.edu/appel87garbage.html</a>
              <ul>
                <li>
                   Everyone should notice that fact from your link : <em>With enough memory, explicit deallocation of garbage may be more expensive than collecting live data</em>
                </li>
                <li>
                   With enough memory. Truth is, using <a href="GarbageCollection.html">GarbageCollection</a> can make your code in practice very memory hungry. I've yet to see a <a href="GarbageCollector.html">GarbageCollector</a> that can be as fast as manual memory management AND as memory tight. It can't be both. 
                </li>
                <li>
                   {Depending <strong>entirely</strong> upon <a href="GarbageCollection.html">GarbageCollection</a> seems wasteful, especially given the facilities already available for inference over memory usage in the source-code.  I'm looking forward to widespread implementation of memory-use inference performed statically upon the source-code... possibly with limited automatic runtime support (such as tracking whether a reference leaves a scope or is referenced at least twice outside its own scope - i.e. the 'smart virtual machine').  In theory, a compile-time inference for explicit deallocation can be nearly as memory-efficient as human programmers (since human programmers need to statically make the exact same deallocation decisions, with perhaps just a slightly more information than is available to the compiler) and is far less prone to error (where something is deleted prematurely).  Automatically injecting additional runtime support for tracking which objects can be deleted immediately goes even beyond what most humans are willing to provide explicitly.  <strong>(Inferred implicit deallocations calculated at compile-time + garbage collection + compiler support for efficient runtime collection)</strong> seems to me to be the best of both worlds... much like <a href="SoftTyping.html">SoftTyping</a> allows the best of static + dynamic typing.  Perhaps we should dub it SoftGarbageCollection?}
                </li>
              </ul>
            </li>
          </ul>
        </li>
      </ul>
      <ul>
        <li>
           Here is a very relevant reference: <a href="http://lambda-the-ultimate.org/node/2552">http://lambda-the-ultimate.org/node/2552</a> (Quantifying the Performance of Garbage Collection vs. Explicit Memory Management) this quantifies the "enough memory" part above. 
        </li>
      </ul>
      <hr/>
      <p>
        See <a href="VirtualMachine.html">VirtualMachine</a>
      </p>
    </div>
  </body>
</html>