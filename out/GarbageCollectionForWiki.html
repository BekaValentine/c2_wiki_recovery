<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Garbage Collection For Wiki
      </h1>
      <p>
        Once a month, run a script which identifies Wiki pages which have not been requested in at least 90 days. The script edits Wiki page MarkedForDeletionFromWiki, appending the date and time the script was run and the names of the pages found.
      </p>
      <p>
        Any page which is edited is automatically removed from the list of pages marked for deletion. Also, any page which is requested from a page <em>other</em> than the marked for deletion list is removed from the list.
      </p>
      <p>
        The next time the script is run, it will check for any sections of its marked list which are more than 90 days old. Pages in that section will be deleted.
      </p>
      <p>
        -- <a href="ChrisBaugh.html">ChrisBaugh</a>
      </p>
      <hr/>
      <p>
        Excellent description of <a href="GarbageCollectionForWiki.html">GarbageCollectionForWiki</a>. Please don't implement it here. A delicate balance exists between deleting pages and creating them. When deletions were impossible, it was slightly out of balance. Now, deletions only happen when someone chooses to do so. This seems to me to be a good thing. If the amount of garbage is large enough to bother you, why not clean up a few pages? If not... just let it be. -- <a href="MichaelChermside.html">MichaelChermside</a>
      </p>
      <hr/>
      <p>
        Michael, no worries about my implementing it here because I don't know how to change the Wiki code on c2, and if I did I wouldn't without checking with Ward first.
      </p>
      <p>
        I thought the idea might spark some interesting discussion, and could be useful to a <a href="WikiClones.html">WikiClones</a> implementor.
      </p>
      <p>
        Thanks. -- <a href="ChrisBaugh.html">ChrisBaugh</a>
      </p>
      <hr/>
      <p>
        Note that this isn't <a href="GarbageCollection.html">GarbageCollection</a> in the normal sense of the term. <a href="GarbageCollection.html">GarbageCollection</a> usually means deleting things which can't be reached from the active things. It would be possible to use this on wiki, too; we could define the RootSet (as, say, <a href="RecentChanges.html">RecentChanges</a> and <a href="WelcomeVisitors.html">WelcomeVisitors</a>, and perhaps some other key pages; or perhaps using an activity metric as described above) and then identify and delete any pages which were not reachable by browsing from there. The hitch with this is that the 'garbage' pages might be reachable via the <a href="ReverseIndex.html">ReverseIndex</a>, and so their loss might be noticed as decreases in the populations of categories.
      </p>
      <p>
        The original proposal is more like TenuringForWiki. One of its consequences would be that links on actively-used pages were deleted.
      </p>
      <p>
        Perhaps the strongest objection to automated reaper is that it can't really judge the value of a page, only how often it is read, how easy it is to find, etc. There may be some really good pages out there which aren't being read; it would be sad if they were to be deleted. On the other hand, there are plenty of junk pages which are fairly active!
      </p>
      <p>
        <em>I think that the proposal has taken care of your objection to an automated reaper. After it runs, the nodes in question will show up in </em><a href="RecentChanges.html">RecentChanges</a>, and the list exists on a wiki page that people can review for 30 days. If nobody has claimed it after 30 days, presumably it has passed human muster as well. -- <a href="BenTilly.html">BenTilly</a><em></em>
      </p>
      <hr/>
      <p>
        <a href="GarbageCollection.html">GarbageCollection</a>, as the term is used here, implicitly assumes that objects are known only by their references from other objects (their <a href="ObjectIdentity.html">ObjectIdentity</a>). Reliable <a href="GarbageCollection.html">GarbageCollection</a> is impossible in any system that allows object references to be computed, because the collector/scavenger has no practical way of determining which computations might result in a reference to any given object. The collector therefore cannot reliably determine when an arbitrary object has become garbage. An alternative is to shuffle objects that have not been referenced in a very long time to tertiary storage - they still exist, but it might take a very long time to access them (because an operator might have to retrieve a piece of plastic from shelf someplace).
      </p>
      <p>
        In a system like Wiki, this potential problem is exacerbated because "object references" - page names - are, by design, easy to calculate. In fact, the opportunity for "accidental" links - such as to <a href="GarbageCollection.html">GarbageCollection</a> in this contribution - is an important aspect of <a href="WikiNature.html">WikiNature</a>.
      </p>
      <p>
        Pages with few or no links from other pages are not, therefore, "worse" than pages with many links. I suggest that we lead ourselves further and further down a mistaken garden path (or rathole, if you prefer) by making an ill-advised analogy between page references on a Wiki and object references in a software system.
      </p>
      <hr/>
      <p>
        <a href="CategoryWikiMaintenance.html">CategoryWikiMaintenance</a> <a href="CategoryGarbageCollection.html">CategoryGarbageCollection</a>
      </p>
    </div>
  </body>
</html>