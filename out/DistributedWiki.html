<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Distributed Wiki
      </h1>
      <p>
        Issues:
      </p>
      <ul>
        <li>
           Access - same content (wikis) available from different locations (servers).
        </li>
        <li>
           Connections - see <a href="UnifiedRecentChanges.html">UnifiedRecentChanges</a> (previously <a href="InterWiki.html">InterWiki</a>) ; linking related content (wikis)
        </li>
        <li>
           Directing - <em>connections</em> to the right <em>access</em> points - a little like a geographic traffic load balancer (there's that open one related to iptables, by VA Linux/sourceforge if I remember correctly).
        </li>
      </ul>
      <p>
        //mawi (more below)
      </p>
      <p>
        Random thoughts:
      </p>
      <p>
        Can the wiki be generalized to support separate installations in different locations, sharing the same repository, with possible redundancy and some kind of replication?
      </p>
      <p>
        Can the wiki model be used for producing text in Smalltalk instead of English? :)
      </p>
      <p>
        <a href="PavelPerikov.html">PavelPerikov</a>.
      </p>
      <p>
        Surely this should be possible. There are some newer attempts at distributed wikis, but they seem to be complete mirrors, which is only one model. A distributed wiki should run across multiple servers, and should be capable of supporting a more robust database as a result. It would be more robust against network failures, and also against server failures. There would possibly be some problems with updates, and some see that as being very significant, though in practice it might only present a small difficulty, as updates could be handled by special processes. One distributed model is that each server will provide a mirror of the wiki, but others might allow for different wiki entries to be handled by different servers.
      </p>
      <p>
        Searching for keywords and locating articles may be a significant overhead in a large wiki, and improvements could be obtained by distributing articles over a number of servers.
      </p>
      <p>
        <em>In the spirit of </em><a href="OnceAndOnlyOnce.html">OnceAndOnlyOnce</a>, the search for keywords can be done by the server and the list of articles also maintained in a simple flat text TitlesIndex and WordsIndex, accessible as a <a href="HyperLink.html">HyperLink</a> by any browser. The list could be run once a day, week or month depending on Administration's desires. Requiring the server to perform a real-time search could be provided on a FeeBasis, if the interval of Daily, Weekly or Monthly were not sufficient.<em> -- </em><a href="DonaldNoyes.html">DonaldNoyes</a> 
      </p>
      <p>
        Delays due to geographical location could be minimised by using multiple distributed servers, and this would have a beneficial effect on the total network traffic.
      </p>
      <p>
        It has been argued that this type of software will be complex, and difficult to manage, but I suspect that once someone has actually implemented such a system, that these objections will diminish rapidly.
      </p>
      <p>
        Another fairly obvious advantage is the possibility that such a wiki could be very large, and may not require a single individual or organisation to pay for the hardware/software infrastructure. Some would worry about the lack of centralised control, however.
      </p>
      <p>
        <a href="DavidMartland.html">DavidMartland</a>
      </p>
      <hr/>
      <p>
        For connecting wikis there is <a href="InterWiki.html">InterWiki</a>. Is there any replicating Wiki software?
      </p>
      <p>
        (Synonyms: Just use "wikis" to mean one server/node/etc. KISS.)
      </p>
      <p>
        //mawi
      </p>
      <hr/>
      <p>
        There's some discussion of a distributed wiki at
        <a href="http://wikifeatures.wiki.taoriver.net/moin.cgi/FailSafeWiki">http://wikifeatures.wiki.taoriver.net/moin.cgi/FailSafeWiki</a>
        . But as far as I know,
        <a href="WikiPedia.html">WikiPedia</a> is the only wiki that has <em>any</em> distributed features.
      </p>
      <p>
        More recently,
        <a href="OddMuse.html">OddMuse</a> has added some page-synchronization abilities:
        <a href="http://oddmuse.org/cgi-bin/wiki/Page_Syncronization">http://oddmuse.org/cgi-bin/wiki/Page_Syncronization</a>
        .
      </p>
      <hr/>
      <p>
        <strong>InfoTopics in a Distributable format</strong>
      </p>
      <p>
        Perhaps one might build topical or categorical wiki spaces which would be "distributable" and to keep it simple the distributable module would be the named using the topic name and the topic date of distribution:
      </p>
      <ul>
        <li>
           ExtremeProgramming_20060527
        </li>
      </ul>
      <p>
        A third identifying classification might be added to identify a single home or place of origin, as in:
      </p>
      <ul>
        <li>
           ExtremeProgramming_WardsWiki_20050627.
        </li>
      </ul>
      <p>
        The first form would be the generalized topic which might be from many sources, while the second form would be the specialized, single source topic space.
      </p>
      <p>
        The distribution, to be efficient would be via a compressed file containing all of the pages in the topic, along with supporting system files such as <a href="PageList.html">PageList</a>, WordList, ChangesIndex (Similar to <a href="RecentChanges.html">RecentChanges</a>, but consisting of the page name, the last date changed, and who modified it on that date), as well as other LinkPages which tie the topic to a centralized server which contains the list and location of all distributable modules in "TheCollection". The several collections would be assembled and made viewable using a local <a href="PersonalWiki.html">PersonalWiki</a> Engine. -- <a href="DonaldNoyes.html">DonaldNoyes</a>
      </p>
      <hr/>
      <p>
        <a href="DavidCary.html">DavidCary</a>
        wants to build a "fault-tolerant" wiki.
        In his eyes, the "lack of centralized control" is a *good* thing.
        I think the only way to make a wiki fault-tolerant to distribute at least 2 copies of each wiki page
        over at least 2 separate wiki servers.
        So no matter what happens to one server, people can still read and write those wiki pages ( at the other server(s) ).
        <a href="http://communitywiki.org/odd/SoftwareBazaar/distributed_wiki">http://communitywiki.org/odd/SoftwareBazaar/distributed_wiki</a>
      </p>
      <hr/>
      <p>
        I have one question: why?
      </p>
      <p>
        What would distributing get you that centralization couldn't? Centralization is generally easier to create and manage software and tools for. Wiki text is not at all bandwidth-intensive compared to say a painting repository.
      </p>
      <p>
        <em>There are several reasons I can think of: demotion of central authority (reduces risk of tyranny by elite), reliability through redundancy, etc. Bandwidth might matter a lot more than you might think depending on the class of Wiki involved (some have quite a few images... but with enough bandwidth, there could be videos and interactive objects with a </em><a href="SmartWiki.html">SmartWiki</a>). That said, I'm not particularly interested in this idea unless it gets two major enhancements: (1) offline operation - as per distributed version control mechanisms, such that I can read and write pages even on my laptop when it's offline then automatically push my changes (with some support for handling conflicts) to the more 'official' server(s), (2) <a href="RuntimeUpgradeableCore.html">RuntimeUpgradeableCore</a>, such that the lack of centralization doesn't become a trap that prevents global updates to the service.<em></em>
      </p>
      <p>
        A more concrete version of the "offline operation" reason: Imagine a wiki located inside a company's network and not generally accessible from outside. When working from a branch office, the central wiki location might only be accessible when a VPN connection is up and running. When the central wiki location is unavailable, a branch office might still want to be able to access (and modify) the pages on the wiki.
      </p>
      <hr/>
      <p>
        This is a solved problem. Use a VCS-backed wiki (such as ikiwiki, <a href="http://ikiwiki.info/)">http://ikiwiki.info/)</a> with a DVCS (such as git). ikiwiki explicitly discusses this sort of thing in a few places: <a href="http://ikiwiki.info/tips/distributed_wikis/">http://ikiwiki.info/tips/distributed_wikis/</a> <a href="http://ikiwiki.info/tips/laptop_wiki_with_git/">http://ikiwiki.info/tips/laptop_wiki_with_git/</a> <a href="http://ikiwiki.info/tips/laptop_wiki_with_git_extended/">http://ikiwiki.info/tips/laptop_wiki_with_git_extended/</a>
      </p>
      <p>
        Or use a DVCS with a built-in wiki (such as Fossil, <a href="http://www.fossil-scm.org/index.html/doc/tip/www/wikitheory.wiki).">http://www.fossil-scm.org/index.html/doc/tip/www/wikitheory.wiki).</a>
      </p>
      <hr/>
      <p>
        See
        <a href="WikiWebTransferProtocol.html">WikiWebTransferProtocol</a>
        <a href="WorldWideWikiWeb.html">WorldWideWikiWeb</a>
        <a href="MultiServerWiki.html">MultiServerWiki</a>
        <a href="PullPushWiki.html">PullPushWiki</a>
      </p>
      <p>
        needs checksum check with other mirrors, server rating, merging, can be done.
      </p>
      <hr/>
      <p>
        <a href="CategoryWiki.html">CategoryWiki</a>
      </p>
    </div>
  </body>
</html>