<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Undo Edits By Ip Bot
      </h1>
      <p>
        It's not that difficult to create a bot / script that undoes edits made from a list of IP's / IP ranges.
      </p>
      <p>
        What's needed, basically, is to:
      </p>
      <ul>
        <li>
           check <a href="RecentChanges.html">RecentChanges</a> for edits by the targeted IP's
        </li>
        <li>
           use <a href="EditCopy.html">EditCopy</a> to quickly revert the edit
        </li>
        <li>
           'touch' the page using a different IP to purge the <a href="EditCopy.html">EditCopy</a>
        </li>
      </ul>
      <p>
        In <a href="PseudoCode.html">PseudoCode</a> rather than real code (lest any one tries and uses such a contraption against this site):
      </p>
      <code>
        Every couple of minutes:<br/>
        {<br/>
        Fetch <a href="RecentChanges.html">RecentChanges</a> from the server<br/>
        For each IP on list:<br/>
        { <br/>
        filter for IP address<br/>
        For each mentioned page not removed:<br/>
        {<br/>
        Send http request for <a href="EditCopy.html">EditCopy</a><br/>
        Send http request to click Save button<br/>
        From different IP address:<br/>
        {<br/>
        Send http request for Edit<br/>
        Add or delete one whitespace character (null changes are rejected)<br/>
        Send http request to click Save button<br/>
        }<br/>
        }<br/>
        }<br/>
        }<br/>
      </code>
      <p>
        The above is easily done in <a href="PerlLanguage.html">PerlLanguage</a>, <a href="JavaScript.html">JavaScript</a> or similar. A bit harder, but certainly doable, in C++ or similar.
      </p>
      <p>
        More complex versions could possibly:
      </p>
      <ul>
        <li>
           Detect earlier changes by an IP and revert to a version <em>before</em> that
        </li>
        <li>
           Instead of reverting to before that version, merge those changes out, leaving other people's contributions intact
        </li>
        <li>
           Detect spam cleanups and leave those alone, even if done by the targeted IP
        </li>
      </ul>
      <p>
        Detecting <em>any</em> edit by user <a href="JohnDoe.html">JohnDoe</a> coming from any random IP and deleting those while leaving a consistent version of the page is very possibly <a href="AiComplete.html">AiComplete</a>, and could only be done by a human masquerading as a bot.
      </p>
      <p>
        But what even the simplest version of such a bot would do, effectively, would be denying edits by a particular user or group of users, since most of us have either a fixed IP or an IP within a certain range.
      </p>
      <p>
        <a href="MichaelSparks.html">MichaelSparks</a> seems to have used a similar approach in fighting spam (since a lot of spam seems to come from a limited range of IP's).
      </p>
      <hr/>
      <p>
        See also: <a href="WikiBot.html">WikiBot</a>, <a href="WikiVandals.html">WikiVandals</a>, <a href="DefensiveScriptIdea.html">DefensiveScriptIdea</a>
      </p>
    </div>
  </body>
</html>