<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Cost Reliability Tradeoff
      </h1>
      <p>
        For the vast majority of software, <em>there is no tradeoff between cost and reliability</em>.
      </p>
      <p>
        It is well-known (perhaps even true) that finding and fixing defects later in the software development process is orders of magnitude more costly than catching them earlier.
      </p>
      <p>
        Therefore, do the math, an approach that finds the bulk of the defects on the first day will cost less for any given level of reliability than an approach that finds them on any other day.
      </p>
      <p>
        One such approach is to have <a href="UnitTest.html">UnitTest</a>s for all code, and to run the <a href="UnitTest.html">UnitTest</a>s incredibly frequently, such as every ten minutes.  When this is done, the bulk of defects inserted are caught within ten minutes. Only a few statements have been added, so that only a few statements need to be considered as the cause of the defect.  The developer's mind is fresh on the subject, and she can easily find the defect.
      </p>
      <p>
        It takes overall less time and therefore less money, to bring the product to any given level of reliability.
      </p>
      <p>
        There <strong>is</strong> no tradeoff between cost and reliability.  Increased reliability done wisely decreases cost. 
      </p>
      <p>
        -- <a href="RonJeffries.html">RonJeffries</a>
      </p>
      <hr/>
      <p>
        How often should the acceptance tests be run?  In a way aren't they more important in determining whether an application has been broken over all? -- <a href="MichaelFeathers.html">MichaelFeathers</a> (will move this to an appropriate acceptance test page once I find it)
      </p>
      <hr/>
      <p>
        We find that running the entire suite of <a href="UnitTest.html">UnitTest</a>s and requiring them all to run at 100% gets most of the job done.  We also run a subset of the <a href="AcceptanceTest.html">AcceptanceTest</a>s before release to the developer config, and all the <a href="AcceptanceTest.html">AcceptanceTest</a>s before release to the customer.  The idea above isn't pure XP, but an example that goes to the effect of the earliest testing possible.  We run the <a href="UnitTest.html">UnitTest</a>s for what we're working on every few minutes, the entire suite every now and again, and all of them before release to the config (ideally at least daily for each developer). -- <a href="RonJeffries.html">RonJeffries</a>
      </p>
      <hr/>
      <p>
        I believe the above may contradict <a href="TestFirstDesign.html">TestFirstDesign</a>.  If one follows test first design, he will avoid putting in the defects in the first place.  This is the true way to avoid the cost reliability trade-off, not by merely using frequent regression <a href="UnitTest.html">UnitTest</a>s as a means of quickly catching problems after they 
        occur.
      </p>
      <hr/>
      <p>
        <a href="CategoryTesting.html">CategoryTesting</a>
      </p>
    </div>
  </body>
</html>