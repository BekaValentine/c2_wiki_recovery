<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Star Schema
      </h1>
      <p>
        <em>** Stars: A Pattern Language for Query Optimized Schema</em>
        by <a href="StevePeterson.html">StevePeterson</a>.
      </p>
      <p>
        Available in <a href="PatternLanguagesOfProgramDesign.html">PatternLanguagesOfProgramDesign</a> I and online.
        <a href="http://c2.com/ppr/stars.html">http://c2.com/ppr/stars.html</a>
      </p>
      <p>
        A Star schema is optimized for data analysis as is typically required in a Decision Support System. The star schema pattern language presented here is an attempt to provide a method for business analysts to develop a schema which is easy to query. 
      </p>
      <hr/>
      <p>
        I'm going to go out on a limb and suggest that a well-normalized database (with respect to the concepts of the domain) is easy to query, and vice versa. No, I don't have the formal education of some of the gurus around here, but I've read enough to suggest that the two go hand in hand. On top of this, I'm going to suggest that many people are inflexible about what they consider the true domain objects in a system. Therefore, arguments about denormalization frequently boil down to:
      </p>
      <dl>
        <dt> </dt>
        <dd>1. A question of the classification of domain objects.</dd>
      </dl>
      <dl>
        <dt> </dt>
        <dd>2. A question of the correct use of keys.</dd>
      </dl>
      <p>
        or at least, they should.
      </p>
      <hr/>
      <p>
        Umm - no.  A well-normalized database is good for having an accurate representation of the details of your data.  In reporting, you are frequently needed to group data across relatively coarse ranges of multiple dimensions, including time, and do output of the same kinds of aggregations over and over.  OLTP systems (used for data entry and editing) are usually normalized, and OLAP systems (used for reporting and decision support lookups) are usually groups of star schema data marts built from queries of the normalized data.
      </p>
      <p>
        In a star schema, you are usually looking at data that has been aggregated from the original, normalized data into a number of cubelets by chopping up dimension space into the smallest useful reporting units, and ignoring all dimensions (attributes) that will not be used for report queries.  The data is usually further denormalized by having the same data represented in multiple star schemas at different levels of granularity (e.g. by day, by month, by quarter, and by year), for faster speed when doing coarser-grained reports and top levels of drill-down.  It is OK to have data duplicated in such a schema because it is read-only and is populated based on a snapshot of normalized data at a point in time.
      </p>
      <p>
        Note that this also separates the concerns of aggregating and report querying which is good.  When querying the data, you just need to know what kinds of aggregate data you have, not how the aggregates are made, and when aggregating the data, you don't need to consider what combinations of dimensions are being queried, sorted, etc., just what aggregates are going into the OLAP system to support the queries one might want to do subsequently.
      </p>
      <p>
        -- <a href="SteveJorgensen.html">SteveJorgensen</a>
      </p>
      <p>
        <em>Why is this [star-schema] better than simply defining some views on the normalised schema for the common queries. That way, you maintain the power and flexibility of the relational model and the normalised schema, and also get the "ease-of-use" and consistent application of the aggregating rules. (I'm assuming the "normalised schema" includes the history needed, so it may still be different from an existing OLTP schema.) Note that I'm talking about the logical view here, not physical aspects such as performance -- they can be taken care of with materialised views, clustering and other tuning of the physical DB.</em>
      </p>
      <hr/>
      <p>
        A star schema has a fact table (<a href="ThirdNormalForm.html">ThirdNormalForm</a>) surrounded by dimension tables (<a href="SecondNormalForm.html">SecondNormalForm</a>). A star schema uses surrogate keys and several techniques for representing data+keys that change over time. (Many source system keys do not change as the data changes, therefore a surrogate key is needed to distinguish the current source data from the old source data.
      </p>
      <p>
        A formal education is not required, but <a href="RalphKimball.html">RalphKimball</a> and associates have written up a lot of experience that's easy to read.
      </p>
      <hr/>
      <p>
        Kimball's "Data Warehouse Toolkit" is worth a read.  The accompanying CD contains his query tool, StarTracker, which embodies some interesting concepts.  Alas, StarTracker is a 16-bit Visual Basic application...  For a time I worked with a firm that did a 32-bit version;  I wrote a companion metadata descriptor tool called <a href="StarManager.html">StarManager</a> to replace Kimball's hypertrophied .ini file.  The ultimate plan was to support temporality in a way most data repositories don't; the operational data store is frequently amnesiac -- Kimball used to call them "twinkling databases."  Sad to say, the company "went 404" some few years back, and we never had the chance to address the main challenge.  
      </p>
    </div>
  </body>
</html>