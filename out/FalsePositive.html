<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        False Positive
      </h1>
      <p>
        A test result that returns True when in fact the result should be False.
      </p>
      <p>
        A <a href="FalsePositive.html">FalsePositive</a> test result indicates a fault with the test procedure rather than a fault with the product under test.
      </p>
      <p>
        <em>Or it may occur when a test is only capable of statistical prediction.  Most medical diagnostics are of this sort; it doesn't mean the test is broken; is just means that we aren't capable of complete accuracy using the current state of the art.</em>
      </p>
      <p>
        Tests are designed to identify product faults.  Sometimes a test will indicate a fault even though there is nothing wrong with the product.  This is called a <a href="FalsePositive.html">FalsePositive</a>.  Usually this means something is wrong with the test procedure or something was wrong with how the test was carried out.  To eliminate <a href="FalsePositive.html">FalsePositive</a> results, improve your test procedure.
      </p>
      <p>
        It is wrong to assume that <a href="FalsePositive.html">FalsePositive</a> results are "OK" because being over cautious is not a bad thing.  This is a naive notion.  Your test results become so cluttered with <a href="FalsePositive.html">FalsePositive</a> results, you can't find the real problems.  It's like TheBoyWhoCriedWolf.
      </p>
      <ul>
        <li>
           False positive results can also be damaging in their own right, and not just as clutter. For example, a false positive on a medical test could cause someone to undergo a dangerous operation.
        </li>
      </ul>
      <p>
        <em>I am reminded of a Navy contract to detect faults in torpedoes.  The requirements stated that any number of false positives were acceptable as long as 100% of faults were always detected.  The solution was to build a simple device that just always reported a fault.  "Here, hook it up to this brick and it will find the faults."  "But that's not even a torpedo."  "It doesn't matter!"</em> -- <a href="WilliamFrantz.html">WilliamFrantz</a>
      </p>
      <hr/>
      <p>
        Remember statistics class? Did you ever confuse Type I and Type II errors? Still can't remember which is which?
      </p>
      <hr/>
      <p>
        Often, it is a strategy to allow for false positives. If a test cannot be perfect, but its error is skewable, shifting it so that false positives are more frequent lowers the risk of FalseNegatives. In turn, this insures that any negative result is trustworthy, at the expense of making the positive result non-trustworthy.
      </p>
      <p>
        Example: if you have a simple, inaccurate test that searches for a deadly disease, skewing the test towards false positives is a good thing. Anyone who gets a negative on the test is guaranteed clean. Anyone who gets a positive on the test is likely to be infected, and you can spend the extra money doing the full spectrum of expensive tests on them. If the disease is in 3% of the population, and there is a 7% chance of false positives, this means that you only need to run the expensive suite over 10% of the population, saving 90% of the test costs.
      </p>
      <p>
        -- <a href="RobMandeville.html">RobMandeville</a>
      </p>
    </div>
  </body>
</html>