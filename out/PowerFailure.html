<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Power Failure
      </h1>
      <p>
        A power failure is an un-announced cessation of power.  Typically we care when this happens to a computer of some kind.
      </p>
      <p>
        This means any data not saved to media (flash ram, disk, battery backed ram) is lost.  Any unsaved indexing information to saved information is also lost, which can make finding the saved information difficult.
      </p>
      <p>
        It has nasty effects in buffered-IO operating systems, like every flavor of Unix.  Unix wants to read and write stuff to and from the hard disk when IT has time to do so, and supplies to the user information from its buffers.  This means that when the power goes down, there can be a lot of stuff in buffers that never gets written to the disk.
      </p>
      <p>
        Defensive programming suggests that you flush your output frequently, and that you don't hold files open for long periods of time.  This can have a performance penalty (your program may run a little slower when flushing) but lowers the risk.
      </p>
      <p>
        Power failures can often damage computers, as can a PowerSurge, or DirtyPower with spikes and dropouts. A decent PowerConditioner with BatteryBackup can minimize both physical damage and the data loss described above. Power problems are the number one cause of disk drive failures, too. I can't believe that businesses don't get them routinely, as many areas, such as CaliforniaState, routinely experience power failures.
      </p>
      <hr/>
      <p>
        It was recently suggested to me that our fault tolerant product could be made simpler, and brought to market sooner, if we defined away power failures. Ie, your environment must provide 1) so many inches of rack space, 2) air conditioning capable of cooling the equipment, and 3) power which simply never goes away.  This struck me as taking a lot of guts.
      </p>
      <hr/>
      <code>
        matthew@gellum:~$ uptime<br/>
      </code>
      <ol>
        <li>
          :37PM  up 8 days,  5:28, 2 users, load averages: 0.47, 0.43, 0.34
        </li>
      </ol>
      <code>
        matthew@gellum:~$ cat /etc/motd | grep 2000<br/>
        |   At the mercy of Northern Electric since 2000 A.D.    |<br/>
      </code>
      <p>
        -- <a href="MatthewTheobalds.html">MatthewTheobalds</a>
      </p>
      <p>
        See <a href="AbuseCase.html">AbuseCase</a>, <a href="TechnicalFailure.html">TechnicalFailure</a> (last prior edit May 29, 2001)
      </p>
    </div>
  </body>
</html>