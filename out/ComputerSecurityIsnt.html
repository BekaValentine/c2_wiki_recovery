<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Computer Security Isnt
      </h1>
      <p>
        <strong></strong><a href="ComputerSecurityIsnt.html">ComputerSecurityIsnt</a> concerned with matters related to security problems of existing systems.<strong></strong>
      </p>
      <hr/>
      <p>
        <em>From </em><a href="IwannaLearnComputerSecurity.html">IwannaLearnComputerSecurity</a><em></em>
      </p>
      <p>
        Analyzing how far an existing system deviates from a set of implicit and unstated security requirements is not part of security. Plugging holes in the crippled software systems which hacks have cobbled together has absolutely nothing to do with the subject of security. The better term for the practice of shysters peddling antivirus software and Unix patches is <em>insecurity</em>. 
      </p>
      <p>
        Security addresses certain human elements, especially those tied to expressiveness of security policies. Security does not deal with all forms of <a href="SocialEngineering.html">SocialEngineering</a>. If a legitimate authority knowingly, explicitly, consciously and deliberately grants access to a specific resource to a malicious party, it is illegitimate to expect any system to fail to function for that malicious party. Certain things are simply beyond security design and any attempt to deal with them at that level will be a sham that needlessly hurts legitimate users. 
      </p>
      <p>
        <em>I think activities related to the implementation of countermeasures to "computer security lapses" fall within the scope of computer security. If you disagree, can you do more clarifications for me pls -- dl</em> 
      </p>
      <p>
        Let's take a specific example. Activities related to the implementation of countermeasures to Unix security lapses do not fall within the scope of computer security. They fall within the purview of computer insecurity. Unix was never designed as a secure system. It still doesn't have a set of self-consistent security requirements, let alone a secure design. Nothing to do with patching it has anything to do with security. 
      </p>
      <p>
        The problem here is that we have two radically different concepts. Security engineering and counter-cracking. The latter is in relation to the former as hacking is to systems design. -- rk 
      </p>
      <p>
        <em>Richard I have a problem there. Most people who work in the computers field are in the business of delivery of end user solutions. Few of us work in research or academic areas. For most of us the bigger problem is responding (sometimes even proactive) to security challenges that may pop up on "legacy environments". If we go Google searching for "Computer security" we expect (and do find) lots of information related to prevention and cures. Do you agree the majority of us have an understanding of computer security different from yours? If so, I hope you are not asking the majority to change the terminology? Do you have a widely used phrase for activities related to the prevention and rectification of security situations related to topics such as </em><a href="WebServices.html">WebServices</a>? -- dl <em></em>
      </p>
      <p>
        A good term to use for any reactionary legacy work may be insecurity optimization. I don't think it's tolerable to accept, use and encourage the use of Orwellian language. Defining computer insecurity as computer security is Orwellian on a theoretical level. And on a practical level, the computer insecurity industry does more to peddle fear and insecurity than to solve any genuine security problems. And finally, the importance of computer insecurity and the amount of effort and energy invested into it, are all way out of proportion to what it deserves.  -- RK 
      </p>
      <p>
        <em>I think "insecurity mitigation" is both more accurate and generates less cognitive dissonance. (Whether generating cognitive dissonance is appropriate to the topic is left as an exercise for the </em>WikiHiveMind.) The crux of the current problem is that the platforms that are out there in the world today (excepting a few deployments of specialty platforms) use an ancient threat model that doesn't consider scenarios like "the user unwittingly runs something malicious" or "application goes rogue due to bug exploitation". Anti-malware and network sniffer tools are just a means of damage control.<em></em>
      </p>
      <p>
        <em>Bombarding the user with permission popups isn't an improvement, it's capitulation in the case of "the user unwittingly runs something malicious", and blatant </em><a href="BlameAvoidance.html">BlameAvoidance</a> on the part of the software vendor. <a href="InternetExplorer.html">InternetExplorer</a> and <a href="WindowsVista.html">WindowsVista</a> are prime examples of this sort of gaffe, as <a href="BruceSchneier.html">BruceSchneier</a> argues at <a href="http://www.schneier.com/blog/archives/2006/04/microsoft_vista.html.">http://www.schneier.com/blog/archives/2006/04/microsoft_vista.html.</a> The <a href="OneLaptopPerChild.html">OneLaptopPerChild</a> project takes a more comprehensive approach to security with BitFrost (see <a href="http://dev.laptop.org/git.do?p=security;a=blob;hb=HEAD;f=bitfrost.txt),">http://dev.laptop.org/git.do?p=security;a=blob;hb=HEAD;f=bitfrost.txt),</a> including treating programs as potential bad actors in their own right, requiring them to declare at installation time what resources they need access to (and disallowing some combinations altogether as too dangerous). -- jh<em> </em>
      </p>
      <hr/>
      <p>
        Donn B. Parker says that computer security involves 6 independent areas:
      </p>
      <ul>
        <li>
           Availability
        </li>
        <li>
           Utility
        </li>
        <li>
           Integrity
        </li>
        <li>
           Authenticity
        </li>
        <li>
           Confidentiality
        </li>
        <li>
           Possession or Control
        </li>
      </ul>
      <p>
        I suspect RK is reacting to the fact that people talking about "security" seem to over-emphasize "control" and "secrecy", making incremental improvements in those areas at the cost of huge losses in the other areas.
      </p>
      <p>
        Sometimes people, in the name of "security", make systems worse in availability, utility, or in other ways. Often, those people claim those things are "outside the bounds of computer security" (<a href="ComputerSecurityTheory.html">ComputerSecurityTheory</a> ), so therefore <a href="ThatsNotMyProblem.html">ThatsNotMyProblem</a>.
      </p>
      <p>
        For example, some systems lock out a username after 3 failed guesses at a password, then you must physically visit the password lady and ask for another password. This makes it trivial for some malicious person to mount a denial-of-service attack by typing in your username, then deliberately making wrong guesses at the password until he gets the "too many wrong guesses" error message. So we get a gain in authenticity, at a huge loss in availability.
      </p>
      <p>
        <em>Password change policy set to reject password changes within some time, e.g. three days. This makes it impossible to change a password a second time if someone looked over your shoulder while you changed it. (The rationale behind is of course that people change their passwords twice when they have to change it regularly - to a new one, and then back to the old one again.) Solution: use a passwd program that keeps a history of changes and prevents re-use.</em>
      </p>
      <p>
        <em>"Availability" also means backups (for data, programs and even hardware), something a lot of IT security departments do not take into account.</em>
      </p>
      <p>
        Occasionally, someone will come up with a brilliant idea that makes the overall system better (more "secure", according to Parker's definition), even when it gives up some control and secrecy. Rather than locking out people who are likely to mess things up ("attackers"), and allowing people who are less likely to mess things up to do *anything* ("friends"), often it's better to provide a safety net so that when things do get messed up (whether maliciously or accidentally), things can be put right before any real damage is done.
      </p>
      <p>
        I'd rather have everyone in the world - including my worst enemy - know exactly what I'm allergic to, rather than run the risk that the emergency techs on the ambulance *don't* know what I'm allergic to. (When it comes to my allergies, I am *not* willing to give up availability/utility in order to increase confidentiality or control.) -- <a href="DavidCary.html">DavidCary</a> 
      </p>
      <hr/>
      <p>
        <a href="CategorySecurity.html">CategorySecurity</a>
      </p>
    </div>
  </body>
</html>