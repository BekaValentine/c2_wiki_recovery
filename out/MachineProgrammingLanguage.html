<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <base href="/mount/ultralaser_home/Projects/c2_wiki_recovery/out/">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="page">
      <h1>
        <img src="wiki.gif" />
        Machine Programming Language
      </h1>
      <p>
        Not to be confused with <a href="AssemblyLanguage.html">AssemblyLanguage</a>, which is a human readable form of this with mnemonics and all this fancy stuff. We are talking opcodes and bits and bytes here.
      </p>
      <p>
        <em></em><a href="EditHint.html">EditHint</a>: merge with <a href="MachineCode.html">MachineCode</a><em></em>
      </p>
      <hr/>
      <p>
        From <a href="TheMostWidelyUsedProgrammingLanguageAtAnyLevel.html">TheMostWidelyUsedProgrammingLanguageAtAnyLevel</a>:
      </p>
      <p>
        The Intel x86 instruction set may be the <a href="ProgrammingLanguage.html">ProgrammingLanguage</a> most widely used at any level.
      </p>
      <p>
        <strong>Hmm.</strong> <em>The instruction set itself is not a programming language per se; even if you were doing X86 assembly it still wouldn't be the "language." Only if you were putting opcodes directly into a file or something. Your point about the X86 family being the one of the most popular cores is well taken.</em>
      </p>
      <p>
        <em>However, even that may be a mistake. It is quite possible that the Intel MCS-51 family (8051, 8052) is the single most popular processor on the face of the planet. In the last 28 years I have seen more products created for this processor architecture than everything else I've worked on combined. The installed base is incredible. TV remote controls operate off of this processor, and there are more TVs than computers by a very wide margin. The MCS-51 has been used in everything you can shake a stick at for more than 30 years. Even after Intel dropped the thing like a hot rock there were a couple dozen silicon kilns churning the things out. Manufacturers such as Dallas and NEC refused to let it die a natural death. Instead they kept coming out with newer, more advanced versions that had greater speed, memory capacity, expanded instruction sets, etc. The last I heard Dallas had a variant that went 60 MHz and had something like 8K of directly addressable core memory. Oy.</em>
      </p>
      <p>
        Why isn't an instruction set a programming language?  It tells the processor what to do.  What else does it need to be considered a programming language?  Mnemonics?  Why is that so important?
      </p>
      <p>
        <em>An instruction set is not a language, it is an instruction set. It is the very lowest level of </em><a href="MachineCode.html">MachineCode</a> that the processor runs on. If you want to enter opcodes directly into memory or whatever then I guess that is your "language;" we start to get lost in semantics here. Is there a larger point to this line of questioning?<em></em>
      </p>
      <p>
        I don't know if there's a point.  The first digital computers were programmed with nothing more than an instruction set.  A university prof made me write a PDP-11 program as octal instructions, no mnemonics.  (It seemed like a cruel joke at first, but it revealed the elegance and symmetry of the PDP-11 instruction set.)  Every possible behavior of a computer can be written with nothing more than the instruction set.  It isn't just <em>a</em> programming language, it's <em>the</em> programming language.  Everything else is <a href="SyntacticSugar.html">SyntacticSugar</a>.  (Is that too extreme?)
      </p>
      <p>
        <em>Yes. To suggest that a programming language is just syntax and fluff is to fly in the face of nearly fifty years of computer science. Rear Admiral </em><a href="GraceHopper.html">GraceHopper</a> didn't create the entire scientific field of compilers for it to be dismissed as "syntactic sugar."<em></em>
      </p>
      <hr/>
      <p>
        <strong>Opcodes as Language</strong>
      </p>
      <p>
        Oops! Sorry. I see what happened here. Y'all took my "programmable hardware" analogy at face value. Hmm. (Misleading argument removed.) The term "programmable hardware" has very specific usage, which I abused in this context to make a point. Through my cavalier usage of the term some folks were <a href="LeadUpTheGardenPath.html">LeadUpTheGardenPath</a>. Sorry about that. <em><ahem></em>
      </p>
      <p>
        Opcodes tell the hardware what to do <em>directly,</em> without any further need of interpretation. Mnemonics are just the way we humans identify the opcodes -- the CPU doesn't know diddly about ROT or LDX or JMP or BNE or any of that good stuff. The CPU understands FED0 AF34 4BD7 0070, etc. Opcodes are numeric representations of the <strong>operation</strong> that the CPU is told to perform, such as adding one to a register or moving the memory contents at this address to that address. This is most assuredly <strong>NOT</strong> any kind of language; it is telling the hardware what to do the same as if you had a bank of switches and flipped them to a certain setting before hitting the "act" button, then set them to another setting, etc.
      </p>
      <p>
        And by the way -- a CPU <em>isn't</em> programmable hardware. It is very rigid hardware with certain inalterable rules. Those rules are specified in the opcode set. The CPU can only perform certain operations, which we identify using mnemonics.
      </p>
      <p>
        Real programmable hardware -- like GALs, PALs, and FPGAs -- are programmed not through any kind of language per se, but through the manipulation of their internal link sets. One doesn't use a language to "talk" to a FPGA; one uses a programmer instrument with a specific set of voltages and driving signals to force link mapping within the FPGA device. Once programmable hardware has been set up it is just as rigid as any set of relays from the 20th century, following a fixed set of rules and exhibiting very clearly defined behavior. Just like a CPU.
      </p>
      <p>
        Hope this helps. -- <a href="MartySchrader.html">MartySchrader</a>
      </p>
      <p>
        <em>Opcodes tell the hardware what to do </em>directly,<em> without any further need of interpretation.</em>  Well, maybe.  Some processors drive internal logic gates from the decoded opcode.  Some processors use them as an entry point into a <a href="MicroCode.html">MicroCode</a> program.  There are also CPUs implemented with programmable hardware.  So, it can depend on the level of your viewpoint.
      </p>
      <code>
        "Great fleas have little fleas upon their backs to bite 'em,<br/>
        And little fleas have lesser fleas, and so <em>ad infinitum</em><br/>
        And the great fleas themselves, in turn, have greater fleas to go on, <br/>
        While these again have greater still, and greater still, and so on."<br/>
        -- Augustas De Morgan, <em>A Budget of Paradoxes</em><br/>
      </code>
      <p>
        Are we back to arguing semantics? I sure hope not, because the industry-accepted "definition" for a complex instruction set microprocessor refers to the kind of hardware I described earlier -- essentially a set of registers and logic gates tied together by the designed-in fixed rules of operation.
      </p>
      <hr/>
      <p>
        <em>The Intel x86 instruction set may be the </em><a href="ProgrammingLanguage.html">ProgrammingLanguage</a> most widely used at any level.<em></em>
      </p>
      <p>
        I'm going to dispute this statement from a completely different direction.
        All Intel x86 processors are either 16-bit or 32-bit processors.
      </p>
      <p>
        As of 2002,
        Less than 10% of all the CPUs sold in the world are 32-bit or more. About 12% of all the CPUs sold in the world are 16-bit. Of all the 32-bit CPUs sold, most are *not* x86 -- "ARM-based chips alone do about triple the volume that Intel and AMD peddle to PC makers.". "Taken as a whole, the average price for a microprocessor, microcontroller, or DSP is just over $6." -- <a href="http://www.embedded.com/shared/printableArticle.jhtml?articleID=9900861">http://www.embedded.com/shared/printableArticle.jhtml?articleID=9900861</a>
      </p>
      <p>
        So if you count up all the physical CPUs currently executing code, and divide them up by which instruction set they used, it is likely that the specific instruction set used by some 8-bit processor family has the biggest fraction. I wish I had more details on which one sold more chips.
        It is even possible that (since high-volume 8-bit processors are most often programmed directly in assembly language) if you divide them up by which programming language was used to program them, some specific assembly language would have the biggest fraction -- (PIC assembly language ? AVR assembly language ? 8048 assembly language ? 8051 assembly language ? 6800 assembly language ?) -- although it is possible that C might have the biggest fraction, since it is used on all of those CPUs.
      </p>
      <hr/>
      <p>
        See <a href="AssemblyLanguage.html">AssemblyLanguage</a>
      </p>
      <p>
        <a href="CategoryProgrammingLanguage.html">CategoryProgrammingLanguage</a>
      </p>
    </div>
  </body>
</html>