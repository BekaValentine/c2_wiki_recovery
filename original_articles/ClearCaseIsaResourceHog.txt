ClearCase requires memory. The mantra of every ClearCase administrator is "Boss, buy more memory.". --unsigned
(Not to mention disk space, process slots,...  Think of it this way: every directory and file in the development area is stored as a special purpose "table like" thing and is queried to dynamically determine which version to use. Which costs, but it's wonderful when you can afford it.  --MarkSwanson)

From ClearCase
----
We had odd performance problems with ClearCase on NT. The exact details are hazy, but we discovered that neither the ClearCase clients nor the server were particularly good at caching IP addresses, putting a high load on the nearest WINS/DNS server. Our internal name server was located on the far side of a congested router. Once we repartitioned the network, performance got better.

----
You should also realize that ClearCase operation is based on RPC calls, and many of them for a single operation. Don't run clients over modem links - your LAN should be using switches, not HUBs. Just multiply the number of files you deal with by the round trip delay for a given link to get a picture. This is the reason each site needs their own server.
LatencyKillsYou, as found in StupidMeasurements.  --PeterVohmann
----
Using ClearCase in its standard mode (dynamic views, AKA  MVFS) means that your entire source tree is mounted on your server.  Every file access, every time your IDE reads data, every rebuild, every directory listing is a hit on your server.  This means that if your server is in a serious grind, 'everybody' feels the impact.  If the server goes down for some reason, you have just lost access to your sources.  I have personally taken five minutes to run an "ls" command on a directory.  --RobMandeville