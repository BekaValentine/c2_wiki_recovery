Is there a way to determine how many people are contributing and reading Wiki?

''You can estimate the number of contributors by watching RecentChanges. But some people contribute from several different hosts, and some people share a host/gateway/proxy server. This should be easier once the new UserName becomes widely used.''

''To watch the readers/lurkers, you'd have to monitor the server logs. They're not immediately available on-line; you'd probably have to ask WardCunningham for access.''

Top Pages Served: http://c2.com/cgi/topten?do=counts&do=wiki (source FindPage)

-----

Wiki is visited by about 5000 people a day. Most people read from 5 to 15 pages on a visit. At least once a day a site tries to read all 7000 pages in one visit. This always fails. Traffic at the site has grown slowly and steadily for five years reaching 2000 visitors a day last fall. Since then volume has grown more rapidly. -- WardCunningham

http://c2.com/mrtg/adsl-year.gif

See MoreAboutTheDatabase.

----

Currently (Dec 2000) about 1600-2000 visitors have created some kind of homepage. Most of them (perhaps about 70-80%) have added one or more signed contributions. 

-----

''This raises two questions: Is this 5000 unique people, or just 5000 visits? Also, why does a read of all 7000 fail? OK, I lied... one more question... how would one go about "reading" all pages?''

See WikiList if you want a complete list of all current page names.
(...from MoreAboutContents)
 Currently at  7811 pages -- 03/14/2000.
 Currently at  8638 pages -- 04/12/2000.
 Currently at 13420 pages -- 12/23/2000.
 Currently at 26289 pages -- 02/07/2004.
 Currently at 28796 pages -- 08/25/2004

Here are a few more historical numbers:
 "Over 700 pages", with "100 visitors a day" in May, 1996 (probably earlier)
 5035 pages on July 7, 1999
 5224 pages on August 8, 1999
 5527 pages on September 18, 1999

See http://c2.com/cgi/wikiPages for current trends.

Ward put in a trap to prevent robots who ignore the robots.txt file from downloading the whole site (and thus causing a denial of service). After frequent enough hits, Wiki will tell the robot to "smeg off" essentially.