0 is a natural number. Or maybe it isn't.

''When I was in high school (in Canada), my math teachers made a point of saying that the natural numbers are 1, 2, 3, ..., and that 0 is not natural. Then in university it seemed that everyone felt that other way, that the naturals should be 0, 1, 2, ... . The high school argument made more sense: 0 is unnatural because it represents the absence of things, and as evidenced by its relatively late historical appearance, it's a less natural number than 1, 2, 3, ... . But, it's just a definition, and it's more convenient to include 0 than to exclude it.''

'''Of course 0 is a natural number.  Burn in inquisition those who say it isn't.  HaHaOnlySerious.'''

They additionally taught me in Canada that while the "natural" numbers were all positive integers starting at 1, the "whole" numbers were the same, with the inclusion of zero.

	 :	''I believe this distinction is primarily the invention of designers of public school curricula.  I haven't researched the point, but I would be willing to bet US$1 that the distinction between "natural" and "whole" in school texts dates back no farther than the break-neck effort to redesign American school curricula to be more "scientific" in the immediately post-Sputnik days.  If so, you can thank the School Mathematics Study Group, and not (say) Euclid, for this (trivial) distinction.  In any case, Real Mathematicians (tm) '''never''' talk about "whole" numbers, and their "natural" numbers definitely include zero.  (Etymologically, the term "whole" is just the Latin term "integer" translated into English, and, used as a name for numbers, stands in contrast to "fractional" -- Latin for "broken" -- numbers.) See further remarks below.''

----
I am forever indebted to one of my early teachers who made it clear that the "common", "natural" or "usual" words were often misleading in mathematics.
	 :	''Mathematicians are like Frenchmen. Whatever you say to them they translate into their own language and at once it is something entirely different.'' -- Goethe
There are three main concepts here:
	* integers: ... -3, -2, -1, 0, 1, 2, 3, ...
	* non-negative integers: 0, 1, 2, 3, ...
	* positive integers: 1, 2, 3, ...

What one person calls natural, another calls "whole". C and Python programmers count from 0, Pascal and Fortran programmers count from 1. Set theorists count from 0, number theorists (usually) from 1.  A prof in my former CS department was notorious for counting from 0 in everyday life.   His books started on page 0.  Once when we were marking exams he counted the people in the room, finding N of them, and came back with N-1 solution sheets.   I alleged that he went, 0,1,2,...N-1, and then forgot to add 1 to
get the correct answer.  He maintained that he forgot to include himself in the set.

	 :	''The last statement ("set theorists... number theorists....") doesn't agree with my experience, or with my exposure to set theorists and number theorists, as I was working my way through graduate school en route to my Ph.D. in mathematics.  '''''N''''' without zero wouldn't even be a monoid (under addition), and so as an algebraic structure would be of little interest to a number theorist.  (When you look at '''''N''''' with multiplication, of course it is a monoid without 0, but then you are often thinking about finite fields, for which there is other notation....)  As for "counting", I can think of two things you might mean: cardinality, in which case you obviously need zero as the cardinality of the null set, or indexing (e.g. terms in a sequence or series), which normally (in my experience) does start with 0 unless there's an obvious good reason not to. -- CameronSmith''

----
'''Therefore''', avoid the term "natural number" and the notation "'''N'''" unless either you are very sure of your audience or the distinction doesn't matter. Refer to "non-negative integers" and "positive integers". If you need a concise notation, you can write "'''Z'''" with a subscripted ">=0" or ">0".

	 :	''Given the comments elsewhere on this page (e.g. "natural" vs. "whole" numbers), this "Therefore" '''might''' be good advice for people writing papers in computer science.  I wouldn't know.  But it is '''not''' what mathematicians do.  The proposed "Z-sub->=0" and "Z-sub->0" notation would look bizarre to me in a math text or published paper. (Actually, I have seen notations like '''''R''''' with a superscript (not subscript) "+" to represent positive reals, so if anything I would expect '''''Z'''''-super-+ rather than '''''Z'''''-sub-anything. --CameronSmith again''  Yes, or N \sup +, as well, for Z \sup +

----
The whole problem of confusing technical definitions with the everyday language equivalent is why NonEuclidean geometry got off to a slow start.  In order to make sense of the resulting theorems, people had to make a broad leap of logic and rewrite the definition of a line; the practice (used in a lot of college TY math books) is that the plain word (such a line) means the everyday definition; while "line" or '''line''' means the technical one. Thus you can write "A '''line''' is defined as a circle on a sphere where the center of the circle has the same location as the center of the sphere." and it will make perfect sense.
[ BelTorak ]

----
In describing EuclidOfAlexandria's description of the Greatest Common Divisor algorithm, DonKnuth remarked that for the ancient Greeks there was no number ''one''.

I guess their thinking was that if you have only one thing, you don't have a "number" of them, now do you?

----
Nonnegative integers can be defined in terms of sets:

  0 = {}     (empty set)
  1 = {{}}
  2 = {{},{{}}}
  ...
  n = n-1 U {n-1}
Can they really be?  That is highly debatable.  See for example Paul Benacerraf's famous paper "What numbers could not be".
It's just common sense that numbers are not any particular abstract objects that happen to have all the same structural relations between them that numbers do (and then some extra ones as well -- 1 is not literally a subset (or a member) of 2, for example).   I don't see what is so wrong with the traditional analysis of numbers that makes them out as properties of sets -- 2 being the property that all sets of 2 elements have in common, for example.  Are the individuation critera for properties more difficult than those for sets, thus rendering the acceptance of properties unacceptable?  They may be more difficult, but if you lost your keys in a big pile of keys, that doesn't justify looking for them in a desert landscape just because they would more clearly stand out there.

----
Unnatural... irrational... negative... imaginary....  Why this litany of pejoratives?  Can't we all just get along?  -(Pi)i.  Maybe not: the problem is that we are all ultimately complex.

----

''as evidenced by its relatively late historical appearance''

Despite the late appearance of the ''numeral'' (which itself is debatable) & some silly musings by Greek philosophers (who also questioned whether one was a number), the ''number'' zero certainly didn't have a late appearance. Ever since people started counting & trading goods (if not before), people knew that if you took one away from one you'd be left with none.

----
Also see ChurchNumeral