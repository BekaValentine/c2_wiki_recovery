Degeneralization of words occurs when a term that has general meaning is given a specific meaning within a certain context.

Some Examples:
	* "pointer" vs "reference" vs "id", etc. - In English, these are nearly perfect synonyms. In programming, these take on very specific meanings. In C++, a "reference" could've been called a "once-seated pointers", or "spointers" for short. To find an example where DeGeneralization has run amok, look to the C++ CORBA binding. Yikes!
	* "memory" - How many times have we heard a user confuse "permanent storage" with "memory", and how can they be blamed for this confusion?
	* QaIsNotQc

Pros: The reader's previous understanding of the term is brought into their minds whenever they hear the term.

Cons: The reader's previous understanding of the term is brought into their minds whenever they hear the term.

When the reader sees a new name, they are forced to acknowledge that there is a new idea. When you reuse an existing name, you run the risk that the reader will confuse old and new ideas.

On the other hand, it can be a "pro" to springboard off of the reader's previous knowledge, usually metaphorically. Metaphors are great, but metaphors break down. Why not use the metaphor to introduce the term, instead of *being* the term?

----

See:
	* NeologismsNotHomonyms - For a more established treatment of this problem
	* CategoryDegeneralization - For a snide category

----

The alternative, creating new terms, also has problems. One of the accusations in TheCurseOfXanadu was that TedNelson insisted on using new terms for new ideas. When an old idea turned out to have newly understood winkles, it needed a new name. This wrought havoc with the code and contributed to AnalysisParalysis.

Maybe, though, the problem was the obscurity of the terms. Quoting from the article:

	 :	''Shapiro also discovered that the group had been working together so long it had developed a kind of private slang. It took months to comprehend what the programmers were talking about. Most of them were book lovers and trivia mongers who enjoyed developing a metaphor based on obscure sources and extending it via even more unlikely combinations. For instance, the object in the Xanadu system that resembled a file was called a bert, after Bertrand Russell. With files called bert, there had to be something called an ernie , and so in the Xanadu publishing system, an ernie was the unit of information for which users would be billed. To understand the details of Xanadu, Shapiro had to learn not only the names for things, but also the history of how those names had come to be.''

"Bert"? Ouch. On the other hand, the same project yielded the term "hypertext", which is a positive example of successfully avoiding "degeneralization".

----

''We share this naming problem with physics, another discipline that likes to take "normal" words and give them different meanings: spin, color, etc. No matter what ordinary word you chose for a new concept, the word's old meaning is going to be at least partly wrong. Your other choice is to coin entirely new words, as does biology and the other descriptive sciences. But it's often better to let the hearer have a partly wrong idea of what you're talking about, based upon the conventional meaning of the word, than to leave the hearer entirely in the dark by using a totally new word.''

The main difference is in the level of precision used in giving these things meaning. There is a shedload of group theory behind the notion of the color charge. No-one seriously argues a case with a physicist over what colour charge is all about without ''trying'' to understand QCD. However, computing science hangs its jargon from the lowest branch, and every part of it ''without'' a mathematical background is in dispute from those who can reach. And the little maths that is left is dismissed as irrelevant to real world experience...despite the continuing widespread use of such irrelevances as compilers (irony alert). How have we come to the point where people think that CS can be completely understood by analogy?
----

''Keep each concept in its proper context and this shouldn't be a problem. Just make sure that the context is clear...'' -- EdwardKiser

Why force future users of the word to surround it with context, when you can embed the context in the word itself?

Context is by definition that which surrounds a thing. If you want a word to stand for something particular regardless of context, you must create a new word, and a definition in the desired context; you are not then "embedding" context but avoiding meaning in other contexts.
----
CategoryDegeneralization 
